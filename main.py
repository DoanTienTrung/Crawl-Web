import sys
import os
import io
from datetime import datetime

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scrapers import (
    CafeFScraper,
    CafelandScraper,
    VnExpressScraper,
    VnEconomyScraper,
    VOVScraper,
    VietnametScraper,
    DanTriRSSScraper,
    ThanhNienRSSScraper,
    TuoiTreRSSScraper,
    LaoDongScraper,
    NLDScraper,
    VietStockScraper,
    ANTTRSSScraper,
    CNARSSScraper,
    QDNDRSSScraper,
    KinhTeNgoaiThuongScraper,
    ThoiBaoNganHangScraper,
    TaiChinhDoanhNghiepScraper,
    BaoChinhPhuScraper,
    TinNhanhChungKhoanScraper,
    NguoiQuanSatScraper,
    ThoiBaoTaiChinhScraper,
    Coin68Scraper,
    VietnamFinanceScraper,
    XaydungChinhsachScraper,
)
from database.models import db
from utils.exporters import export_to_csv, export_to_json

def scrape_xaydungchinhsach(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape X√¢y d·ª±ng ch√≠nh s√°ch - C·ªïng th√¥ng tin Ch√≠nh ph·ªß"""
    print("\n" + "="*60)
    print("üèõÔ∏è  X√ÇY D·ª∞NG CH√çNH S√ÅCH")
    print("="*60)
    
    scraper = XaydungChinhsachScraper()
    articles = scraper.fetch_news(max_articles=10)
    
    if articles:
        _save_and_export(articles, "xaydungchinhsach", save_to_db, export_csv)
    else:
        print("‚ö† Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ X√¢y d·ª±ng ch√≠nh s√°ch.")
        
    return articles

def scrape_vietnamfinance(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape VietnamFinance - Tin t·ª©c t√†i ch√≠nh, ƒë·∫ßu t∆∞ (Trang ch·ªß)"""
    print("\n" + "="*60)
    print("üì∞  VIETNAMFINANCE (VIETNAMFINANCE.VN)")
    print("="*60)

    scraper = VietnamFinanceScraper()
    articles = scraper.fetch_news(max_articles=15)

    if articles:
        _save_and_export(articles, "vietnamfinance", save_to_db, export_csv)
    else:
        print("‚ö† Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt m·ªõi n√†o t·ª´ VietnamFinance (ho·∫∑c b√†i ƒë√£ t·ªìn t·∫°i).")

    return articles

def scrape_coin68(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Coin68 - Tin t·ª©c th·ªã tr∆∞·ªùng Ti·ªÅn m√£ h√≥a (M·ª•c Hot News)"""
    print("\n" + "="*60)
    print("ü™ô  COIN68 (COIN68.COM)")
    print("="*60)

    scraper = Coin68Scraper()
    articles = scraper.fetch_news(max_articles=10)

    if articles:
        _save_and_export(articles, "coin68", save_to_db, export_csv)
    else:
        print("‚ö† Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt m·ªõi n√†o t·ª´ Coin68.")

    return articles

def scrape_nguoiquansat(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Ng∆∞·ªùi Quan S√°t v·ªõi c·∫•u tr√∫c m·ªõi"""
    print("\n" + "="*60)
    print("üìà NG∆Ø·ªúI QUAN S√ÅT (NGUOIQUANSAT.VN)")
    print("="*60)

    scraper = NguoiQuanSatScraper()
    articles = scraper.fetch_news(max_articles=10)

    _save_and_export(articles, "nguoiquansat", save_to_db, export_csv)
    return articles

def scrape_thoibaotaichinh(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Th·ªùi b√°o T√†i ch√≠nh Vi·ªát Nam (thoibaotaichinhvietnam.vn)"""
    print("\n" + "="*60)
    print("üìà TH·ªúI B√ÅO T√ÄI CH√çNH (THOIBAOTAICHINHVN.VN)")
    print("="*60)

    scraper = ThoiBaoTaiChinhScraper()
    articles = scraper.fetch_news(max_articles=10)

    _save_and_export(articles, "thoibaotaichinh", save_to_db, export_csv)

    return articles


def scrape_taichinhdoanhnghiep(save_to_db: bool = True, export_csv: bool = True) -> list:
    """H√†m ƒëi·ªÅu ph·ªëi qu√©t tin t·ª´ T√†i ch√≠nh Doanh nghi·ªáp (taichinhdoanhnghiep.net.vn)"""
    print("\n" + "="*60)
    print("üíº T√ÄI CH√çNH DOANH NGHI·ªÜP SCRAPER")
    print("="*60)

    scraper = TaiChinhDoanhNghiepScraper()

    try:
        articles = scraper.fetch_news(max_articles=15)
    except Exception as e:
        print(f"‚ùå L·ªói khi qu√©t tin: {str(e)}")
        return []

    if articles:
        print(f"‚ú® ƒê√£ thu th·∫≠p t·ªïng c·ªông {len(articles)} b√†i vi·∫øt h·ª£p l·ªá.")

        if save_to_db or export_csv:
            _save_and_export(articles, "taichinhdoanhnghiep", save_to_db, export_csv)
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt m·ªõi n√†o ho·∫∑c c·∫•u h√¨nh Selector b·ªã sai.")

    return articles

def scrape_kinhte(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Kinh t·∫ø v√† Ngo·∫°i th∆∞∆°ng"""
    print("\n" + "="*60)
    print("üìà KINH T·∫æ NGO·∫†I TH∆Ø∆†NG SCRAPER")
    print("="*60)

    scraper = KinhTeNgoaiThuongScraper()
    articles = scraper.fetch_news()
    
    _save_and_export(articles, "kinhte", save_to_db, export_csv)
    return articles

def scrape_qdnd(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape b√°o Qu√¢n ƒë·ªôi nh√¢n d√¢n qua RSS"""
    print("\n" + "="*60)
    print("üéñÔ∏è QU√ÇN ƒê·ªòI NH√ÇN D√ÇN (QDND) RSS SCRAPER")
    print("="*60)

    scraper = QDNDRSSScraper()
    articles = scraper.fetch_news()
    
    _save_and_export(articles, "qdnd", save_to_db, export_csv)
    return articles


def scrape_cafef(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape CafeF"""
    print("\n" + "="*60)
    print("üîµ CAFEF.VN SCRAPER")
    print("="*60)

    scraper = CafeFScraper()
    articles = scraper.fetch_news(max_pages=1, max_articles_per_page=20)

    _save_and_export(articles, "cafef", save_to_db, export_csv)
    return articles


def scrape_cafeland(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Cafeland"""
    print("\n" + "="*60)
    print("üü† CAFELAND.VN SCRAPER ")
    print("="*60)

    scraper = CafelandScraper()
    articles = scraper.fetch_news(max_pages=1, max_articles_per_page=20)

    _save_and_export(articles, "cafeland", save_to_db, export_csv)
    return articles


def scrape_vnexpress(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape VnExpress"""
    print("\n" + "="*60)
    print("üü¢ VNEXPRESS.NET SCRAPER ")
    print("="*60)

    scraper = VnExpressScraper()
    articles = scraper.fetch_news(max_pages=1)

    _save_and_export(articles, "vnexpress", save_to_db, export_csv)
    return articles


def scrape_tuoitre(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape TuoiTre s·ª≠ d·ª•ng RSS"""
    print("\n" + "="*60)
    print("üü¢ TUOITRE.VN RSS SCRAPER")
    print("="*60)

    scraper = TuoiTreRSSScraper()
    articles = scraper.fetch_news()

    _save_and_export(articles, "tuoitre", save_to_db, export_csv)
    return articles


def scrape_vneconomy(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape VnEconomy using RSS feed"""
    print("\n" + "="*60)
    print("üü° VNECONOMY.VN ")
    print("="*60)

    scraper = VnEconomyScraper()
    articles = scraper.fetch_news(max_articles=20)

    if not articles:
        print(f"‚ö† No articles scraped from vneconomy! ")
        return []

    _save_and_export(articles, "vneconomy", save_to_db, export_csv)
    return articles


def scrape_vov(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape VOV"""
    print("\n" + "="*60)
    print("üî¥ VOV.VN SCRAPER")
    print("="*60)

    scraper = VOVScraper()
    articles = scraper.fetch_news(max_pages=1)

    _save_and_export(articles, "vov", save_to_db, export_csv)
    return articles

def scrape_cna(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Channel NewsAsia (CNA) s·ª≠ d·ª•ng RSS Feed"""
    print("\n" + "="*60)
    print("üî¥ CHANNEL NEWSASIA (CNA)")
    print("="*60)

    scraper = CNARSSScraper()
    articles = scraper.fetch_news()

    _save_and_export(articles, "cna", save_to_db, export_csv)

    return articles


def scrape_vietnamnet(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Vietnamnet"""
    print("\n" + "="*60)
    print("üü£ VIETNAMNET.VN SCRAPER ")
    print("="*60)

    scraper = VietnametScraper()
    articles = scraper.fetch_news()

    _save_and_export(articles, "vietnamnet", save_to_db, export_csv)
    return articles


def scrape_dantri(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape DanTri s·ª≠ d·ª•ng RSS"""
    print("\n" + "="*60)
    print("üî¥ DANTRI.COM.VN RSS SCRAPER")
    print("="*60)

    scraper = DanTriRSSScraper()
    articles = scraper.fetch_news()

    _save_and_export(articles, "dantri", save_to_db, export_csv)
    return articles


def scrape_thanhnien(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape ThanhNien s·ª≠ d·ª•ng RSS"""
    print("\n" + "="*60)
    print("üîµ THANHNIEN.VN RSS SCRAPER")
    print("="*60)

    scraper = ThanhNienRSSScraper()
    articles = scraper.fetch_news()

    _save_and_export(articles, "thanhnien", save_to_db, export_csv)

    return articles


def scrape_laodong(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape LaoDong"""
    print("\n" + "="*60)
    print("üü° LAODONG.VN SCRAPER")
    print("="*60)

    scraper = LaoDongScraper()
    articles = scraper.fetch_news(max_articles=20)

    _save_and_export(articles, "laodong", save_to_db, export_csv)

    return articles


def scrape_nld(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape NLD (Ng∆∞·ªùi Lao ƒê·ªông)"""
    print("\n" + "="*60)
    print("üîµ NLD.COM.VN SCRAPER")
    print("="*60)

    scraper = NLDScraper()
    articles = scraper.fetch_news(max_articles=20)

    _save_and_export(articles, "nld", save_to_db, export_csv)

    return articles

def scrape_baochinhphu(save_to_db: bool = True, export_csv: bool = True) -> list:
    """ƒêi·ªÅu ph·ªëi qu√©t tin t·ª´ B√°o Ch√≠nh ph·ªß"""
    print("\n" + "="*60)
    print("üèõÔ∏è B√ÅO CH√çNH PH·ª¶ SCRAPER")
    print("="*60)

    scraper = BaoChinhPhuScraper()
    articles = scraper.fetch_news(max_articles=15)

    if articles:
        _save_and_export(articles, "baochinhphu", save_to_db, export_csv)
    return articles


def scrape_tinnhanhchungkhoan(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Tin nhanh ch·ª©ng kho√°n"""
    print("\n" + "="*60)
    print("üìà TIN NHANH CH·ª®NG KHO√ÅN SCRAPER")
    print("="*60)

    scraper = TinNhanhChungKhoanScraper()
    articles = scraper.fetch_news(max_articles=10)

    _save_and_export(articles, "tinnhanhchungkhoan", save_to_db, export_csv)
    return articles


def scrape_vietstock(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape VietStock"""
    print("\n" + "="*60)
    print("üü¢ VIETSTOCK.VN SCRAPER")
    print("="*60)

    scraper = VietStockScraper()
    articles = scraper.fetch_news(max_articles=15)

    _save_and_export(articles, "vietstock", save_to_db, export_csv)

    return articles

def scrape_antt(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape ANTT.vn s·ª≠ d·ª•ng RSS"""
    print("\n" + "="*60)
    print("üü† ANTT.VN RSS SCRAPER")
    print("=" * 60)

    scraper = ANTTRSSScraper()
    articles = scraper.fetch_news()

    _save_and_export(articles, "antt", save_to_db, export_csv)
    return articles

def scrape_thoibaonganhang(save_to_db: bool = True, export_csv: bool = True) -> list:
    """H√†m ƒëi·ªÅu ph·ªëi Scraper cho Th·ªùi b√°o Ng√¢n h√†ng (thoibaonganhang.vn)"""
    print("\n" + "="*60)
    print("üè¶ TH·ªúI B√ÅO NG√ÇN H√ÄNG (TBNH) SCRAPER")
    print("="*60)

    scraper = ThoiBaoNganHangScraper()
    articles = scraper.fetch_news(max_articles=15)

    if articles:
        _save_and_export(articles, "thoibaonganhang", save_to_db, export_csv)
    else:
        print("\n‚ö† No articles scraped from thoibaonganhang!")

    return articles

def _save_and_export(articles: list, source_name: str, save_to_db: bool, export_csv: bool):
    """Helper function ƒë·ªÉ save v√† export"""
    if not articles:
        print(f"\n‚ö† No articles scraped from {source_name}!")
        return

    print(f"\nüìä Total articles scraped: {len(articles)}")

    if save_to_db:
        print("\nüíæ Saving to database...")
        saved_count = 0
        for article in articles:
            # article tuple: (published_at, title, link, content, source, stock_related, sentiment_score, server_pushed, category)
            if len(article) == 9:
                data = article[:8]
                category = article[8]
                if db.insert_news_with_category(data, category):
                    saved_count += 1
            else:
                if db.insert_news(article):
                    saved_count += 1

        print(f"‚úì Saved {saved_count}/{len(articles)} articles to database")

    if export_csv:
        print("\nüìÅ Exporting to CSV...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        articles_dict = []
        for a in articles:
            articles_dict.append({
                'published_at': a[0],
                'title': a[1],
                'link': a[2],
                'content': a[3],
                'source': a[4],
                'stock_related': a[5],
                'sentiment_score': a[6],
                'server_pushed': a[7],
                'category': a[8] if len(a) > 8 else '',
            })

        csv_path = export_to_csv(articles_dict, filename=f"{source_name}_{timestamp}")
        print(f"‚úì Exported to: {csv_path}")


def scrape_all():
    """Scrape t·∫•t c·∫£ c√°c ngu·ªìn"""
    print("\n" + "="*60)
    print("üöÄ MULTISOURCE NEWS SCRAPER ")
    print("="*60)

    print("\n[1] Setting up database... ")
    try:
        db.create_tables()
    except Exception as e:
        print(f"‚ö† Database warning: {e} ")
        print("Will export to CSV only... ")

    all_articles = []

    all_articles.extend(scrape_cafef(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_cafeland(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vnexpress(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vneconomy(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vov(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vietnamnet(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_dantri(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_thanhnien(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_tuoitre(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_laodong(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_nld(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vietstock(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_antt(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_cna(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_qdnd(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_kinhte(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_thoibaonganhang(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_taichinhdoanhnghiep(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_baochinhphu(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_tinnhanhchungkhoan(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_nguoiquansat(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_thoibaotaichinh(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_coin68(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vietnamfinance(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_xaydungchinhsach(save_to_db=True, export_csv=False))

    print("\n" + "="*60)
    print("üìÅ FINAL EXPORT")
    print("="*60)

    if all_articles:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        articles_dict = []
        for a in all_articles:
            articles_dict.append({
                'published_at': a[0],
                'title': a[1],
                'link': a[2],
                'content': a[3],
                'source': a[4],
                'stock_related': a[5],
                'sentiment_score': a[6],
                'server_pushed': a[7],
                'category': a[8] if len(a) > 8 else '',
            })

        csv_path = export_to_csv(articles_dict, filename=f"all_news_{timestamp}")
        json_path = export_to_json(articles_dict, filename=f"all_news_{timestamp}")

        print(f"\n{'='*60}")
        print(f"‚úÖ DONE!")
        print(f"{'='*60}")
        print(f"Total articles: {len(all_articles)}")
        print(f"CSV: {csv_path}")
        print(f"JSON: {json_path}")

    return all_articles


if __name__ == "__main__":
    mode = sys.argv[1].lower() if len(sys.argv) > 1 else 'all'

    try:
        if mode == 'cafef':
            db.create_tables()
            scrape_cafef()
        elif mode == 'dantri':
            db.create_tables()
            scrape_dantri()
        elif mode == 'thanhnien':
            db.create_tables()
            scrape_thanhnien()
        elif mode == 'cafeland':
            db.create_tables()
            scrape_cafeland()
        elif mode == 'vietnamfinance':
            db.create_tables()
            scrape_vietnamfinance()
        elif mode == 'vnexpress':
            db.create_tables()
            scrape_vnexpress()
        elif mode == 'thoibaonganhang':
            db.create_tables()
            scrape_thoibaonganhang()
        elif mode == 'coin68':
            db.create_tables()
            scrape_coin68()
        elif mode == 'vneconomy':
            db.create_tables()
            scrape_vneconomy()
        elif mode == 'xaydungchinhsach':
            db.create_tables()
            scrape_xaydungchinhsach()
        elif mode == 'nguoiquansat':
            db.create_tables()
            scrape_nguoiquansat()
        elif mode == 'taichinhdoanhnghiep':
            db.create_tables()
            scrape_taichinhdoanhnghiep()
        elif mode == 'antt':
            db.create_tables()
            scrape_antt()
        elif mode == 'baochinhphu':
            db.create_tables()
            scrape_baochinhphu()
        elif mode == 'tinnhanhchungkhoan':
            db.create_tables()
            scrape_tinnhanhchungkhoan()
        elif mode == 'qdnd':
            db.create_tables()
            scrape_qdnd()
        elif mode == 'thoibaotaichinh':
            db.create_tables()
            scrape_thoibaotaichinh()
        elif mode == 'tuoitre':
            db.create_tables()
            scrape_tuoitre()
        elif mode == 'vov':
            db.create_tables()
            scrape_vov()
        elif mode == 'kinhte':
            db.create_tables()
            scrape_kinhte()
        elif mode == 'vietnamnet':
            db.create_tables()
            scrape_vietnamnet()
        elif mode == 'laodong':
            db.create_tables()
            scrape_laodong()
        elif mode == 'cna':
            db.create_tables()
            scrape_cna()
        elif mode == 'nld':
            db.create_tables()
            scrape_nld()
        elif mode == 'vietstock':
            db.create_tables()
            scrape_vietstock()
        else:
            scrape_all()

    except KeyboardInterrupt:
        print("\n\n‚ö† Scraping interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        raise
