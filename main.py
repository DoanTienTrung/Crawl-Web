"""
News Scraper - Main Entry Point
Tool crawl tin t·ª©c t·ª´ nhi·ªÅu ngu·ªìn v√† l∆∞u v√†o PostgreSQL + Export CSV

Supported sources:
- CafeF.vn
- Cafeland.vn
- VnExpress.net
- VnEconomy.vn
- VOV.vn
- Vietnamnet.vn

Usage:
    python main.py qdnd
    python main.py agromonitor
    python main.py vietstock
    python main.py antt
    python main.py tuoitre
    python main.py laodong
    python main.py thanhnien
    python main.py dantri
    python main.py                  # Scrape t·∫•t c·∫£ sources
    python main.py cafef            # Ch·ªâ CafeF
    python main.py cafeland         # Ch·ªâ Cafeland
    python main.py vnexpress        # Ch·ªâ VnExpress
    python main.py vneconomy        # Ch·ªâ VnEconomy
    python main.py vov              # Ch·ªâ VOV
    python main.py vietnamnet       # Ch·ªâ Vietnamnet
    python main.py csv              # Scrape CafeF v√† export CSV only
    python main.py test             # Test mode
"""

import sys
import os
import io
from datetime import datetime

# Fix Unicode encoding for Windows console
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scrapers.multi_source_scraper import (
    CafeFScraper,
    CafelandScraper,
    VnExpressScraper,
    VnEconomyScraper,
    VOVScraper,
    VietnametScraper,
    DanTriRSSScraper,
    ThanhNienRSSScraper,
    TuoiTreRSSScraper,
    LaoDongScraper,
    NLDScraper,
    VietStockScraper,
    ANTTRSSScraper,
    # AgroMonitorScraper,
    CNARSSScraper,
    QDNDRSSScraper,
    KinhTeNgoaiThuongScraper,
    ThoiBaoNganHangScraper,
    TaiChinhDoanhNghiepScraper,
    BaoChinhPhuScraper,
    TinNhanhChungKhoanScraper,
    NguoiQuanSatScraper,
    ThoiBaoTaiChinhScraper,
)
from database.models import db
from utils.exporters import export_to_csv, export_to_json

def scrape_nguoiquansat(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Ng∆∞·ªùi Quan S√°t v·ªõi c·∫•u tr√∫c m·ªõi"""
    print("\n" + "="*60)
    print("üìà NG∆Ø·ªúI QUAN S√ÅT (NGUOIQUANSAT.VN) - main.py")
    print("="*60)
    
    scraper = NguoiQuanSatScraper()
    # Qu√©t 10 b√†i m·ªõi nh·∫•t
    articles = scraper.fetch_news(max_articles=10)
    
    _save_and_export(articles, "nguoiquansat", save_to_db, export_csv)
    return articles

def scrape_thoibaotaichinh(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Th·ªùi b√°o T√†i ch√≠nh Vi·ªát Nam (thoibaotaichinhvietnam.vn)"""
    print("\n" + "="*60)
    print("üìà TH·ªúI B√ÅO T√ÄI CH√çNH (THOIBAOTAICHINHVN.VN) - main.py")
    print("="*60)
    
    scraper = ThoiBaoTaiChinhScraper()
    
    # Crawl 10 b√†i m·ªõi nh·∫•t t·ª´ trang ch·ªß
    # (n·∫øu mu·ªën theo chuy√™n m·ª•c, thay url trong scraper.fetch_news())
    articles = scraper.fetch_news(max_articles=10)
    
    # L∆∞u v√† xu·∫•t CSV gi·ªëng c·∫•u tr√∫c c≈©
    _save_and_export(articles, "thoibaotaichinh", save_to_db, export_csv)
    
    return articles


def scrape_taichinhdoanhnghiep(save_to_db: bool = True, export_csv: bool = True) -> list:
    """
    H√†m ƒëi·ªÅu ph·ªëi qu√©t tin t·ª´ T√†i ch√≠nh Doanh nghi·ªáp (taichinhdoanhnghiep.net.vn)
    """
    print("\n" + "="*60)
    print("üíº T√ÄI CH√çNH DOANH NGHI·ªÜP SCRAPER")
    print("="*60)

    # 1. Kh·ªüi t·∫°o scraper
    scraper = TaiChinhDoanhNghiepScraper()
    
    # 2. L·∫•y d·ªØ li·ªáu b√†i vi·∫øt (m·∫∑c ƒë·ªãnh l·∫•y 15 b√†i m·ªõi nh·∫•t t·ª´ trang ch·ªß)
    try:
        articles = scraper.fetch_news(max_articles=15)
    except Exception as e:
        print(f"‚ùå L·ªói khi qu√©t tin: {str(e)}")
        return []

    # 3. X·ª≠ l√Ω l∆∞u tr·ªØ
    if articles:
        print(f"‚ú® ƒê√£ thu th·∫≠p t·ªïng c·ªông {len(articles)} b√†i vi·∫øt h·ª£p l·ªá.")
        
        # G·ªçi h√†m helper d√πng chung ƒë·ªÉ save v√†o DB v√† xu·∫•t CSV
        # L∆∞u √Ω: ƒê·∫£m b·∫£o b·∫°n ƒë√£ c√≥ h√†m _save_and_export ho·∫∑c logic t∆∞∆°ng t·ª± trong main.py
        if save_to_db or export_csv:
            _save_and_export(articles, "taichinhdoanhnghiep", save_to_db, export_csv)
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt m·ªõi n√†o ho·∫∑c c·∫•u h√¨nh Selector b·ªã sai.")

    return articles

def scrape_kinhte(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Kinh t·∫ø v√† Ngo·∫°i th∆∞∆°ng"""
    print("\n" + "="*60)
    print("üìà KINH T·∫æ NGO·∫†I TH∆Ø∆†NG SCRAPER")
    print("="*60)

    scraper = KinhTeNgoaiThuongScraper()
    articles = scraper.fetch_news()
    
    _save_and_export(articles, "kinhte", save_to_db, export_csv)
    return articles

def scrape_qdnd(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape b√°o Qu√¢n ƒë·ªôi nh√¢n d√¢n qua RSS"""
    print("\n" + "="*60)
    print("üéñÔ∏è QU√ÇN ƒê·ªòI NH√ÇN D√ÇN (QDND) RSS SCRAPER")
    print("="*60)

    scraper = QDNDRSSScraper()
    # H√†m fetch_news() trong class RSS kh√¥ng c·∫ßn tham s·ªë max_articles 
    # v√¨ n√≥ ƒë√£ gi·ªõi h·∫°n 20 b√†i b√™n trong logic.
    articles = scraper.fetch_news()
    
    _save_and_export(articles, "qdnd", save_to_db, export_csv)
    return articles


def scrape_cafef(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape CafeF"""
    print("\n - main.py:58" + "="*60)
    print("üîµ CAFEF.VN SCRAPER - main.py:59")
    print("="*60)

    scraper = CafeFScraper()
    articles = scraper.fetch_news(max_pages=1, max_articles_per_page=20)
    
    _save_and_export(articles, "cafef", save_to_db, export_csv)
    return articles

# def scrape_agromonitor(save_to_db: bool = True, export_csv: bool = True, max_articles: int = 20) -> list:
#     """Scrape AgroMonitor.vn"""
#     print("\n" + "="*60)
#     print("üü¢ AGROMONITOR.VN SCRAPER")
#     print("="*60)

#     # Kh·ªüi t·∫°o scraper chuy√™n bi·ªát cho AgroMonitor
#     scraper = AgroMonitorScraper()

#     # L·∫•y tin t·ª´ trang ch·ªß/tin m·ªõi nh·∫•t (category 16)
#     # L∆∞u √Ω: N·∫øu trang n√†y y√™u c·∫ßu login ƒë·ªÉ th·∫•y n·ªôi dung,
#     # b·∫°n c·∫ßn ƒë·∫£m b·∫£o self.headers trong class scraper ƒë√£ c√≥ Cookie.
#     articles = scraper.fetch_news(max_pages=1, max_articles_per_page=max_articles)

#     # S·ª≠ d·ª•ng h√†m ti·ªán √≠ch chung c·ªßa b·∫°n ƒë·ªÉ l∆∞u v√† xu·∫•t d·ªØ li·ªáu
#     _save_and_export(articles, "agromonitor", save_to_db, export_csv)

#     return articles

def scrape_cafeland(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Cafeland"""
    print("\n" + "="*60)
    print("üü† CAFELAND.VN SCRAPER - main.py:72")
    print("="*60)

    scraper = CafelandScraper()
    articles = scraper.fetch_news(max_pages=1, max_articles_per_page=20)

    _save_and_export(articles, "cafeland", save_to_db, export_csv)
    return articles


def scrape_vnexpress(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape VnExpress"""
    print("\n" + "="*60)
    print("üü¢ VNEXPRESS.NET SCRAPER - main.py:85")
    print("="*60)

    scraper = VnExpressScraper()
    articles = scraper.fetch_news(max_pages=1)  # Crawl page ƒë·∫ßu ti√™n
    
    _save_and_export(articles, "vnexpress", save_to_db, export_csv)
    return articles


def scrape_tuoitre(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape TuoiTre s·ª≠ d·ª•ng RSS"""
    print("\n" + "="*60)
    print("üü¢ TUOITRE.VN RSS SCRAPER - main.py:98")
    print("="*60)

    scraper = TuoiTreRSSScraper()
    articles = scraper.fetch_news()
    
    _save_and_export(articles, "tuoitre", save_to_db, export_csv)
    return articles


def scrape_vneconomy(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape VnEconomy using RSS feed"""
    print("\n" + "="*60)
    print("üü° VNECONOMY.VN RSS SCRAPER - main.py:111")
    print("="*60)
    
    scraper = VnEconomyScraper()
    
    # V·ªõi RSS, ch√∫ng ta ch·ªâ c·∫ßn truy·ªÅn t·ªïng s·ªë b√†i vi·∫øt mu·ªën l·∫•y (max_articles)
    # Kh√¥ng c·∫ßn chia trang (max_pages) v√¨ RSS tr·∫£ v·ªÅ danh s√°ch tin m·ªõi nh·∫•t t·∫≠p trung
    articles = scraper.fetch_news(max_articles=20)
    
    if not articles:
        print(f"‚ö† No articles scraped from vneconomy! - main.py")
        return []

    _save_and_export(articles, "vneconomy", save_to_db, export_csv)
    return articles


def scrape_vov(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape VOV"""
    print("\n" + "="*60)
    print("üî¥ VOV.VN SCRAPER - main.py:124")
    print("="*60)

    scraper = VOVScraper()
    articles = scraper.fetch_news(max_pages=1)  # Crawl page ƒë·∫ßu ti√™n

    _save_and_export(articles, "vov", save_to_db, export_csv)
    return articles

def scrape_cna(save_to_db: bool = True, export_csv: bool = True) -> list:
    """
    Scrape Channel NewsAsia (CNA) s·ª≠ d·ª•ng RSS Feed
    """
    print("\n" + "="*60)
    print("üî¥ CHANNEL NEWSASIA (CNA) RSS SCRAPER")
    print("="*60)

    scraper = CNARSSScraper()
    articles = scraper.fetch_news()
    
    _save_and_export(articles, "cna", save_to_db, export_csv)
    
    return articles


def scrape_vietnamnet(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Vietnamnet"""
    print("\n" + "="*60)
    print("üü£ VIETNAMNET.VN SCRAPER - main.py:137")
    print("="*60)

    scraper = VietnametScraper()
    articles = scraper.fetch_news()  # Crawl page ƒë·∫ßu ti√™n

    _save_and_export(articles, "vietnamnet", save_to_db, export_csv)
    return articles


def scrape_dantri(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape DanTri s·ª≠ d·ª•ng RSS"""
    print("\n" + "="*60)
    print("üî¥ DANTRI.COM.VN RSS SCRAPER - main.py:150")
    print("="*60)

    scraper = DanTriRSSScraper()
    # RSS c·ªßa D√¢n tr√≠ kh√¥ng c·∫ßn truy·ªÅn s·ªë trang nh∆∞ c√†o HTML th√¥ng th∆∞·ªùng
    articles = scraper.fetch_news() 
    
    _save_and_export(articles, "dantri", save_to_db, export_csv)
    return articles


def scrape_thanhnien(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape ThanhNien s·ª≠ d·ª•ng RSS"""
    print("\n" + "="*60)
    print("üîµ THANHNIEN.VN RSS SCRAPER - main.py:164")
    print("="*60)

    # Kh·ªüi t·∫°o scraper chuy√™n bi·ªát cho Thanh Ni√™n
    scraper = ThanhNienRSSScraper()

    # L·∫•y tin t·ª©c t·ª´ RSS (m·∫∑c ƒë·ªãnh l·∫•y 20 b√†i m·ªõi nh·∫•t nh∆∞ ƒë√£ thi·∫øt l·∫≠p trong Class)
    articles = scraper.fetch_news()

    # G·ªçi h√†m helper ƒë·ªÉ l∆∞u v√† xu·∫•t d·ªØ li·ªáu
    _save_and_export(articles, "thanhnien", save_to_db, export_csv)

    return articles


def scrape_laodong(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape LaoDong"""
    print("\n" + "="*60)
    print("üü° LAODONG.VN SCRAPER")
    print("="*60)

    scraper = LaoDongScraper()

    # L·∫•y 20 b√†i m·ªõi nh·∫•t t·ª´ trang tin m·ªõi
    articles = scraper.fetch_news(max_articles=20)

    # G·ªçi h√†m helper ƒë·ªÉ l∆∞u v√† xu·∫•t d·ªØ li·ªáu
    _save_and_export(articles, "laodong", save_to_db, export_csv)

    return articles


def scrape_nld(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape NLD (Ng∆∞·ªùi Lao ƒê·ªông)"""
    print("\n" + "="*60)
    print("üîµ NLD.COM.VN SCRAPER")
    print("="*60)

    scraper = NLDScraper()

    # L·∫•y 20 b√†i m·ªõi nh·∫•t t·ª´ trang tin 24h
    articles = scraper.fetch_news(max_articles=20)

    # G·ªçi h√†m helper ƒë·ªÉ l∆∞u v√† xu·∫•t d·ªØ li·ªáu
    _save_and_export(articles, "nld", save_to_db, export_csv)

    return articles

def scrape_baochinhphu(save_to_db: bool = True, export_csv: bool = True) -> list:
    """ƒêi·ªÅu ph·ªëi qu√©t tin t·ª´ B√°o Ch√≠nh ph·ªß"""
    print("\n" + "="*60)
    print("üèõÔ∏è B√ÅO CH√çNH PH·ª¶ SCRAPER")
    print("="*60)

    scraper = BaoChinhPhuScraper()
    articles = scraper.fetch_news(max_articles=15)

    if articles:
        # S·ª≠ d·ª•ng h√†m helper c·ªßa b·∫°n ƒë·ªÉ l∆∞u
        _save_and_export(articles, "baochinhphu", save_to_db, export_csv)
    return articles


def scrape_tinnhanhchungkhoan(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Tin nhanh ch·ª©ng kho√°n"""
    print("\n" + "="*60)
    print("üìà TIN NHANH CH·ª®NG KHO√ÅN SCRAPER")
    print("="*60)

    scraper = TinNhanhChungKhoanScraper()
    articles = scraper.fetch_news(max_articles=10)

    _save_and_export(articles, "tinnhanhchungkhoan", save_to_db, export_csv)
    return articles


def scrape_vietstock(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape VietStock"""
    print("\n" + "="*60)
    print("üü¢ VIETSTOCK.VN SCRAPER")
    print("="*60)

    scraper = VietStockScraper()

    # L·∫•y 15 b√†i m·ªõi nh·∫•t t·ª´ trang m·ªõi c·∫≠p nh·∫≠t
    articles = scraper.fetch_news(max_articles=15)

    # G·ªçi h√†m helper ƒë·ªÉ l∆∞u v√† xu·∫•t d·ªØ li·ªáu
    _save_and_export(articles, "vietstock", save_to_db, export_csv)

    return articles

def scrape_antt(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape ANTT.vn s·ª≠ d·ª•ng RSS"""
    print("\n" + "="*60)
    print("üü† ANTT.VN RSS SCRAPER")
    print("=" * 60)

    # Kh·ªüi t·∫°o class scraper 
    scraper = ANTTRSSScraper()
    
    # L·∫•y tin t·ª´ RSS
    articles = scraper.fetch_news() 
    
    _save_and_export(articles, "antt", save_to_db, export_csv)
    return articles

def scrape_thoibaonganhang(save_to_db: bool = True, export_csv: bool = True) -> list:
    """
    H√†m ƒëi·ªÅu ph·ªëi Scraper cho Th·ªùi b√°o Ng√¢n h√†ng (thoibaonganhang.vn)
    """
    print("\n" + "="*60)
    print("üè¶ TH·ªúI B√ÅO NG√ÇN H√ÄNG (TBNH) SCRAPER")
    print("="*60)

    # Kh·ªüi t·∫°o class scraper (ƒë·∫£m b·∫£o b·∫°n ƒë√£ import class ThoiBaoNganHangScraper)
    scraper = ThoiBaoNganHangScraper()
    
    # Th·ª±c hi·ªán b√≥c t√°ch d·ªØ li·ªáu
    articles = scraper.fetch_news(max_articles=15)
    
    # L∆∞u v√† xu·∫•t d·ªØ li·ªáu (S·ª≠ d·ª•ng h√†m d√πng chung trong d·ª± √°n c·ªßa b·∫°n)
    if articles:
        _save_and_export(articles, "thoibaonganhang", save_to_db, export_csv)
    else:
        print("\n‚ö† No articles scraped from thoibaonganhang!")
        
    return articles

def _save_and_export(articles: list, source_name: str, save_to_db: bool, export_csv: bool):
    """Helper function ƒë·ªÉ save v√† export"""
    if not articles:
        print(f"\n‚ö† No articles scraped from {source_name}! - main.py:197")
        return
    
    print(f"\nüìä Total articles scraped: {len(articles)} - main.py:200")
    
    # Save to database
    if save_to_db:
        print("\nüíæ Saving to database... - main.py:204")
        saved_count = 0
        for article in articles:
            # article tuple: (published_at, title, link, content, source, stock_related, sentiment_score, server_pushed, category)
            if len(article) == 9:
                data = article[:8]  # Exclude category for basic insert
                category = article[8]
                if db.insert_news_with_category(data, category):
                    saved_count += 1
            else:
                if db.insert_news(article):
                    saved_count += 1
        
        print(f"‚úì Saved {saved_count}/{len(articles)} articles to database - main.py:217")
    
    # Export to CSV
    if export_csv:
        print("\nüìÅ Exporting to CSV... - main.py:221")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Convert tuples to dicts for export
        articles_dict = []
        for a in articles:
            articles_dict.append({
                'published_at': a[0],
                'title': a[1],
                'link': a[2],
                'content': a[3],
                'source': a[4],
                'stock_related': a[5],
                'sentiment_score': a[6],
                'server_pushed': a[7],
                'category': a[8] if len(a) > 8 else '',
            })
        
        csv_path = export_to_csv(articles_dict, filename=f"{source_name}_{timestamp}")
        print(f"‚úì Exported to: {csv_path} - main.py:240")


def scrape_all():
    """Scrape t·∫•t c·∫£ c√°c ngu·ªìn"""
    print("\n" + "="*60)
    print("üöÄ MULTISOURCE NEWS SCRAPER - main.py:246")
    print("="*60)
    
    # Setup database
    print("\n[1] Setting up database... - main.py:250")
    try:
        db.create_tables()
    except Exception as e:
        print(f"‚ö† Database warning: {e} - main.py:254")
        print("Will export to CSV only... - main.py:255")
    
    all_articles = []
    
    # Scrape t·ª´ng source
    all_articles.extend(scrape_cafef(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_cafeland(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vnexpress(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vneconomy(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vov(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vietnamnet(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_dantri(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_thanhnien(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_tuoitre(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_laodong(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_nld(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vietstock(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_antt(save_to_db=True, export_csv=False))
    # all_articles.extend(scrape_agromonitor(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_cna(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_qdnd(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_kinhte(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_thoibaonganhang(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_taichinhdoanhnghiep(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_baochinhphu(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_tinnhanhchungkhoan(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_nguoiquansat(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_thoibaotaichinh(save_to_db=True, export_csv=False))

    # Export all to CSV
    print("\n" + "="*60)
    print("üìÅ FINAL EXPORT - main.py:273")
    print("="*60)
    
    if all_articles:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        articles_dict = []
        for a in all_articles:
            articles_dict.append({
                'published_at': a[0],
                'title': a[1],
                'link': a[2],
                'content': a[3],
                'source': a[4],
                'stock_related': a[5],
                'sentiment_score': a[6],
                'server_pushed': a[7],
                'category': a[8] if len(a) > 8 else '',
            })
        
        csv_path = export_to_csv(articles_dict, filename=f"all_news_{timestamp}")
        json_path = export_to_json(articles_dict, filename=f"all_news_{timestamp}")
        
        print(f"\n{'='*60} - main.py:295")
        print(f"‚úÖ DONE! - main.py:296")
        print(f"{'='*60} - main.py:297")
        print(f"Total articles: {len(all_articles)} - main.py:298")
        print(f"CSV: {csv_path} - main.py:299")
        print(f"JSON: {json_path} - main.py:300")

    return all_articles



def test_mode():
    """Test v·ªõi m·ªôt source"""
    print("= - main.py:305"*60)
    print("üß™ TEST MODE  CafeF Only - main.py:306")
    print("= - main.py:307"*60)

    scraper = CafeFScraper()
    scraper.delay = 1  # Faster for testing

    print("\nFetching 3 articles for testing... - main.py:312")
    articles = scraper.fetch_news(max_pages=1, max_articles_per_page=3)
    
    print(f"\nüìä Results: {len(articles)} articles - main.py:315")
    for i, article in enumerate(articles, 1):
        print(f"\n Article {i} - main.py:317")
        print(f"Title: {article[1][:60]}... - main.py:318")
        print(f"Link: {article[2][:60]}... - main.py:319")
        print(f"Published: {datetime.fromtimestamp(article[0]) if article[0] else 'N/A'} - main.py:320")
        print(f"Content: {len(article[3])} chars - main.py:321")
        print(f"Source: {article[4]} - main.py:322")


def show_help():
    """Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n"""
    print("""
MULTI-SOURCE NEWS SCRAPER
=========================

Usage:
    python main.py                  Scrape t·∫•t c·∫£ sources (CafeF, Cafeland, VnExpress, VnEconomy, VOV, Vietnamnet)
    python main.py cafef            Ch·ªâ scrape CafeF
    python main.py cafeland         Ch·ªâ scrape Cafeland
    python main.py vnexpress        Ch·ªâ scrape VnExpress
    python main.py vneconomy        Ch·ªâ scrape VnEconomy
    python main.py vov              Ch·ªâ scrape VOV
    python main.py vietnamnet       Ch·ªâ scrape Vietnamnet
    python main.py csv              Scrape CafeF, export CSV only (kh√¥ng c·∫ßn DB)
    python main.py test             Test mode (3 b√†i t·ª´ CafeF)
    python main.py help             Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n n√†y

Database Schema:
    Table: news
    - id (UUID, primary key)
    - published_at (bigint, Unix timestamp)
    - title (text, NOT NULL, UNIQUE)
    - link (text)
    - content (text)
    - source (text)
    - stock_related (text)
    - sentiment_score (text)
    - server_pushed (boolean)
    - created_at (timestamp)
    - category (text)

Output:
    - Database: PostgreSQL table 'news'
    - CSV files: exports/
    - JSON files: exports/
""")


if __name__ == "__main__":
    mode = sys.argv[1].lower() if len(sys.argv) > 1 else 'all'
    
    try:
        if mode == 'cafef':
            db.create_tables()
            scrape_cafef()
        elif mode == 'dantri':       
            db.create_tables()
            scrape_dantri()
        elif mode == 'thanhnien':
            db.create_tables()
            scrape_thanhnien()
        elif mode == 'cafeland':
            db.create_tables()
            scrape_cafeland()
        elif mode == 'vnexpress':
            db.create_tables()
            scrape_vnexpress()
        elif mode == 'thoibaonganhang':
            db.create_tables()
            scrape_thoibaonganhang()
        elif mode == 'vneconomy':
            db.create_tables()
            scrape_vneconomy()
        elif mode == 'nguoiquansat':
            db.create_tables()
            scrape_nguoiquansat()
        elif mode == 'taichinhdoanhnghiep':
            db.create_tables()
            scrape_taichinhdoanhnghiep()
        elif mode == 'antt':
            db.create_tables()
            scrape_antt()
        elif mode == 'baochinhphu':
            db.create_tables()
            scrape_baochinhphu()
        elif mode == 'tinnhanhchungkhoan':
            db.create_tables()
            scrape_tinnhanhchungkhoan()
        elif mode == 'qdnd':
            db.create_tables()
            scrape_qdnd()
        elif mode == 'thoibaotaichinh':
            db.create_tables()
            scrape_thoibaotaichinh()
        elif mode == 'tuoitre':
            db.create_tables()
            scrape_tuoitre()
        elif mode == 'vov':
            db.create_tables()
            scrape_vov()
        elif mode == 'kinhte':
            db.create_tables()
            scrape_kinhte()
        elif mode == 'vietnamnet':
            db.create_tables()
            scrape_vietnamnet()
        elif mode == 'laodong':
            db.create_tables()
            scrape_laodong()
        # elif mode == 'agromonitor':
        #     db.create_tables()
        #     scrape_agromonitor()
        elif mode == 'cna':
            db.create_tables()
            scrape_cna()
        elif mode == 'nld':
            db.create_tables()
            scrape_nld()
        elif mode == 'vietstock':
            db.create_tables()
            scrape_vietstock()
        elif mode == 'csv':
            scrape_cafef(save_to_db=False, export_csv=True)
        elif mode == 'test':
            test_mode()
        elif mode in ['help', '-h', '--help']:
            show_help()
        else:
            scrape_all()
            
    except KeyboardInterrupt:
        print("\n\n‚ö† Scraping interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        raise
