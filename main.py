"""
News Scraper - Main Entry Point
Tool crawl tin t·ª©c t·ª´ nhi·ªÅu ngu·ªìn v√† l∆∞u v√†o PostgreSQL + Export CSV

Supported sources:
- CafeF.vn
- VnExpress.net
- VnEconomy.vn
- VOV.vn
- Vietnamnet.vn

Usage:
    python main.py                  # Scrape t·∫•t c·∫£ sources
    python main.py cafef            # Ch·ªâ CafeF
    python main.py vnexpress        # Ch·ªâ VnExpress
    python main.py vneconomy        # Ch·ªâ VnEconomy
    python main.py vov              # Ch·ªâ VOV
    python main.py vietnamnet       # Ch·ªâ Vietnamnet
    python main.py csv              # Scrape CafeF v√† export CSV only
    python main.py test             # Test mode
"""

import sys
import os
from datetime import datetime

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scrapers.multi_source_scraper import (
    CafeFScraper,
    VnExpressScraper,
    VnEconomyScraper,
    VOVScraper,
    VietnametScraper,
)
from database.models import db
from utils.exporters import export_to_csv, export_to_json


def scrape_cafef(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape CafeF"""
    print("\n" + "="*60)
    print("üîµ CAFEF.VN SCRAPER")
    print("="*60)

    scraper = CafeFScraper()
    articles = scraper.fetch_news(max_pages=4, max_articles_per_page=20)
    
    _save_and_export(articles, "cafef", save_to_db, export_csv)
    return articles


def scrape_vnexpress(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape VnExpress"""
    print("\n" + "="*60)
    print("üü¢ VNEXPRESS.NET SCRAPER")
    print("="*60)
    
    scraper = VnExpressScraper()
    articles = scraper.fetch_news(max_pages=2)
    
    _save_and_export(articles, "vnexpress", save_to_db, export_csv)
    return articles


def scrape_vneconomy(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape VnEconomy"""
    print("\n" + "="*60)
    print("üü° VNECONOMY.VN SCRAPER")
    print("="*60)
    
    scraper = VnEconomyScraper()
    articles = scraper.fetch_news(max_pages=2)
    
    _save_and_export(articles, "vneconomy", save_to_db, export_csv)
    return articles


def scrape_vov(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape VOV"""
    print("\n" + "="*60)
    print("üî¥ VOV.VN SCRAPER")
    print("="*60)
    
    scraper = VOVScraper()
    articles = scraper.fetch_news()
    
    _save_and_export(articles, "vov", save_to_db, export_csv)
    return articles


def scrape_vietnamnet(save_to_db: bool = True, export_csv: bool = True) -> list:
    """Scrape Vietnamnet"""
    print("\n" + "="*60)
    print("üü£ VIETNAMNET.VN SCRAPER")
    print("="*60)

    scraper = VietnametScraper()
    articles = scraper.fetch_news()  # Crawl t·∫•t c·∫£ c√°c pages

    _save_and_export(articles, "vietnamnet", save_to_db, export_csv)
    return articles


def _save_and_export(articles: list, source_name: str, save_to_db: bool, export_csv: bool):
    """Helper function ƒë·ªÉ save v√† export"""
    if not articles:
        print(f"\n‚ö† No articles scraped from {source_name}!")
        return
    
    print(f"\nüìä Total articles scraped: {len(articles)}")
    
    # Save to database
    if save_to_db:
        print("\nüíæ Saving to database...")
        saved_count = 0
        for article in articles:
            # article tuple: (published_at, title, link, content, source, stock_related, sentiment_score, server_pushed, category)
            if len(article) == 9:
                data = article[:8]  # Exclude category for basic insert
                category = article[8]
                if db.insert_news_with_category(data, category):
                    saved_count += 1
            else:
                if db.insert_news(article):
                    saved_count += 1
        
        print(f"  ‚úì Saved {saved_count}/{len(articles)} articles to database")
    
    # Export to CSV
    if export_csv:
        print("\nüìÅ Exporting to CSV...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Convert tuples to dicts for export
        articles_dict = []
        for a in articles:
            articles_dict.append({
                'published_at': a[0],
                'title': a[1],
                'link': a[2],
                'content': a[3],
                'source': a[4],
                'stock_related': a[5],
                'sentiment_score': a[6],
                'server_pushed': a[7],
                'category': a[8] if len(a) > 8 else '',
            })
        
        csv_path = export_to_csv(articles_dict, filename=f"{source_name}_{timestamp}")
        print(f"  ‚úì Exported to: {csv_path}")


def scrape_all():
    """Scrape t·∫•t c·∫£ c√°c ngu·ªìn"""
    print("="*60)
    print("üöÄ MULTI-SOURCE NEWS SCRAPER")
    print("="*60)
    
    # Setup database
    print("\n[1] Setting up database...")
    try:
        db.create_tables()
    except Exception as e:
        print(f"‚ö† Database warning: {e}")
        print("Will export to CSV only...")
    
    all_articles = []
    
    # Scrape t·ª´ng source
    all_articles.extend(scrape_cafef(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vnexpress(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vneconomy(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vov(save_to_db=True, export_csv=False))
    all_articles.extend(scrape_vietnamnet(save_to_db=True, export_csv=False))
    
    # Export all to CSV
    print("\n" + "="*60)
    print("üìÅ FINAL EXPORT")
    print("="*60)
    
    if all_articles:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        articles_dict = []
        for a in all_articles:
            articles_dict.append({
                'published_at': a[0],
                'title': a[1],
                'link': a[2],
                'content': a[3],
                'source': a[4],
                'stock_related': a[5],
                'sentiment_score': a[6],
                'server_pushed': a[7],
                'category': a[8] if len(a) > 8 else '',
            })
        
        csv_path = export_to_csv(articles_dict, filename=f"all_news_{timestamp}")
        json_path = export_to_json(articles_dict, filename=f"all_news_{timestamp}")
        
        print(f"\n{'='*60}")
        print(f"‚úÖ DONE!")
        print(f"{'='*60}")
        print(f"Total articles: {len(all_articles)}")
        print(f"CSV: {csv_path}")
        print(f"JSON: {json_path}")


def test_mode():
    """Test v·ªõi m·ªôt source"""
    print("="*60)
    print("üß™ TEST MODE - CafeF Only")
    print("="*60)

    scraper = CafeFScraper()
    scraper.delay = 1  # Faster for testing

    print("\nFetching 3 articles for testing...")
    articles = scraper.fetch_news(max_pages=1, max_articles_per_page=3)
    
    print(f"\nüìä Results: {len(articles)} articles")
    for i, article in enumerate(articles, 1):
        print(f"\n--- Article {i} ---")
        print(f"Title: {article[1][:60]}...")
        print(f"Link: {article[2][:60]}...")
        print(f"Published: {datetime.fromtimestamp(article[0]) if article[0] else 'N/A'}")
        print(f"Content: {len(article[3])} chars")
        print(f"Source: {article[4]}")


def show_help():
    """Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n"""
    print("""
MULTI-SOURCE NEWS SCRAPER
=========================

Usage:
    python main.py                  Scrape t·∫•t c·∫£ sources (CafeF, VnExpress, VnEconomy, VOV, Vietnamnet)
    python main.py cafef            Ch·ªâ scrape CafeF
    python main.py vnexpress        Ch·ªâ scrape VnExpress
    python main.py vneconomy        Ch·ªâ scrape VnEconomy
    python main.py vov              Ch·ªâ scrape VOV
    python main.py vietnamnet       Ch·ªâ scrape Vietnamnet
    python main.py csv              Scrape CafeF, export CSV only (kh√¥ng c·∫ßn DB)
    python main.py test             Test mode (3 b√†i t·ª´ CafeF)
    python main.py help             Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n n√†y

Database Schema:
    Table: news
    - id (UUID, primary key)
    - published_at (bigint, Unix timestamp)
    - title (text, NOT NULL, UNIQUE)
    - link (text)
    - content (text)
    - source (text)
    - stock_related (text)
    - sentiment_score (text)
    - server_pushed (boolean)
    - created_at (timestamp)
    - category (text)

Output:
    - Database: PostgreSQL table 'news'
    - CSV files: exports/
    - JSON files: exports/
""")


if __name__ == "__main__":
    mode = sys.argv[1].lower() if len(sys.argv) > 1 else 'all'
    
    try:
        if mode == 'cafef':
            db.create_tables()
            scrape_cafef()
        elif mode == 'vnexpress':
            db.create_tables()
            scrape_vnexpress()
        elif mode == 'vneconomy':
            db.create_tables()
            scrape_vneconomy()
        elif mode == 'vov':
            db.create_tables()
            scrape_vov()
        elif mode == 'vietnamnet':
            db.create_tables()
            scrape_vietnamnet()
        elif mode == 'csv':
            scrape_cafef(save_to_db=False, export_csv=True)
        elif mode == 'test':
            test_mode()
        elif mode in ['help', '-h', '--help']:
            show_help()
        else:
            scrape_all()
            
    except KeyboardInterrupt:
        print("\n\n‚ö† Scraping interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        raise
