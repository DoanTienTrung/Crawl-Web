from scrapers.base import NewsScraperBase
from typing import List, Tuple, Optional
from bs4 import BeautifulSoup
from datetime import datetime


class Coin68Scraper(NewsScraperBase):
    def __init__(self):
        super().__init__()
        self.source = "coin68.com"
        self.base_url = "https://coin68.com"

    def fetch_news(self, max_articles: int = 10) -> List[Tuple]:
        all_articles = []
        url = self.base_url

        print(f"\nüì° ƒêang qu√©t c·∫•u tr√∫c Hot News Coin68...")
        html = self.fetch_html(url)
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')

        # 1. T√¨m t·∫•t c·∫£ c√°c kh·ªëi tin d·ª±a tr√™n class 'css-19idom' b·∫°n cung c·∫•p
        items = soup.find_all('div', class_='css-19idom')

        article_urls = []
        for item in items:
            # 2. T√¨m th·∫ª a ch·ª©a ti√™u ƒë·ªÅ (th·∫ª a n·∫±m trong div css-112x203 nh∆∞ m·∫´u c·ªßa b·∫°n)
            link_el = item.select_one('div.css-112x203 a')
            if link_el and link_el.get('href'):
                href = link_el.get('href')
                full_url = f"{self.base_url}{href}" if href.startswith('/') else href

                if full_url not in article_urls:
                    article_urls.append(full_url)

            if len(article_urls) >= max_articles:
                break

        print(f"‚úì T√¨m th·∫•y {len(article_urls)} b√†i vi·∫øt t·ª´ giao di·ªán Hot News.")

        # 3. L·∫•y chi ti·∫øt t·ª´ng b√†i
        for i, article_url in enumerate(article_urls, 1):
            print(f"[{i}/{len(article_urls)}] ƒêang c√†o: {article_url}", flush=True)
            self.sleep()
            data = self._fetch_article_detail(article_url)
            if data:
                all_articles.append(data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html_text = self.fetch_html(link)
        if not html_text: return None
        soup = BeautifulSoup(html_text, 'html.parser')

        # 1. L·∫•y Title - D√πng selector ·ªïn ƒë·ªãnh (h1 ho·∫∑c MuiTypography-h2)
        title = ""
        title_el = soup.find('h1')
        if title_el:
            title = title_el.get_text(strip=True)

        # 2. L·∫•y Category (D·ª±a tr√™n breadcrumbs)
        category = "CRYPTO"
        # T√¨m breadcrumb ch·ª©a link /article/ (ƒë√≥ l√† category)
        category_link = soup.select_one('.MuiBreadcrumbs-li a[href*="/article/"]')
        if category_link:
            span = category_link.find('span')
            if span:
                category = span.get_text(strip=True).upper()

        # 3. L·∫•y Published At - T√¨m th·∫ª span ch·ª©a ng√†y
        published_at = int(datetime.now().timestamp())
        # Th·ª≠ t√¨m span ch·ª©a pattern ng√†y DD/MM/YYYY
        for span in soup.find_all('span'):
            text = span.get_text(strip=True)
            if '/' in text and len(text) == 10:  # Format: DD/MM/YYYY
                try:
                    dt = datetime.strptime(text, "%d/%m/%Y")
                    published_at = int(dt.timestamp())
                    break
                except:
                    continue

        # 4. L·∫•y Content - D√πng div#content (kh√¥ng c·∫ßn class c·ª• th·ªÉ)
        paragraphs = []
        content_div = soup.find('div', id='content')

        if content_div:
            # Ch·ªâ l·∫•y text t·ª´ c√°c th·∫ª p, b·ªè qua c√°c th·∫ª script/iframe/ads
            for p in content_div.find_all('p', recursive=True):
                # Lo·∫°i b·ªè c√°c ƒëo·∫°n text ch·ª©a "·∫¢nh:", "Ngu·ªìn:", "C√≥ th·ªÉ b·∫°n quan t√¢m"
                txt = p.get_text(strip=True)
                if len(txt) > 30 and not any(x in txt for x in ["·∫¢nh:", "Ngu·ªìn:", "t·ªïng h·ª£p"]):
                    paragraphs.append(txt)

        content = "\n\n".join(paragraphs)

        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán cu·ªëi c√πng ƒë·ªÉ tr√°nh l∆∞u b√†i r·ªóng
        if not title or not content:
            return None

        return (published_at, title, link, content, self.source, "NA", "NA", False, category)
