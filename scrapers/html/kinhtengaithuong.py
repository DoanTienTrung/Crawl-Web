from scrapers.base import NewsScraperBase
from typing import List, Tuple, Optional
from bs4 import BeautifulSoup
from datetime import datetime
import re


class KinhTeNgoaiThuongScraper(NewsScraperBase):
    def __init__(self):
        super().__init__()
        self.source = "kinhtengoaithuong.vn"
        # Th√™m header ƒë·ªÉ gi·∫£ l·∫≠p tr√¨nh duy·ªát xem tin t·ª©c
        self.headers.update({
            'Referer': 'https://www.google.com/',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        })

    def fetch_news(self, max_articles: int = 15) -> List[Tuple]:
        all_articles = []
        url = "https://kinhtengoaithuong.vn/"

        print(f"\nüì° ƒêang qu√©t trang ch·ªß: {url}")
        html = self.fetch_html(url)

        if not html:
            print("‚ö† Kh√¥ng th·ªÉ truy c·∫≠p trang ch·ªß kinhtengoaithuong.vn")
            return []

        soup = BeautifulSoup(html, 'html.parser')
        potential_links = soup.select('h2 a, h3 a, .post-title a, .entry-title a')

        article_urls = []
        seen_urls = set()

        for a in potential_links:
            href = a.get('href', '')
            if not href: continue

            # Chu·∫©n h√≥a link tuy·ªát ƒë·ªëi
            if href.startswith('/'):
                href = f"https://kinhtengoaithuong.vn{href}"

            # L·ªçc: Ph·∫£i thu·ªôc domain, kh√¥ng ph·∫£i trang ch·ªß, kh√¥ng ph·∫£i link r√°c
            if "kinhtengoaithuong.vn" in href and href != "https://kinhtengoaithuong.vn/":
                # Lo·∫°i b·ªè c√°c trang ch·ª©c nƒÉng
                if not any(x in href for x in ['/category/', '/tag/', '/author/', '/contact/', '/gioi-thieu/', '/c/']):
                    if href not in seen_urls:
                        seen_urls.add(href)
                        article_urls.append(href)

        # 2. N·∫øu Selector tr√™n kh√¥ng ra k·∫øt qu·∫£, d√πng Regex ƒë·ªÉ qu√©t to√†n b·ªô link
        if not article_urls:
            print("üîç Th·ª≠ qu√©t link b·∫±ng ph∆∞∆°ng th·ª©c Regex...")
            for a in soup.find_all('a', href=True):
                href = a['href']
                # Link b√†i vi·∫øt th∆∞·ªùng c√≥ ƒë·ªô s√¢u path > 3 (v√≠ d·ª• domain.vn/ten-bai-viet/)
                if "kinhtengoaithuong.vn" in href and len(href.strip('/').split('/')) >= 3:
                     if href not in seen_urls and not any(x in href for x in ['/category/', '/tag/', '/c/']):
                        seen_urls.add(href)
                        article_urls.append(href)

        article_urls = article_urls[:max_articles]
        print(f"‚úì T√¨m th·∫•y {len(article_urls)} b√†i vi·∫øt t·ª´ trang ch·ªß.")

        for i, article_url in enumerate(article_urls, 1):
            print(f"[{i}/{len(article_urls)}] Fetching: {article_url[:50]}...", flush=True)
            self.sleep()
            article_data = self._fetch_article_detail(article_url)
            if article_data:
                all_articles.append(article_data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')

        # 1. Ti√™u ƒë·ªÅ
        title_el = soup.find('h1')
        title = title_el.get_text(strip=True) if title_el else ''
        if not title: return None

        # 2. Ng√†y xu·∫•t b·∫£n
        published_at = int(datetime.now().timestamp())
        # T√¨m trong c√°c th·∫ª meta ho·∫∑c class date ph·ªï bi·∫øn c·ªßa trang
        date_el = soup.select_one('.detail-date, .post-date, .time')
        if date_el:
            date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})', date_el.get_text())
            if date_match:
                d, m, y = date_match.groups()
                try: published_at = int(datetime(int(y), int(m), int(d)).timestamp())
                except: pass

        # 3. N·ªôi dung (S·ª≠ d·ª•ng Selector: .article-content)
        content_el = soup.select_one('.article-content')
        content = ""
        if content_el:
            # Thu th·∫≠p d·ªØ li·ªáu t·ª´ c·∫£ th·∫ª <p> v√† c√°c d√≤ng trong <table> (ch√∫ th√≠ch ·∫£nh)
            # ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng s√≥t th√¥ng tin quan tr·ªçng
            parts = []

            # L·∫•y c√°c ƒëo·∫°n vƒÉn b·∫£n
            for p in content_el.find_all(['p', 'td']):
                # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a v√† c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
                txt = p.get_text(strip=True)
                if txt and len(txt) > 5: # Ch·ªâ l·∫•y c√°c ƒëo·∫°n c√≥ nghƒ©a
                    parts.append(txt)

            content = ' '.join(parts)

        # 4. Chuy√™n m·ª•c (Category)
        category = "T√ÄI CH√çNH" 
        bread_items = soup.select('a[itemprop="item"]')
        if len(bread_items) >= 2:
            # L·∫•y text t·ª´ thu·ªôc t√≠nh title ho·∫∑c t·ª´ th·∫ª span b√™n trong
            category = bread_items[1].get('title') or bread_items[1].get_text(strip=True)

        category = category.upper().strip()

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA", 
            False,
            category
        )
