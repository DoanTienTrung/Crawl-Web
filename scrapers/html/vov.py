from scrapers.base import NewsScraperBase
from typing import List, Tuple, Optional
from bs4 import BeautifulSoup
from datetime import datetime


class VOVScraper(NewsScraperBase):
    """
    Scraper cho VOV.vn - crawl t·ª´ trang "Tin m·ªõi c·∫≠p nh·∫≠t"
    """

    def __init__(self):
        super().__init__()
        self.source = "vov.vn"
        self.headers['Referer'] = 'https://vov.vn/'
        self.delay = 7  # TƒÉng delay t·ª´ 3 l√™n 5 gi√¢y ƒë·ªÉ tr√°nh rate limit  

    def fetch_news(self, max_pages: int = 1) -> List[Tuple]:
        """
        Fetch tin t·ª©c t·ª´ VOV trang "Tin m·ªõi c·∫≠p nh·∫≠t"

        Args:
            max_pages: S·ªë trang t·ªëi ƒëa c·∫ßn crawl (m·∫∑c ƒë·ªãnh 1)
        """
        all_articles = []

        print(f"\nüì∞ Crawling VOV.vn  Tin m·ªõi c·∫≠p nh·∫≠t - multi_source_scraper.py:364")

        # Pagination: page 0, page 1, page 2, ...
        for page in range(max_pages):
            if page == 0:
                url = "https://vov.vn/tin-moi-cap-nhat"
            else:
                url = f"https://vov.vn/tin-moi-cap-nhat?page={page}"

            print(f"\n  üìÑ Page {page + 1}/{max_pages}: {url} - multi_source_scraper.py:373")
            self.sleep()

            html = self.fetch_html(url)
            if not html:
                print(f"‚ö† Failed to fetch page {page}, stopping - multi_source_scraper.py:378")
                break

            # Check for anti-bot redirect (VOV uses multiple levels of JavaScript redirects)
            max_redirects = 5
            redirect_count = 0
            import re as re_module
            from urllib.parse import unquote

            while ('Attention Required' in html or 'window.location.href' in html) and redirect_count < max_redirects:
                redirect_count += 1
                print(f"‚ö† Antibot detected (level {redirect_count}), extracting redirect URL... - multi_source_scraper.py:389")

                # Extract redirect URL from JavaScript
                match = re_module.search(r'window\.location\.href\s*=\s*["\']([^"\']+)["\']', html)
                if match:
                    redirect_url = match.group(1)
                    redirect_url = unquote(redirect_url)
                    print(f"‚Üí Redirecting to: {redirect_url[:80]}... - multi_source_scraper.py:396")

                    # Fetch the redirect URL
                    self.sleep()
                    html = self.fetch_html(redirect_url)
                    if not html:
                        print(f"‚ö† Failed to fetch redirect URL, stopping - multi_source_scraper.py:402")
                        break
                    print(f"DEBUG: After redirect {redirect_count}, HTML length: {len(html)} - multi_source_scraper.py:404")
                else:
                    print(f"‚ö† Could not extract redirect URL, stopping - multi_source_scraper.py:406")
                    break

            # If we still have anti-bot page after max redirects, stop
            if redirect_count >= max_redirects and 'Attention Required' in html:
                print(f"‚ö† Max redirects ({max_redirects}) reached, still getting antibot page. Stopping. - multi_source_scraper.py:411")
                break

            soup = BeautifulSoup(html, 'html.parser')

            # Select taxonomy-content divs
            content_divs = soup.select('div.taxonomy-content')

            # Debug: Show what we found
            print(f"DEBUG: Found {len(content_divs)} div.taxonomycontent - multi_source_scraper.py:420")

            if not content_divs:
                print(f"‚ö† No articles found on page {page}, stopping - multi_source_scraper.py:423")
                # Debug: Try alternative selector
                alt_divs = soup.select('.card')
                print(f"DEBUG: Alternative .card selector found {len(alt_divs)} elements - multi_source_scraper.py:426")
                break

            print(f"Found {len(content_divs)} articles on page {page} - multi_source_scraper.py:429")

            for div in content_divs:
                try:
                    # Extract title
                    title_el = div.select_one('h5.media-title') or div.select_one('h3.card-title')
                    title = title_el.get_text(strip=True) if title_el else ''

                    # Extract link
                    link_el = div.select_one('a.vovvn-title')
                    href = link_el.get('href', '') if link_el else ''
                    link = href if href.startswith('http') else f"https://vov.vn{href}"

                    # Extract description
                    desc_el = div.select_one('p.mt-2')
                    description = desc_el.get_text(strip=True) if desc_el else ''

                    if not title or not link:
                        continue

                    # Fetch article detail with retry if content is empty
                    self.sleep()
                    article_data = self._fetch_article_detail(link, title, description)

                    # Retry once if content is empty
                    if article_data and not article_data[3]:  # article_data[3] is content
                        print(f"‚ö† Empty content, retrying: {title[:50]}...")
                        self.sleep()
                        article_data = self._fetch_article_detail(link, title, description)

                    if article_data:
                        all_articles.append(article_data)

                except Exception as e:
                    print(f"‚úó Error parsing article: {e} - multi_source_scraper.py:456")
                    continue

            # Check pagination ƒë·ªÉ xem c√≥ trang ti·∫øp theo kh√¥ng
            pagination = soup.select_one('ul.pagination')
            if not pagination:
                print(f"‚ö† No pagination found, stopping - multi_source_scraper.py:462")
                break

        print(f"\n  ‚úì Total articles collected: {len(all_articles)} - multi_source_scraper.py:465")
        return all_articles

    def _fetch_article_detail(self, link: str, title: str, description: str) -> Optional[Tuple]:
        """Fetch chi ti·∫øt m·ªôt b√†i b√°o"""
        html = self.fetch_html(link)
        if not html:
            print(f"‚úó Failed to fetch HTML for: {link}")
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # Extract date
        # Format m·ªõi: "Th·ª© Hai, 16:54, 29/12/2025" trong div.col-md-4.mb-2
        # Format c≈©: "Th·ª© Ba, 22:35, 26/08/2025" trong .article-date .col-md-4
        published_at = int(datetime.now().timestamp())  # Default to now

        # Th·ª≠ selector m·ªõi tr∆∞·ªõc
        date_el = soup.select_one('div.col-md-4.mb-2') or soup.select_one('.article-date .col-md-4')

        if date_el:
            date_text = date_el.get_text(strip=True)
            # Format: "Th·ª© Hai, 16:54, 29/12/2025"
            parts = [p.strip() for p in date_text.split(',')]
            if len(parts) >= 3:
                time_part = parts[1]  # "16:54"
                date_part = parts[2]  # "29/12/2025"
                datetime_str = f"{date_part} {time_part}"
                parsed_ts = self.parse_date_to_timestamp(datetime_str, "%d/%m/%Y %H:%M")
                if parsed_ts > 0:
                    published_at = parsed_ts

        # Extract content
        content_parts = []

        # 1. L·∫•y summary/lead t·ª´ div.article-summary
        summary_el = soup.select_one('div.row.article-summary div.col-lg-8 > div')
        if summary_el:
            summary_text = summary_el.get_text(strip=True)
            if summary_text:
                content_parts.append(summary_text)

        # 2. L·∫•y content t·ª´ div.article-content
        # Th·ª≠ nhi·ªÅu selector kh√°c nhau v√¨ c·∫•u tr√∫c HTML c√≥ th·ªÉ kh√°c nhau
        content_el = (
            soup.select_one('div.row.article-content div.text-long') or
            soup.select_one('div.article-content div.text-long') or
            soup.select_one('div.text-long')
        )

        if content_el:
            # Th·ª≠ l·∫•y t·ª´ th·∫ª <p> tr∆∞·ªõc (cho b√†i b√°o th∆∞·ªùng)
            paragraphs = content_el.select('p')
            p_text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

            if p_text and p_text.strip():
                content_parts.append(p_text)
            else:
                # N·∫øu kh√¥ng c√≥ <p>, l·∫•y t·ª´ <figcaption> (cho b√†i multimedia/gallery)
                figcaptions = content_el.select('figure figcaption')
                fig_text = ' '.join([fig.get_text(strip=True) for fig in figcaptions if fig.get_text(strip=True)])
                if fig_text:
                    content_parts.append(fig_text)

        # K·∫øt h·ª£p t·∫•t c·∫£ ph·∫ßn content
        content = ' '.join(content_parts) if content_parts else description

        # Extract category
        category = "Tin t·ª©c"  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh

        # ∆Øu ti√™n 1: L·∫•y t·ª´ navbar chuy√™n m·ª•c
        # C·∫•u tr√∫c: <a class="navbar-brand special-header-title" ...>
        nav_category = soup.select_one('a.special-header-title')

        # ∆Øu ti√™n 2: L·∫•y t·ª´ breadcrumb
        # C·∫•u tr√∫c: li.breadcrumb-item-first a
        breadcrumb_category = soup.select_one('li.breadcrumb-item-first a, .breadcrumb-item a')

        if nav_category:
            category = nav_category.get_text(strip=True)
        elif breadcrumb_category:
            category = breadcrumb_category.get_text(strip=True)

        # Chu·∫©n h√≥a: N·∫øu l·∫•y tr√∫ng ch·ªØ "Trang ch·ªß" th√¨ ƒë·∫∑t l·∫°i m·∫∑c ƒë·ªãnh
        if category.lower() in ['trang ch·ªß', 'home', 'vov.vn', 'vov']:
            category = "Tin t·ª©c"

        # Chuy·ªÉn th√†nh ch·ªØ hoa ƒë·ªÉ ƒë·ªìng b·ªô d·ªØ li·ªáu
        category = category.upper()

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA",
            False,
            category,
        )
