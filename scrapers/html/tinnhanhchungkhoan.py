from scrapers.base import NewsScraperBase
from typing import List, Tuple, Optional
from bs4 import BeautifulSoup
from datetime import datetime
import copy
import re


class TinNhanhChungKhoanScraper(NewsScraperBase):
    """
    Scraper cho tinnhanhchungkhoan.vn
    L·∫•y 10 b√†i vi·∫øt m·ªõi nh·∫•t t·ª´ trang ch·ªß
    """
    def __init__(self):
        super().__init__()
        self.source = "tinnhanhchungkhoan.vn"
        self.headers.update({
            'Referer': 'https://www.tinnhanhchungkhoan.vn/',
        })

    def fetch_news(self, max_articles: int = 15) -> List[Tuple]:
        """L·∫•y b√†i vi·∫øt m·ªõi nh·∫•t t·ª´ trang ch·ªß"""
        all_articles = []
        url = "https://www.tinnhanhchungkhoan.vn/"

        print(f"\nüì° ƒêang qu√©t Tin nhanh ch·ª©ng kho√°n: {url}")
        html = self.fetch_html(url)
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')

        # T√¨m c√°c link b√†i vi·∫øt tr√™n trang ch·ªß
        article_urls = []
        seen_urls = set()

        # T√¨m c√°c link b√†i vi·∫øt (th·ª≠ nhi·ªÅu selector ph·ªï bi·∫øn)
        links = soup.select('article a, .news-item a, .article-link, h2 a, h3 a, .cms-link a')

        for link in links:
            href = link.get('href', '')
            if not href:
                continue

            # Chu·∫©n h√≥a URL
            if not href.startswith('http'):
                if href.startswith('/'):
                    href = f"https://www.tinnhanhchungkhoan.vn{href}"
                else:
                    href = f"https://www.tinnhanhchungkhoan.vn/{href}"

            # Ch·ªâ l·∫•y c√°c link b√†i vi·∫øt t·ª´ domain tinnhanhchungkhoan.vn
            # v√† tr√°nh c√°c link menu, category, tag
            if 'tinnhanhchungkhoan.vn' in href and href not in seen_urls:
                # B·ªè qua c√°c link kh√¥ng ph·∫£i b√†i vi·∫øt
                skip_patterns = ['/tag/', '/author/', '/category/', '#', 'javascript:']
                if any(pattern in href for pattern in skip_patterns):
                    continue

                seen_urls.add(href)
                article_urls.append(href)

                if len(article_urls) >= max_articles:
                    break

        print(f"T√¨m th·∫•y {len(article_urls)} b√†i vi·∫øt")

        # Fetch chi ti·∫øt t·ª´ng b√†i
        for i, article_url in enumerate(article_urls, 1):
            print(f"[{i}/{len(article_urls)}] Fetching: {article_url[:70]}...", flush=True)
            self.sleep()
            article_data = self._fetch_article_detail(article_url)
            if article_data:
                all_articles.append(article_data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        """L·∫•y chi ti·∫øt m·ªôt b√†i vi·∫øt"""
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # 1. Title - <h1 class="article__header cms-title">
        title_el = soup.select_one('h1.article__header.cms-title')
        if not title_el:
            # Fallback: th·ª≠ selector kh√°c
            title_el = soup.select_one('h1.cms-title')
        if not title_el:
            return None
        title = title_el.get_text(strip=True)

        # 2. Published_at - <time class="time" datetime="..." data-time="1767315611">
        published_at = int(datetime.now().timestamp())  # Default
        time_el = soup.select_one('time.time')
        if time_el:
            # ∆Øu ti√™n d√πng data-time v√¨ n√≥ l√† Unix timestamp s·∫µn
            data_time = time_el.get('data-time', '')
            if data_time and data_time.isdigit():
                published_at = int(data_time)
            else:
                # Fallback: parse datetime attribute
                datetime_str = time_el.get('datetime', '')
                if datetime_str:
                    try:
                        # Format: "2026-01-02T08:00:11+0700"
                        # Lo·∫°i b·ªè timezone offset ƒë·ªÉ parse
                        datetime_str_clean = re.sub(r'[+-]\d{4}$', '', datetime_str)
                        dt = datetime.fromisoformat(datetime_str_clean)
                        published_at = int(dt.timestamp())
                    except:
                        pass

        # 3. Category - <li class="main-cate"><a title="...">
        category = "Ch·ª©ng kho√°n"  # Default
        cat_el = soup.select_one('li.main-cate a')
        if cat_el:
            category = cat_el.get_text(strip=True)

        # 4. Content - sapo + body
        content_parts = []

        # Sapo
        sapo_el = soup.select_one('div.article__sapo.cms-desc')
        if sapo_el:
            content_parts.append(sapo_el.get_text(strip=True))

        # Body
        body_el = soup.select_one('div.article__body.cms-body')
        if body_el:
            # Lo·∫°i b·ªè ads v√† script
            body_copy = copy.copy(body_el)
            for noise in body_copy.select('.ads_middle, script, style, .banner'):
                noise.decompose()

            # L·∫•y text t·ª´ c√°c th·∫ª p, h2, h3
            paragraphs = body_copy.find_all(['p', 'h2', 'h3'])
            text_parts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
            content_parts.extend(text_parts)

        content = ' '.join(content_parts).strip()

        if len(content) < 50:  # B√†i vi·∫øt qu√° ng·∫Øn, b·ªè qua
            return None

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA",
            False,
            category,
        )
