from scrapers.base import NewsScraperBase
from typing import List, Tuple, Optional
from bs4 import BeautifulSoup
from datetime import datetime
import copy


class ThoiBaoNganHangScraper(NewsScraperBase):
    def __init__(self):
        super().__init__()
        self.source = "thoibaonganhang.vn"
        self.headers.update({
            'Referer': 'https://thoibaonganhang.vn/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

    def fetch_news(self, max_articles: int = 15) -> List[Tuple]:
        all_articles = []
        url = "https://thoibaonganhang.vn/"

        print(f"\nüì° ƒêang qu√©t trang ch·ªß Th·ªùi b√°o Ng√¢n h√†ng: {url}")
        html = self.fetch_html(url)
        if not html: return []

        soup = BeautifulSoup(html, 'html.parser')
        article_urls = []
        seen_urls = set()

        # Qu√©t t·∫•t c·∫£ link b√†i vi·∫øt c√≥ ƒëu√¥i .html v√† ch·ª©a s·ªë ID
        links = soup.find_all('a', href=True)
        for link in links:
            href = link.get('href', '')
            if ".html" in href and any(char.isdigit() for char in href):
                if not href.startswith('http'):
                    href = f"https://thoibaonganhang.vn{href}"

                if not any(x in href for x in ['/video-', '/anh-', '/chuyen-muc/', '/tags/']):
                    if href not in seen_urls:
                        seen_urls.add(href)
                        article_urls.append(href)

        article_urls = article_urls[:max_articles]
        print(f"‚úì T√¨m th·∫•y {len(article_urls)} b√†i vi·∫øt ti·ªÅm nƒÉng.")

        for i, article_url in enumerate(article_urls, 1):
            print(f"[{i}/{len(article_urls)}] Fetching: {article_url[:60]}...")
            self.sleep() # Kh√¥ng truy·ªÅn tham s·ªë ƒë·ªÉ tr√°nh l·ªói NewsScraperBase
            article_data = self._fetch_article_detail(article_url)
            if article_data:
                all_articles.append(article_data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')

        # 1. Ti√™u ƒë·ªÅ
        title_el = soup.find('h1')
        if not title_el: return None
        title = title_el.get_text(strip=True)

        # 2. Ng√†y xu·∫•t b·∫£n (.format_date)
        published_at = int(datetime.now().timestamp())
        date_el = soup.select_one('.format_date')
        if date_el:
            try:
                dt = datetime.strptime(date_el.get_text(strip=True), '%d/%m/%Y')
                published_at = int(dt.timestamp())
            except: pass

        # 3. Chuy√™n m·ª•c (.bx-cat-link)
        category = "NG√ÇN H√ÄNG"
        cat_el = soup.select_one('.bx-cat-link')
        if cat_el:
            category = cat_el.get_text(strip=True).upper()

        # 4. N·ªôi dung (G·ªôp Sapo + Body)
        content = ""
        container = soup.select_one('.article-detail-body')
        if container:
            content_box = copy.copy(container)

            # L·∫•y ph·∫ßn Sapo (T√≥m t·∫Øt)
            sapo_el = content_box.select_one('.article-detail-desc')
            sapo_text = sapo_el.get_text(strip=True) if sapo_el else ""

            # Lo·∫°i b·ªè r√°c tr∆∞·ªõc khi l·∫•y Body
            for noise in content_box.select('.article-share-button, .article-extension, script, style, .article-detail-desc'):
                noise.decompose()

            # L·∫•y c√°c ƒëo·∫°n vƒÉn b·∫£n ch√≠nh
            paragraphs = content_box.find_all('p')
            if paragraphs:
                body_text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            else:
                body_text = content_box.get_text(" ", strip=True)

            # K·∫øt h·ª£p Sapo v√† Body
            content = f"{sapo_text} {body_text}".strip()

        if not content or len(content) < 50:
            return None


        return (published_at, title, link, content, self.source, "NA", "NA", False, category)
