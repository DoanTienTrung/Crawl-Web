from scrapers.base import NewsScraperBase
from typing import List, Tuple, Optional
from bs4 import BeautifulSoup
from datetime import datetime


class VietnametScraper(NewsScraperBase):
    """
    Scraper cho Vietnamnet.vn
    T∆∞∆°ng t·ª± h√†m fetch_vietnamnet_news trong Rust
    """

    def __init__(self):
        super().__init__()
        self.source = "vietnamnet.vn"
        self.headers['Referer'] = 'https://vietnamnet.vn/'

    def fetch_news(self, max_pages: int = 1, target_date: str = None) -> List[Tuple]:
        """
        Fetch tin t·ª©c t·ª´ Vietnamnet (tin t·ª©c 24h by date)

        Args:
            max_pages: S·ªë trang t·ªëi ƒëa c·∫ßn crawl (m·∫∑c ƒë·ªãnh 1)
            target_date: Ng√†y c·∫ßn crawl theo format 'dd/mm/yyyy'. N·∫øu None, d√πng ng√†y hi·ªán t·∫°i
        """
        all_articles = []

        # Get date for the bydate parameter
        if target_date:
            date_str = target_date
        else:
            today = datetime.now()
            date_str = today.strftime("%d/%m/%Y")

        print(f"\nüìÖ Crawling Vietnamnet for date: {date_str} - multi_source_scraper.py:560")

        # Start from page 0
        page = 0

        while True:
            # Build URL with date filter
            url = f"https://vietnamnet.vn/tin-tuc-24h-p{page}?bydate={date_str}-{date_str}&cate="

            print(f"\n  üìÑ Page {page}: {url} - multi_source_scraper.py:569")
            self.sleep()

            html = self.fetch_html(url)
            if not html:
                print(f"‚ö† Failed to fetch page {page}, stopping - multi_source_scraper.py:574")
                break

            soup = BeautifulSoup(html, 'html.parser')

            # Select posts
            posts = soup.select('div.horizontalPost.version-news')

            if not posts:
                print(f"‚ö† No articles found on page {page}, stopping - multi_source_scraper.py:583")
                break

            print(f"Found {len(posts)} articles on page {page} - multi_source_scraper.py:586")

            for post in posts:
                try:
                    # Extract title and link
                    title_el = post.select_one('h3.horizontalPost__main-title a')
                    if not title_el:
                        continue

                    title = title_el.get_text(strip=True)
                    href = title_el.get('href', '')
                    link = href if href.startswith('http') else f"https://vietnamnet.vn{href}"

                    if not title or not link:
                        continue

                    # Fetch article detail
                    self.sleep()
                    article_data = self._fetch_article_detail(link, title)
                    if article_data:
                        all_articles.append(article_data)

                except Exception as e:
                    print(f"‚úó Error parsing article: {e} - multi_source_scraper.py:609")
                    continue

            # Check if we should continue to next page
            if max_pages is not None and page >= max_pages - 1:
                print(f"Reached max_pages limit ({max_pages}) - multi_source_scraper.py:614")
                break

            # Check if there's a next page by reading pagination numbers
            pagination = soup.select_one('div.pagination ul.pagination__list')
            if not pagination:
                print(f"No pagination found, stopping - multi_source_scraper.py:620")
                break

            # Find all numbered page links (excluding the pagination-next button)
            page_items = pagination.select('li.pagination__list-item:not(.pagination-next) a')
            page_numbers = []

            for item in page_items:
                page_text = item.get_text(strip=True)
                if page_text.isdigit():
                    page_numbers.append(int(page_text))

            if page_numbers:
                max_page_num = max(page_numbers)
                print(f"Pagination detected: pages 1{max_page_num} (current: page {page + 1}) - multi_source_scraper.py:634")

                # Current page is 0-indexed, but display is 1-indexed
                # If we're at the last page, stop
                if page + 1 >= max_page_num:
                    print(f"Reached the last page ({page + 1}/{max_page_num}) - multi_source_scraper.py:639")
                    break
            else:
                print(f"No page numbers found in pagination, stopping - multi_source_scraper.py:642")
                break

            # Move to next page
            page += 1

        print(f"\n  ‚úì Total articles collected: {len(all_articles)} from {page + 1} page(s) - multi_source_scraper.py:648")
        return all_articles

    def _fetch_article_detail(self, link: str, title: str) -> Optional[Tuple]:
        """Fetch chi ti·∫øt m·ªôt b√†i b√°o"""
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # Extract date
        date_el = soup.select_one('div.bread-crumb-detail__time') or soup.select_one('span.time')
        published_at = 0

        if date_el:
            date_text = date_el.get_text(strip=True)

            # Clean date string
            # Format: "Th·ª© S√°u, 26/12/2025 - 22:10" ‚Üí "26/12/2025 22:10"
            # Remove "Th·ª© X, " ·ªü ƒë·∫ßu
            if date_text.startswith('Th·ª©'):
                parts = date_text.split(',', 1)
                if len(parts) > 1:
                    date_text = parts[1].strip()

            # Remove d·∫•u " - " gi·ªØa date v√† time
            date_text = date_text.replace(' - ', ' ')

            # Try various formats
            for fmt in ["%d/%m/%Y %H:%M", "%d-%m-%Y %H:%M", "%d/%m/%Y"]:
                parsed_ts = self.parse_date_to_timestamp(date_text, fmt)
                if parsed_ts > 0:
                    published_at = parsed_ts
                    break

        # Extract content
        content_el = soup.select_one('div.maincontent') or soup.select_one('div.article-content')
        content = ""

        if content_el:
            paragraphs = content_el.select('p')
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # Extract category t·ª´ breadcrumb
        category = "Tin t·ª©c"  # Default

        # Th·ª≠ t√¨m t·ª´ breadcrumb - th∆∞·ªùng category l√† item th·ª© 2 (sau "Trang ch·ªß")
        breadcrumb_links = soup.select('ul.breadcrumb li a, .breadcrumb a, .bread-crumb a')
        if len(breadcrumb_links) >= 2:
            # Item ƒë·∫ßu ti√™n th∆∞·ªùng l√† "Trang ch·ªß", item th·ª© 2 l√† category
            category_el = breadcrumb_links[1]
            category_text = category_el.get_text(strip=True)
            if category_text and category_text.lower() not in ['trang ch·ªß', 'home', 'vietnamnet']:
                category = category_text
            # N·∫øu kh√¥ng c√≥ text, th·ª≠ l·∫•y t·ª´ title attribute
            elif not category_text:
                category = category_el.get('title', category).strip()

        # Fallback: T√¨m link category g·∫ßn khu v·ª±c date (n·∫øu breadcrumb kh√¥ng c√≥)
        if category == "Tin t·ª©c":
            # T√¨m trong parent c·ªßa bread-crumb-detail__time
            date_parent = soup.select_one('div.bread-crumb-detail__time')
            if date_parent and date_parent.parent:
                nearby_links = date_parent.parent.select('a[title]')
                for link_el in nearby_links:
                    href = link_el.get('href', '')
                    # Category links th∆∞·ªùng c√≥ href ng·∫Øn nh∆∞ "/thoi-su", "/kinh-doanh"
                    if href.startswith('/') and href.count('/') == 1 and len(href) < 30:
                        category = link_el.get_text(strip=True) or link_el.get('title', category)
                        break

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA",
            False,
            category,
        )
