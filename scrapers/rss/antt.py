from scrapers.base import NewsScraperBase
from typing import List, Tuple, Optional
from bs4 import BeautifulSoup
from datetime import datetime
import feedparser
import re


class ANTTRSSScraper(NewsScraperBase):
    """
    Scraper cho ANTT.vn sá»­ dá»¥ng RSS Feed
    """

    def __init__(self):
        super().__init__()
        self.source = "antt.vn"
        self.rss_url = "https://antt.vn/rss/trang-chu.rss"

    def fetch_news(self) -> List[Tuple]:
        """
        Láº¥y tá»‘i Ä‘a 20 tin tá»©c má»›i nháº¥t tá»« RSS feed cá»§a ANTT
        """
        all_articles = []
        print(f"\nğŸ“¡ Äang Ä‘á»c RSS tá»«: {self.rss_url}")

        feed = feedparser.parse(self.rss_url)

        if not feed.entries:
            print("âš  KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t nÃ o trong RSS.")
            return []

        entries_to_process = feed.entries[:20]
        print(f"TÃ¬m tháº¥y {len(feed.entries)} bÃ i. Sáº½ xá»­ lÃ½ {len(entries_to_process)} bÃ i má»›i nháº¥t.")

        for entry in entries_to_process:
            link = entry.link

            # RSS cá»§a ANTT thÆ°á»ng khÃ´ng cÃ³ tháº» <category> trá»±c tiáº¿p cho tá»«ng entry nhÆ° DÃ¢n TrÃ­
            # Ta sáº½ máº·c Ä‘á»‹nh lÃ  'TIN Má»šI' vÃ  Ä‘á»ƒ hÃ m detail láº¥y tá»« Meta Tag
            rss_category = getattr(entry, 'category', 'TIN Má»šI')

            print(f"Fetching: {link[:60]}...")
            self.sleep() # Äáº£m báº£o khÃ´ng crawl quÃ¡ nhanh

            article_data = self._fetch_article_detail(link, rss_category)
            if article_data:
                all_articles.append(article_data)

        print(f"\nâœ“ Tá»•ng sá»‘ bÃ i viáº¿t tá»« ANTT thu tháº­p Ä‘Æ°á»£c: {len(all_articles)}")
        return all_articles

    def _fetch_article_detail(self, link: str, rss_category: str = None) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o tá»« ANTT"""
        # Fix: ANTT RSS tráº£ vá» URLs dáº¡ng "-nXXXXXX.html" nhÆ°ng website dÃ¹ng "-XXXXXX.htm"
        # Cáº§n bá» chá»¯ 'n' trÆ°á»›c sá»‘ vÃ  Ä‘á»•i extension
        import re
        original_link = link
        if link.endswith('.html'):
            # Thay -nXXXXXX.html thÃ nh -XXXXXX.htm
            link = re.sub(r'-n(\d+)\.html$', r'-\1.htm', link)
        elif link.endswith('.htm'):
            # Náº¿u Ä‘Ã£ lÃ  .htm nhÆ°ng váº«n cÃ³ 'n', thÃ¬ bá» 'n'
            link = re.sub(r'-n(\d+)\.htm$', r'-\1.htm', link)

        if link != original_link:
            print(f"  â†’ URL fixed: ...{original_link[-50:]} â†’ ...{link[-50:]}")

        # ANTT server tráº£ vá» encoding sai (ISO-8859-1) nhÆ°ng content lÃ  UTF-8
        # Cáº§n fetch vá»›i explicit UTF-8 encoding
        try:
            resp = self.session.get(link, headers=self.headers, timeout=30)
            resp.raise_for_status()
            # Force UTF-8 decoding
            html = resp.content.decode('utf-8', errors='replace')
        except Exception as e:
            print(f"âœ— Error fetching {link}: {e}")
            return None

        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # 1. TrÃ­ch xuáº¥t TiÃªu Ä‘á» (ANTT dÃ¹ng div.title_detail)
        title_el = soup.select_one('.title_detail') or soup.select_one('h1')
        title = title_el.get_text(strip=True) if title_el else ''

        if not title:
            print(f"  âš  No title found for {link[-60:]}")
            return None

        # 2. TrÃ­ch xuáº¥t NgÃ y xuáº¥t báº£n
        # ANTT dÃ¹ng format: "02/01/2026 11:02:40"
        published_at = 0
        date_el = soup.select_one('.time_home')
        if date_el:
            date_text = date_el.get_text(strip=True)
            # Format: dd/mm/yyyy hh:mm:ss
            date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})\s+(\d{1,2}):(\d{2}):(\d{2})', date_text)
            if date_match:
                day, month, year, hour, minute, second = date_match.groups()
                try:
                    dt = datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))
                    published_at = int(dt.timestamp())
                except:
                    pass

        # 3. TrÃ­ch xuáº¥t Ná»™i dung
        content = ""
        # ANTT: content náº±m trong cÃ¡c tháº» p trá»±c tiáº¿p trong content_main
        content_el = soup.select_one('.content_main')
        if content_el:
            # Loáº¡i bá» cÃ¡c thÃ nh pháº§n khÃ´ng cáº§n thiáº¿t
            for unwanted in content_el.select('.related-box, .ad-container, script, style, .tag_detail, .article-footer'):
                unwanted.decompose()

            paragraphs = content_el.select('p')
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # 4. TrÃ­ch xuáº¥t ChuyÃªn má»¥c
        # ANTT: category náº±m trong breadcrumb (item thá»© 2)
        category = "TIN Má»šI"

        # TÃ¬m breadcrumb items
        breadcrumb_items = soup.select('a[itemprop="url"] span[itemprop="title"]')

        # Láº¥y item thá»© 2 (index 1) - Ä‘Ã³ lÃ  category chÃ­nh
        # Item 1: Trang chá»§, Item 2: Category, Item 3: Sub-category
        if len(breadcrumb_items) >= 2:
            category = breadcrumb_items[1].get_text(strip=True)

        category = category.upper().strip()

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",   
            "NA",  
            False, 
            category,
        )
