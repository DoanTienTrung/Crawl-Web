from scrapers.base import NewsScraperBase
from typing import List, Tuple, Optional
from bs4 import BeautifulSoup
from datetime import datetime
import feedparser


class CNARSSScraper(NewsScraperBase):
    """
    Scraper cho ChannelNewsAsia.com s·ª≠ d·ª•ng RSS Feed
    """

    def __init__(self):
        super().__init__()
        self.source = "channelnewsasia.com"
        # Link RSS ch√≠nh th·ª©c c·ªßa CNA
        self.rss_url = "https://www.channelnewsasia.com/api/v1/rss-outbound-feed?_format=xml"

    def fetch_news(self) -> List[Tuple]:
        """
        L·∫•y t·ªëi ƒëa 20 tin t·ª©c m·ªõi nh·∫•t t·ª´ RSS feed c·ªßa CNA
        """
        all_articles = []
        print(f"\nüì° ƒêang ƒë·ªçc RSS t·ª´: {self.rss_url}")

        # 1. S·ª≠ d·ª•ng feedparser ƒë·ªÉ ƒë·ªçc n·ªôi dung RSS
        feed = feedparser.parse(self.rss_url)

        if not feed.entries:
            print("‚ö† Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o trong RSS.")
            return []

        # 2. Gi·ªõi h·∫°n 20 b√†i vi·∫øt ƒë·∫ßu ti√™n
        entries_to_process = feed.entries[:20]
        print(f"T√¨m th·∫•y {len(feed.entries)} b√†i. S·∫Ω x·ª≠ l√Ω {len(entries_to_process)} b√†i m·ªõi nh·∫•t.")

        for entry in entries_to_process:
            link = entry.link

            # L·∫•y Category (CNA th∆∞·ªùng ƒë·ªÉ trong tags ho·∫∑c category field c·ªßa RSS)
            rss_category = "WORLD" # M·∫∑c ƒë·ªãnh cho CNA
            if hasattr(entry, 'tags'):
                rss_category = entry.tags[0].term if entry.tags else "ASIA"

            # L·∫•y timestamp tr·ª±c ti·∫øp t·ª´ RSS (CNA h·ªó tr·ª£ c·ª±c t·ªët ph·∫ßn n√†y)
            published_at = 0
            if hasattr(entry, 'published_parsed'):
                published_at = int(datetime(*entry.published_parsed[:6]).timestamp())

            print(f"Fetching: {link[:60]}...")
            self.sleep()

            # Truy·ªÅn c√°c th√¥ng tin ƒë√£ c√≥ v√†o h√†m detail
            article_data = self._fetch_article_detail(link, rss_category, published_at)
            if article_data:
                all_articles.append(article_data)

        print(f"\n‚úì T·ªïng s·ªë b√†i vi·∫øt CNA thu th·∫≠p ƒë∆∞·ª£c: {len(all_articles)}")
        return all_articles

    def _fetch_article_detail(self, link: str, rss_category: str = None, rss_published_at: int = 0) -> Optional[Tuple]:
        """Fetch chi ti·∫øt m·ªôt b√†i b√°o t·ª´ CNA"""
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # 1. Tr√≠ch xu·∫•t Ti√™u ƒë·ªÅ (CNA th∆∞·ªùng d√πng class h1.entry-title ho·∫∑c class li√™n quan ƒë·∫øn content)
        title_el = soup.select_one('h1.page-title') or soup.select_one('h1')
        title = title_el.get_text(strip=True) if title_el else ''

        if not title:
            return None

        # 2. Tr√≠ch xu·∫•t Ng√†y xu·∫•t b·∫£n (∆Øu ti√™n l·∫•y t·ª´ RSS ƒë√£ c√≥ ·ªü b∆∞·ªõc tr∆∞·ªõc)
        published_at = rss_published_at

        # 3. Tr√≠ch xu·∫•t N·ªôi dung
        content = ""
        # Selector cho n·ªôi dung b√†i vi·∫øt c·ªßa CNA (th∆∞·ªùng l√† div ch·ª©a text-long ho·∫∑c c√°c th·∫ª p trong article)
        content_el = soup.select_one('.content-wrapper') or soup.select_one('.text-long')
        if content_el:
            # Lo·∫°i b·ªè c√°c th√†nh ph·∫ßn r√°c nh∆∞ "Also read", Video player, Ads
            for unwanted in content_el.select('.related-section, .video-embed, .ad-slot, .infographic'):
                unwanted.decompose()

            paragraphs = content_el.select('p')
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # 4. Tr√≠ch xu·∫•t Chuy√™n m·ª•c
        category = rss_category if rss_category else "WORLD"
        category = category.upper().strip()

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",  
            "NA",  
            False, 
            category,
        )
