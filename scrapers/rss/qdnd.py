from scrapers.base import NewsScraperBase
from typing import List, Tuple, Optional
from bs4 import BeautifulSoup
from datetime import datetime
import feedparser
import re


class QDNDRSSScraper(NewsScraperBase):
    def __init__(self):
        super().__init__()
        self.source = "qdnd.vn"
        self.rss_url = "https://www.qdnd.vn/rss/cate/tin-tuc-moi-nhat.rss"

    def fetch_news(self, max_articles: int = 10) -> List[Tuple]:
        all_articles = []

        print(f"\nðŸ“¡ Äang Ä‘á»c RSS tá»«: {self.rss_url}")

        # 1. Fetch RSS vá»›i requests (vÃ¬ feedparser trá»±c tiáº¿p bá»‹ cháº·n bá»Ÿi redirect)
        try:
            import requests
            response = requests.get(self.rss_url, headers=self.headers, timeout=15)
            response.raise_for_status()

            # Parse RSS content báº±ng feedparser
            feed = feedparser.parse(response.text)
        except Exception as e:
            print(f"âš  Lá»—i khi fetch RSS: {e}")
            return []

        if not feed.entries:
            print("âš  KhÃ´ng thá»ƒ láº¥y dá»¯ liá»‡u tá»« RSS.")
            return []

        # 2. Láº¥y danh sÃ¡ch cÃ¡c link bÃ i viáº¿t
        article_links = []
        for entry in feed.entries:
            link = entry.link
            if link not in article_links:
                article_links.append(link)
            if len(article_links) >= max_articles:
                break

        print(f"âœ“ TÃ¬m tháº¥y {len(article_links)} bÃ i viáº¿t má»›i tá»« RSS.")

        # 3. Duyá»‡t tá»«ng bÃ i Ä‘á»ƒ cÃ o ná»™i dung chi tiáº¿t
        for i, link in enumerate(article_links, 1):
            print(f"[{i}/{len(article_links)}] Äang cÃ o: {link}", flush=True)
            self.sleep()
            data = self._fetch_article_detail(link)
            if data:
                all_articles.append(data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')

        # 1. Title: BÃ¡o QDND dÃ¹ng class post-title
        title_el = soup.select_one('h1.post-title')
        title = title_el.get_text(strip=True) if title_el else ""

        # 2. Category: Láº¥y tá»« breadcrumb (link Ä‘áº§u tiÃªn)
        category = "MILITARY"
        # TÃ¬m link Ä‘áº§u tiÃªn trong breadcrumb vá»›i rel="v:url" vÃ  property="v:title"
        cate_el = soup.find('a', rel='v:url', property='v:title')
        if cate_el:
            category = cate_el.get_text(strip=True).upper()

        # 3. Published At: Láº¥y tá»« class post-date (VÃ­ dá»¥: Chá»§ nháº­t, 04/01/2026)
        published_at = int(datetime.now().timestamp())
        date_el = soup.select_one('.post-date')
        if date_el:
            date_match = re.search(r'(\d{2}/\d{2}/\d{4})', date_el.get_text())
            if date_match:
                try:
                    dt = datetime.strptime(date_match.group(1), "%d/%m/%Y")
                    published_at = int(dt.timestamp())
                except: pass

        # 4. Content: BÃ¡o QDND dÃ¹ng class post-content
        paragraphs = []
        # Láº¥y Sapo (TÃ³m táº¯t)
        sapo_el = soup.select_one('.post-summary')
        if sapo_el:
            paragraphs.append(sapo_el.get_text(strip=True))

        # Láº¥y ná»™i dung chÃ­nh
        content_area = soup.select_one('.post-content')
        if content_area:
            # Loáº¡i bá» cÃ¡c div quáº£ng cÃ¡o, video, áº£nh liÃªn quan náº¿u cÃ³
            for r in content_area.select('.related-post, .video-wrapper, .author-info'):
                r.decompose()

            for p in content_area.find_all('p'):
                txt = p.get_text(strip=True)
                if len(txt) > 30:
                    paragraphs.append(txt)

        content = "\n\n".join(paragraphs)

        if not title or len(content) < 100:
            return None

        return (published_at, title, link, content, self.source, "NA", "NA", False, category)
