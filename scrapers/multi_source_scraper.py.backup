
import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import re
from typing import List, Tuple, Optional
import gzip
import brotli
from io import BytesIO
import copy
import html


class NewsScraperBase:
    """Base class cho táº¥t cáº£ news scrapers"""
    
    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Accept-Language': 'en-IN,en-US;q=0.9,en;q=0.8,vi;q=0.7',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        }
        self.delay = 2  # Delay giá»¯a cÃ¡c request (giÃ¢y)
    
    def fetch_html(self, url: str) -> Optional[str]:
        """Fetch vÃ  decode HTML tá»« URL"""
        try:
            resp = self.session.get(url, headers=self.headers, timeout=30)
            resp.raise_for_status()
            
            # Handle content encoding
            content_encoding = resp.headers.get('Content-Encoding', '')
            
            if 'br' in content_encoding:
                try:
                    return brotli.decompress(resp.content).decode('utf-8')
                except:
                    pass
            
            if 'gzip' in content_encoding:
                try:
                    return gzip.decompress(resp.content).decode('utf-8')
                except:
                    pass
            
            return resp.text
            
        except Exception as e:
            print(f"âœ— Error fetching {url}: {e} - multi_source_scraper.py:54")
            return None
    
    def parse_date_to_timestamp(self, date_str: str, format_str: str) -> int:
        """Parse date string thÃ nh Unix timestamp"""
        try:
            dt = datetime.strptime(date_str.strip(), format_str)
            return int(dt.timestamp())
        except Exception as e:
            print(f"âš  Could not parse date '{date_str}': {e} - multi_source_scraper.py:63")
            return 0
    
    def sleep(self):
        """Delay giá»¯a cÃ¡c request"""
        time.sleep(self.delay)


class VnExpressScraper(NewsScraperBase):
    """
    Scraper cho VnExpress.net - crawl tá»« trang "Tin tá»©c 24h"
    """

    def __init__(self):
        super().__init__()
        self.source = "vnexpress.net"
        self.headers['Referer'] = 'https://vnexpress.net/'

    def fetch_news(self, max_pages: int = 1) -> List[Tuple]:
        """
        Fetch tin tá»©c tá»« VnExpress trang "Tin tá»©c 24h"

        Args:
            max_pages: Sá»‘ trang tá»‘i Ä‘a cáº§n crawl (máº·c Ä‘á»‹nh 1)
        """
        all_articles = []

        print(f"\nğŸ“° Crawling VnExpress.net  Tin tá»©c 24h - multi_source_scraper.py:90")

        for page in range(1, max_pages + 1):
            self.sleep()

            # Build URL
            if page == 1:
                url = "https://vnexpress.net/tin-tuc-24h"
            else:
                url = f"https://vnexpress.net/tin-tuc-24h-p{page}"

            print(f"\nğŸ“„ Page {page}/{max_pages}: {url} - multi_source_scraper.py:101")

            html = self.fetch_html(url)
            if not html:
                print(f"âš  Failed to fetch page {page}, skipping - multi_source_scraper.py:105")
                continue

            # Parse listing page
            soup = BeautifulSoup(html, 'html.parser')
            articles = soup.select('article.item-news')

            if not articles:
                print(f"âš  No articles found on page {page} - multi_source_scraper.py:113")
                continue

            print(f"Found {len(articles)} articles on page {page} - multi_source_scraper.py:116")

            for article in articles:
                try:
                    # Extract title and link
                    title_el = article.select_one('h3.title-news a')
                    if not title_el:
                        continue

                    title = title_el.get_text(strip=True)
                    link = title_el.get('href', '')

                    if not title or not link:
                        continue

                    # Extract description
                    desc_el = article.select_one('p.description a')
                    description = desc_el.get_text(strip=True) if desc_el else ""

                    # Fetch article detail
                    self.sleep()
                    article_data = self._fetch_article_detail(link, title, description)
                    if article_data:
                        all_articles.append(article_data)

                except Exception as e:
                    print(f"âœ— Error parsing article: {e} - multi_source_scraper.py:142")
                    continue

        print(f"\nâœ“ Total articles collected: {len(all_articles)} - multi_source_scraper.py:145")
        return all_articles
    
    def _fetch_article_detail(self, link: str, title: str, description: str) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o"""
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # Extract date
        # Format: "Thá»© hai, 29/12/2025, 15:50 (GMT+7)"
        date_el = soup.select_one('span.date')
        published_at = 0

        if date_el:
            date_text = date_el.get_text(strip=True)
            parts = date_text.split(',')
            if len(parts) >= 3:
                date_part = parts[1].strip()  # "29/12/2025"
                time_part = parts[2].strip().split(' ')[0]  # "15:50"
                datetime_str = f"{date_part} {time_part}"
                published_at = self.parse_date_to_timestamp(datetime_str, "%d/%m/%Y %H:%M")

        # Extract content
        content_els = soup.select('article.fck_detail p.Normal')
        content = ' '.join([p.get_text(strip=True) for p in content_els if p.get_text(strip=True)])

        if not content:
            content = description

        # Extract category tá»« breadcrumb
        # VD: <ul.breadcrumb><li><a href="/suc-khoe">Sá»©c khá»e</a></li>...
        category = "Tin tá»©c 24h"  # Default

        breadcrumb_links = soup.select('ul.breadcrumb li a, .breadcrumb a')
        if breadcrumb_links:
            # Láº¥y item Ä‘áº§u tiÃªn lÃ m category chÃ­nh
            category_el = breadcrumb_links[0]
            category_text = category_el.get_text(strip=True)
            if category_text:
                category = category_text.upper()

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA",
            False,
            category,
        )


class VnEconomyScraper(NewsScraperBase):
    """
    Scraper cho VnEconomy.vn sá»­ dá»¥ng RSS Feed
    """
    
    def __init__(self):
        super().__init__()
        self.source = "vneconomy.vn"
        self.rss_url = "https://vneconomy.vn/tin-moi.rss"

    def fetch_news(self, max_articles: int = 20) -> List[Tuple]:
        """Fetch tin tá»©c tá»« VnEconomy qua RSS"""
        all_articles = []
        
        print(f"\nğŸ“° Fetching VnEconomy RSS: {self.rss_url}")
        
        # Sá»­ dá»¥ng feedparser Ä‘á»ƒ Ä‘á»c RSS
        feed = feedparser.parse(self.rss_url)
        
        if not feed.entries:
            print("âš  KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t nÃ o trong RSS feed.")
            return []

        # Giá»›i háº¡n sá»‘ lÆ°á»£ng bÃ i viáº¿t
        entries = feed.entries[:max_articles]
        print(f"âœ“ TÃ¬m tháº¥y {len(entries)} bÃ i viáº¿t tá»« RSS.")

        for entry in entries:
            try:
                # 1. TiÃªu Ä‘á»
                title = entry.title

                # 2. Link
                link = entry.link

                # 3. NgÃ y xuáº¥t báº£n (Chuyá»ƒn sang timestamp)
                # pubDate format: Fri, 02 Jan 2026 09:06:03 GMT
                published_at = int(datetime.now().timestamp())
                if hasattr(entry, 'published_parsed'):
                    published_at = int(datetime(*entry.published_parsed[:6]).timestamp())

                # 4. Ná»™i dung (Æ¯u tiÃªn content:encoded vÃ¬ nÃ³ Ä‘áº§y Ä‘á»§ nháº¥t)
                content = ""
                if hasattr(entry, 'content'):
                    # content thÆ°á»ng lÃ  má»™t list cÃ¡c dict
                    raw_content = entry.content[0].value
                    content = self._clean_rss_content(raw_content)
                elif hasattr(entry, 'description'):
                    content = self._clean_rss_content(entry.description)

                # 5. ChuyÃªn má»¥c
                category = "TIN Má»šI"
                if hasattr(entry, 'category'):
                    category = entry.category.upper()

                # 6. TÃ¡c giáº£ (Náº¿u cÃ³ trong RSS)
                author = "NA"
                if hasattr(entry, 'author'):
                    author = entry.author

                article_data = (
                    published_at,
                    title,
                    link,
                    content,
                    self.source,
                    author,
                    "NA",
                    False,
                    category,
                )
                all_articles.append(article_data)
                
            except Exception as e:
                print(f"âœ— Lá»—i khi xá»­ lÃ½ item RSS: {e}")
                continue

        print(f"âœ“ ÄÃ£ xá»­ lÃ½ xong {len(all_articles)} bÃ i viáº¿t.")
        return all_articles

    def _clean_rss_content(self, raw_html: str) -> str:
        """LÃ m sáº¡ch HTML trong ná»™i dung RSS"""
        if not raw_html:
            return ""
        
        # Decode cÃ¡c kÃ½ tá»± thá»±c thá»ƒ nhÆ° &#237; -> Ã­
        decoded_html = html.unescape(raw_html)
        
        # Sá»­ dá»¥ng BeautifulSoup Ä‘á»ƒ loáº¡i bá» toÃ n bá»™ tag HTML
        soup = BeautifulSoup(decoded_html, 'html.parser')
        
        # VnEconomy thÆ°á»ng Ä‘á»ƒ tÃ³m táº¯t trong tháº» <h2>, ta láº¥y cáº£ <h2> vÃ  <p>
        text_parts = []
        for tag in soup.find_all(['h2', 'p']):
            txt = tag.get_text(strip=True)
            if txt:
                text_parts.append(txt)
                
        cleaned_text = " ".join(text_parts)
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y h2/p thÃ¬ láº¥y toÃ n bá»™ text
        if not cleaned_text:
            cleaned_text = soup.get_text(" ", strip=True)
            
        return cleaned_text

class VOVScraper(NewsScraperBase):
    """
    Scraper cho VOV.vn - crawl tá»« trang "Tin má»›i cáº­p nháº­t"
    """

    def __init__(self):
        super().__init__()
        self.source = "vov.vn"
        self.headers['Referer'] = 'https://vov.vn/'
        self.delay = 3  # VOV cáº§n delay lÃ¢u hÆ¡n

    def fetch_news(self, max_pages: int = 1) -> List[Tuple]:
        """
        Fetch tin tá»©c tá»« VOV trang "Tin má»›i cáº­p nháº­t"

        Args:
            max_pages: Sá»‘ trang tá»‘i Ä‘a cáº§n crawl (máº·c Ä‘á»‹nh 1)
        """
        all_articles = []

        print(f"\nğŸ“° Crawling VOV.vn  Tin má»›i cáº­p nháº­t - multi_source_scraper.py:364")

        # Pagination: page 0, page 1, page 2, ...
        for page in range(max_pages):
            if page == 0:
                url = "https://vov.vn/tin-moi-cap-nhat"
            else:
                url = f"https://vov.vn/tin-moi-cap-nhat?page={page}"

            print(f"\n  ğŸ“„ Page {page + 1}/{max_pages}: {url} - multi_source_scraper.py:373")
            self.sleep()

            html = self.fetch_html(url)
            if not html:
                print(f"âš  Failed to fetch page {page}, stopping - multi_source_scraper.py:378")
                break

            # Check for anti-bot redirect (VOV uses multiple levels of JavaScript redirects)
            max_redirects = 5
            redirect_count = 0
            import re as re_module
            from urllib.parse import unquote

            while ('Attention Required' in html or 'window.location.href' in html) and redirect_count < max_redirects:
                redirect_count += 1
                print(f"âš  Antibot detected (level {redirect_count}), extracting redirect URL... - multi_source_scraper.py:389")

                # Extract redirect URL from JavaScript
                match = re_module.search(r'window\.location\.href\s*=\s*["\']([^"\']+)["\']', html)
                if match:
                    redirect_url = match.group(1)
                    redirect_url = unquote(redirect_url)
                    print(f"â†’ Redirecting to: {redirect_url[:80]}... - multi_source_scraper.py:396")

                    # Fetch the redirect URL
                    self.sleep()
                    html = self.fetch_html(redirect_url)
                    if not html:
                        print(f"âš  Failed to fetch redirect URL, stopping - multi_source_scraper.py:402")
                        break
                    print(f"DEBUG: After redirect {redirect_count}, HTML length: {len(html)} - multi_source_scraper.py:404")
                else:
                    print(f"âš  Could not extract redirect URL, stopping - multi_source_scraper.py:406")
                    break

            # If we still have anti-bot page after max redirects, stop
            if redirect_count >= max_redirects and 'Attention Required' in html:
                print(f"âš  Max redirects ({max_redirects}) reached, still getting antibot page. Stopping. - multi_source_scraper.py:411")
                break

            soup = BeautifulSoup(html, 'html.parser')

            # Select taxonomy-content divs
            content_divs = soup.select('div.taxonomy-content')

            # Debug: Show what we found
            print(f"DEBUG: Found {len(content_divs)} div.taxonomycontent - multi_source_scraper.py:420")

            if not content_divs:
                print(f"âš  No articles found on page {page}, stopping - multi_source_scraper.py:423")
                # Debug: Try alternative selector
                alt_divs = soup.select('.card')
                print(f"DEBUG: Alternative .card selector found {len(alt_divs)} elements - multi_source_scraper.py:426")
                break

            print(f"Found {len(content_divs)} articles on page {page} - multi_source_scraper.py:429")

            for div in content_divs:
                try:
                    # Extract title
                    title_el = div.select_one('h5.media-title') or div.select_one('h3.card-title')
                    title = title_el.get_text(strip=True) if title_el else ''

                    # Extract link
                    link_el = div.select_one('a.vovvn-title')
                    href = link_el.get('href', '') if link_el else ''
                    link = href if href.startswith('http') else f"https://vov.vn{href}"

                    # Extract description
                    desc_el = div.select_one('p.mt-2')
                    description = desc_el.get_text(strip=True) if desc_el else ''

                    if not title or not link:
                        continue

                    # Fetch article detail
                    self.sleep()
                    article_data = self._fetch_article_detail(link, title, description)
                    if article_data:
                        all_articles.append(article_data)

                except Exception as e:
                    print(f"âœ— Error parsing article: {e} - multi_source_scraper.py:456")
                    continue

            # Check pagination Ä‘á»ƒ xem cÃ³ trang tiáº¿p theo khÃ´ng
            pagination = soup.select_one('ul.pagination')
            if not pagination:
                print(f"âš  No pagination found, stopping - multi_source_scraper.py:462")
                break

        print(f"\n  âœ“ Total articles collected: {len(all_articles)} - multi_source_scraper.py:465")
        return all_articles
    
    def _fetch_article_detail(self, link: str, title: str, description: str) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o"""
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # Extract date
        # Format má»›i: "Thá»© Hai, 16:54, 29/12/2025" trong div.col-md-4.mb-2
        # Format cÅ©: "Thá»© Ba, 22:35, 26/08/2025" trong .article-date .col-md-4
        published_at = int(datetime.now().timestamp())  # Default to now

        # Thá»­ selector má»›i trÆ°á»›c
        date_el = soup.select_one('div.col-md-4.mb-2') or soup.select_one('.article-date .col-md-4')

        if date_el:
            date_text = date_el.get_text(strip=True)
            # Format: "Thá»© Hai, 16:54, 29/12/2025"
            parts = [p.strip() for p in date_text.split(',')]
            if len(parts) >= 3:
                time_part = parts[1]  # "16:54"
                date_part = parts[2]  # "29/12/2025"
                datetime_str = f"{date_part} {time_part}"
                parsed_ts = self.parse_date_to_timestamp(datetime_str, "%d/%m/%Y %H:%M")
                if parsed_ts > 0:
                    published_at = parsed_ts

        # Extract content
        content_el = soup.select_one('div.row.article-content div.col div.text-long')
        content = description

        if content_el:
            paragraphs = content_el.select('p')
            content_text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            if content_text:
                content = content_text

        # Extract category
        category = "Tin tá»©c"  # GiÃ¡ trá»‹ máº·c Ä‘á»‹nh

        # Æ¯u tiÃªn 1: Láº¥y tá»« navbar chuyÃªn má»¥c 
        # Cáº¥u trÃºc: <a class="navbar-brand special-header-title" ...>
        nav_category = soup.select_one('a.special-header-title')
        
        # Æ¯u tiÃªn 2: Láº¥y tá»« breadcrumb 
        # Cáº¥u trÃºc: li.breadcrumb-item-first a
        breadcrumb_category = soup.select_one('li.breadcrumb-item-first a, .breadcrumb-item a')

        if nav_category:
            category = nav_category.get_text(strip=True)
        elif breadcrumb_category:
            category = breadcrumb_category.get_text(strip=True)

        # Chuáº©n hÃ³a: Náº¿u láº¥y trÃºng chá»¯ "Trang chá»§" thÃ¬ Ä‘áº·t láº¡i máº·c Ä‘á»‹nh
        if category.lower() in ['trang chá»§', 'home', 'vov.vn', 'vov']:
            category = "Tin tá»©c"
            
        # Chuyá»ƒn thÃ nh chá»¯ hoa Ä‘á»ƒ Ä‘á»“ng bá»™ dá»¯ liá»‡u
        category = category.upper()

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA",
            False,
            category,
        )


class VietnametScraper(NewsScraperBase):
    """
    Scraper cho Vietnamnet.vn
    TÆ°Æ¡ng tá»± hÃ m fetch_vietnamnet_news trong Rust
    """
    
    def __init__(self):
        super().__init__()
        self.source = "vietnamnet.vn"
        self.headers['Referer'] = 'https://vietnamnet.vn/'
    
    def fetch_news(self, max_pages: int = 1, target_date: str = None) -> List[Tuple]:
        """
        Fetch tin tá»©c tá»« Vietnamnet (tin tá»©c 24h by date)

        Args:
            max_pages: Sá»‘ trang tá»‘i Ä‘a cáº§n crawl (máº·c Ä‘á»‹nh 1)
            target_date: NgÃ y cáº§n crawl theo format 'dd/mm/yyyy'. Náº¿u None, dÃ¹ng ngÃ y hiá»‡n táº¡i
        """
        all_articles = []

        # Get date for the bydate parameter
        if target_date:
            date_str = target_date
        else:
            today = datetime.now()
            date_str = today.strftime("%d/%m/%Y")

        print(f"\nğŸ“… Crawling Vietnamnet for date: {date_str} - multi_source_scraper.py:560")

        # Start from page 0
        page = 0

        while True:
            # Build URL with date filter
            url = f"https://vietnamnet.vn/tin-tuc-24h-p{page}?bydate={date_str}-{date_str}&cate="

            print(f"\n  ğŸ“„ Page {page}: {url} - multi_source_scraper.py:569")
            self.sleep()

            html = self.fetch_html(url)
            if not html:
                print(f"âš  Failed to fetch page {page}, stopping - multi_source_scraper.py:574")
                break

            soup = BeautifulSoup(html, 'html.parser')

            # Select posts
            posts = soup.select('div.horizontalPost.version-news')

            if not posts:
                print(f"âš  No articles found on page {page}, stopping - multi_source_scraper.py:583")
                break

            print(f"Found {len(posts)} articles on page {page} - multi_source_scraper.py:586")

            for post in posts:
                try:
                    # Extract title and link
                    title_el = post.select_one('h3.horizontalPost__main-title a')
                    if not title_el:
                        continue

                    title = title_el.get_text(strip=True)
                    href = title_el.get('href', '')
                    link = href if href.startswith('http') else f"https://vietnamnet.vn{href}"

                    if not title or not link:
                        continue

                    # Fetch article detail
                    self.sleep()
                    article_data = self._fetch_article_detail(link, title)
                    if article_data:
                        all_articles.append(article_data)

                except Exception as e:
                    print(f"âœ— Error parsing article: {e} - multi_source_scraper.py:609")
                    continue

            # Check if we should continue to next page
            if max_pages is not None and page >= max_pages - 1:
                print(f"Reached max_pages limit ({max_pages}) - multi_source_scraper.py:614")
                break

            # Check if there's a next page by reading pagination numbers
            pagination = soup.select_one('div.pagination ul.pagination__list')
            if not pagination:
                print(f"No pagination found, stopping - multi_source_scraper.py:620")
                break

            # Find all numbered page links (excluding the pagination-next button)
            page_items = pagination.select('li.pagination__list-item:not(.pagination-next) a')
            page_numbers = []

            for item in page_items:
                page_text = item.get_text(strip=True)
                if page_text.isdigit():
                    page_numbers.append(int(page_text))

            if page_numbers:
                max_page_num = max(page_numbers)
                print(f"Pagination detected: pages 1{max_page_num} (current: page {page + 1}) - multi_source_scraper.py:634")

                # Current page is 0-indexed, but display is 1-indexed
                # If we're at the last page, stop
                if page + 1 >= max_page_num:
                    print(f"Reached the last page ({page + 1}/{max_page_num}) - multi_source_scraper.py:639")
                    break
            else:
                print(f"No page numbers found in pagination, stopping - multi_source_scraper.py:642")
                break

            # Move to next page
            page += 1

        print(f"\n  âœ“ Total articles collected: {len(all_articles)} from {page + 1} page(s) - multi_source_scraper.py:648")
        return all_articles
    
    def _fetch_article_detail(self, link: str, title: str) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o"""
        html = self.fetch_html(link)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # Extract date
        date_el = soup.select_one('div.bread-crumb-detail__time') or soup.select_one('span.time')
        published_at = 0

        if date_el:
            date_text = date_el.get_text(strip=True)

            # Clean date string
            # Format: "Thá»© SÃ¡u, 26/12/2025 - 22:10" â†’ "26/12/2025 22:10"
            # Remove "Thá»© X, " á»Ÿ Ä‘áº§u
            if date_text.startswith('Thá»©'):
                parts = date_text.split(',', 1)
                if len(parts) > 1:
                    date_text = parts[1].strip()

            # Remove dáº¥u " - " giá»¯a date vÃ  time
            date_text = date_text.replace(' - ', ' ')

            # Try various formats
            for fmt in ["%d/%m/%Y %H:%M", "%d-%m-%Y %H:%M", "%d/%m/%Y"]:
                parsed_ts = self.parse_date_to_timestamp(date_text, fmt)
                if parsed_ts > 0:
                    published_at = parsed_ts
                    break
        
        # Extract content
        content_el = soup.select_one('div.maincontent') or soup.select_one('div.article-content')
        content = ""

        if content_el:
            paragraphs = content_el.select('p')
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # Extract category tá»« breadcrumb
        category = "Tin tá»©c"  # Default

        # Thá»­ tÃ¬m tá»« breadcrumb - thÆ°á»ng category lÃ  item thá»© 2 (sau "Trang chá»§")
        breadcrumb_links = soup.select('ul.breadcrumb li a, .breadcrumb a, .bread-crumb a')
        if len(breadcrumb_links) >= 2:
            # Item Ä‘áº§u tiÃªn thÆ°á»ng lÃ  "Trang chá»§", item thá»© 2 lÃ  category
            category_el = breadcrumb_links[1]
            category_text = category_el.get_text(strip=True)
            if category_text and category_text.lower() not in ['trang chá»§', 'home', 'vietnamnet']:
                category = category_text
            # Náº¿u khÃ´ng cÃ³ text, thá»­ láº¥y tá»« title attribute
            elif not category_text:
                category = category_el.get('title', category).strip()

        # Fallback: TÃ¬m link category gáº§n khu vá»±c date (náº¿u breadcrumb khÃ´ng cÃ³)
        if category == "Tin tá»©c":
            # TÃ¬m trong parent cá»§a bread-crumb-detail__time
            date_parent = soup.select_one('div.bread-crumb-detail__time')
            if date_parent and date_parent.parent:
                nearby_links = date_parent.parent.select('a[title]')
                for link_el in nearby_links:
                    href = link_el.get('href', '')
                    # Category links thÆ°á»ng cÃ³ href ngáº¯n nhÆ° "/thoi-su", "/kinh-doanh"
                    if href.startswith('/') and href.count('/') == 1 and len(href) < 30:
                        category = link_el.get_text(strip=True) or link_el.get('title', category)
                        break

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA",
            False,
            category,
        )


class CafelandScraper(NewsScraperBase):
    """
    Scraper cho Cafeland.vn - Báº¥t Ä‘á»™ng sáº£n
    """

    def __init__(self):
        super().__init__()
        self.source = "cafeland.vn"
        self.headers['Referer'] = 'https://cafeland.vn/'

    def fetch_news(self, max_pages: int = 1, max_articles_per_page: int = 20) -> List[Tuple]:
        """
        Fetch tin tá»©c tá»« Cafeland trang "Báº¥t Ä‘á»™ng sáº£n má»›i nháº¥t"

        Args:
            max_pages: Sá»‘ trang tá»‘i Ä‘a cáº§n crawl (máº·c Ä‘á»‹nh 1)
            max_articles_per_page: Sá»‘ bÃ i tá»‘i Ä‘a má»—i trang (máº·c Ä‘á»‹nh 20)
        """
        all_articles = []

        print(f"\nğŸ“° Crawling Cafeland.vn  Báº¥t Ä‘á»™ng sáº£n má»›i nháº¥t - multi_source_scraper.py:753")

        for page in range(1, max_pages + 1):
            self.sleep()

            # Build URL
            if page == 1:
                url = "https://cafeland.vn/bat-dong-san-moi-nhat/"
            else:
                url = f"https://cafeland.vn/bat-dong-san-moi-nhat/page/{page}/"

            print(f"\nğŸ“„ Page {page}/{max_pages}: {url} - multi_source_scraper.py:764")

            html = self.fetch_html(url)
            if not html:
                print(f"âš  Failed to fetch page {page}, skipping - multi_source_scraper.py:768")
                continue

            # Parse listing page
            soup = BeautifulSoup(html, 'html.parser')
            # Note: Articles are in <li class="loadBoxHomeMore">, not <div>
            articles = soup.select('li.loadBoxHomeMore')

            if not articles:
                print(f"âš  No articles found on page {page} - multi_source_scraper.py:777")
                continue

            print(f"Found {len(articles)} articles on page {page} - multi_source_scraper.py:780")

            # Limit articles per page
            articles = articles[:max_articles_per_page]

            for article in articles:
                try:
                    # Extract title and link
                    title_el = article.select_one('h3 a')
                    if not title_el:
                        continue

                    title = title_el.get_text(strip=True)
                    link = title_el.get('href', '')

                    if not title or not link:
                        continue

                    # Make sure link is absolute
                    if not link.startswith('http'):
                        link = f"https://cafeland.vn{link}"

                    # Extract description
                    desc_els = article.select('p')
                    description = desc_els[1].get_text(strip=True) if len(desc_els) > 1 else ""

                    # Fetch article detail
                    self.sleep()
                    article_data = self._fetch_article_detail(link, title, description)
                    if article_data:
                        all_articles.append(article_data)

                except Exception as e:
                    print(f"âœ— Error parsing article: {e} - multi_source_scraper.py:813")
                    continue

        print(f"\nâœ“ Total articles collected: {len(all_articles)} - multi_source_scraper.py:816")
        return all_articles

    def _fetch_article_detail(self, link: str, title: str, description: str) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o"""
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # Extract date tá»« div.info-date.right
        # Format: "31/12/2025 9:05 PM"
        published_at = 0
        date_el = soup.select_one('div.info-date.right')

        if date_el:
            # Get text content, skipping audio element
            date_text = date_el.get_text(strip=True)
            # Remove any extra whitespace
            date_text = re.sub(r'\s+', ' ', date_text).strip()

            # Parse: "31/12/2025 9:05 PM"
            # Try to match the date pattern
            date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})\s+(\d{1,2}):(\d{2})\s*(AM|PM)', date_text)
            if date_match:
                day, month, year, hour, minute, period = date_match.groups()
                try:
                    hour = int(hour)
                    # Convert to 24-hour format
                    if period == 'PM' and hour != 12:
                        hour += 12
                    elif period == 'AM' and hour == 12:
                        hour = 0

                    dt = datetime(int(year), int(month), int(day), hour, int(minute))
                    published_at = int(dt.timestamp())
                except Exception as e:
                    print(f"âš  Could not parse date '{date_text}': {e} - multi_source_scraper.py:854")

        # Extract content
        # Try content containers (IDs for news articles, class for project pages)
        content_els = soup.select('#sevenBoxNewContentInfo, #sevenBoxNewContentInfoNo, #sevenBoxNewContenDAtInfo, div.sevenPostContent')
        content = description

        if content_els:
            paragraphs = []
            # Only use the first matching container to avoid duplication
            el = content_els[0]

            # Get div.sevenPostDes (description)
            desc_div = el.select_one('div.sevenPostDes')
            if desc_div:
                paragraphs.append(desc_div)

            # Get all headings and paragraphs in document order
            for elem in el.select('h2, h3, h4, h5, h6, p'):
                # If it's a heading, add it directly
                if elem.name in ['h2', 'h3', 'h4', 'h5', 'h6']:
                    paragraphs.append(elem)

                # If it's a paragraph, apply filters
                elif elem.name == 'p':
                    # Skip if paragraph only contains <em> tag
                    if len(elem.find_all()) == 1 and elem.find('em'):
                        continue

                    # Skip paragraphs with javascript links or image title links
                    p_text = elem.get_text(strip=True)
                    js_link = elem.find('a', href=lambda x: x and x.startswith('javascript:'))
                    if js_link or 'Click vÃ o' in p_text:
                        continue

                    # Skip navigation links (e.g., ">> Xem thÃªm cÃ¡c dá»± Ã¡n...")
                    if p_text.startswith('>>') or 'Xem thÃªm' in p_text:
                        # Check if it's purely a navigation link (strong>a structure)
                        strong_tag = elem.find('strong')
                        if strong_tag and strong_tag.find('a'):
                            continue

                    # Add valid paragraphs (including those with inline links)
                    paragraphs.append(elem)

            content_text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            if content_text:
                content = content_text

        # Extract category tá»« breadcrumb
        # <a itemprop="item"><span itemprop="name">Thá»‹ TrÆ°á»ng</span></a>
        # Breadcrumb cÃ³ dáº¡ng: [Trang chá»§, Thá»‹ TrÆ°á»ng, Thá»‹ TrÆ°á»ng Báº¥t Äá»™ng Sáº£n]
        category = "Báº¥t Ä‘á»™ng sáº£n"  # Default

        breadcrumb_links = soup.select('a[itemprop="item"] span[itemprop="name"]')
        if len(breadcrumb_links) >= 2:
            # Láº¥y item thá»© 2 (bá» qua "Trang chá»§")
            category_text = breadcrumb_links[1].get_text(strip=True)
            if category_text:
                category = category_text.upper()

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA",
            False,
            category,
        )


class CafeFScraper(NewsScraperBase):
    """
    Scraper cho CafeF.vn
    """

    def __init__(self):
        super().__init__()
        self.source = "cafef.vn"
        self.headers['Referer'] = 'https://cafef.vn/'

    def fetch_news(self, max_pages: int = 1, max_articles_per_page: int = 20) -> List[Tuple]:
        """
        Fetch tin tá»©c tá»« CafeF tá»« trang /doc-nhanh

        Args:
            max_pages: Sá»‘ trang tá»‘i Ä‘a (máº·c Ä‘á»‹nh 1)
            max_articles_per_page: Sá»‘ bÃ i tá»‘i Ä‘a má»—i trang (máº·c Ä‘á»‹nh 20)
        """
        all_articles = []
        seen_urls = {}  # Track URLs vÃ  page number: {full_url: page_number}

        for page in range(1, max_pages + 1):
            # Build pagination URL
            if page == 1:
                url = "https://cafef.vn/doc-nhanh.chn"
            else:
                url = f"https://cafef.vn/doc-nhanh/trang-{page}.chn"

            print(f"\nğŸ“„ Page {page}/{max_pages}: {url} - multi_source_scraper.py:956")
            self.sleep()

            html = self.fetch_html(url)
            if not html:
                print(f"âš  Failed to fetch page {page}, skipping - multi_source_scraper.py:961")
                continue

            soup = BeautifulSoup(html, 'html.parser')

            # Find all article links vá»›i pattern -188*.chn
            links = soup.find_all('a', href=re.compile(r'-\d{15,}\.chn$'))

            article_urls = []

            for link in links:
                href = link.get('href', '')
                if href:
                    full_url = href if href.startswith('http') else f"https://cafef.vn{href}"
                    # Exclude pagination pages
                    if '/trang-' in full_url:
                        continue
                    if full_url not in seen_urls:
                        seen_urls[full_url] = page
                        article_urls.append(full_url)

            if not article_urls:
                print(f"âš  No new articles on page {page} - multi_source_scraper.py:983")
                continue

            print(f"Found {len(article_urls)} new article URLs on page {page} - multi_source_scraper.py:986")

            # Limit articles per page
            article_urls = article_urls[:max_articles_per_page]

            # Fetch article details
            for i, article_url in enumerate(article_urls, 1):
                print(f"[{i}/{len(article_urls)}] Fetching: {article_url[:60]}... - multi_source_scraper.py:993")
                self.sleep()

                article_data = self._fetch_article_detail(article_url)
                if article_data:
                    all_articles.append(article_data)

        print(f"\nâœ“ Total articles collected: {len(all_articles)} - multi_source_scraper.py:1000")
        return all_articles
    
    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o CafeF"""
        html = self.fetch_html(link)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # Extract title
        title_el = soup.select_one('h1.title') or soup.select_one('h1')
        title = title_el.get_text(strip=True) if title_el else ''
        
        if not title:
            return None
        
        # Extract date tá»« span.pdate[data-role="publishdate"]
        # Format: "29-12-2025 - 16:16 PM"
        published_at = 0

        date_el = soup.select_one('span.pdate[data-role="publishdate"]')
        if date_el:
            date_text = date_el.get_text(strip=True)
            # Parse: "29-12-2025 - 16:16 PM" -> datetime
            date_match = re.search(r'(\d{1,2})-(\d{1,2})-(\d{4})\s*-\s*(\d{1,2}):(\d{2})', date_text)
            if date_match:
                day, month, year, hour, minute = date_match.groups()
                try:
                    dt = datetime(int(year), int(month), int(day), int(hour), int(minute))
                    published_at = int(dt.timestamp())
                except:
                    pass
        
        # Extract content
        content_selectors = ['.detail-content', '.contentdetail', '.detail_content', 'article .content']
        content = ""
        
        for selector in content_selectors:
            content_el = soup.select_one(selector)
            if content_el:
                paragraphs = content_el.select('p')
                content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                if content:
                    break
        
        # Extract category tá»« a[data-role="cate-name"]
        category = "Äá»ŒC NHANH"  # Default

        category_el = soup.select_one('a[data-role="cate-name"]')
        if category_el:
            # Láº¥y text hoáº·c title náº¿u text rá»—ng, sau Ä‘Ã³ in hoa
            category = (category_el.get_text(strip=True) or category_el.get('title', 'Äá»ŒC NHANH')).strip().upper()


        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA",
            False,
            category,
        )


class DanTriRSSScraper(NewsScraperBase):
    """
    Scraper cho DanTri.com.vn sá»­ dá»¥ng RSS Feed
    """

    def __init__(self):
        super().__init__()
        self.source = "dantri.com.vn"
        self.rss_url = "https://dantri.com.vn/rss/tin-moi-nhat.rss"

    def fetch_news(self) -> List[Tuple]:
        """
        Láº¥y tá»‘i Ä‘a 20 tin tá»©c má»›i nháº¥t tá»« RSS feed cá»§a DÃ¢n trÃ­
        """
        all_articles = []
        print(f"\nğŸ“¡ Äang Ä‘á»c RSS tá»«: {self.rss_url} - multi_source_scraper.py:1084")
        
        # 1. Sá»­ dá»¥ng feedparser Ä‘á»ƒ Ä‘á»c ná»™i dung RSS
        feed = feedparser.parse(self.rss_url)
        
        if not feed.entries:
            print("âš  KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t nÃ o trong RSS. - multi_source_scraper.py:1090")
            return []

        # 2. Giá»›i háº¡n chá»‰ xá»­ lÃ½ 20 bÃ i viáº¿t Ä‘áº§u tiÃªn
        entries_to_process = feed.entries[:20]
        print(f"TÃ¬m tháº¥y {len(feed.entries)} bÃ i. Sáº½ xá»­ lÃ½ {len(entries_to_process)} bÃ i má»›i nháº¥t. - multi_source_scraper.py:1095")

        for entry in entries_to_process:
            link = entry.link
            
            # Láº¥y Category trá»±c tiáº¿p tá»« tháº» <category> cá»§a RSS Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c (vÃ­ dá»¥: ChÃ­nh trá»‹)
            rss_category = getattr(entry, 'category', 'TIN Má»šI')
            
            print(f"Fetching: {link[:60]}... - multi_source_scraper.py:1103")
            self.sleep()
            
            # Truyá»n rss_category vÃ o hÃ m detail Ä‘á»ƒ xá»­ lÃ½
            article_data = self._fetch_article_detail(link, rss_category)
            if article_data:
                all_articles.append(article_data)

        print(f"\nâœ“ Tá»•ng sá»‘ bÃ i viáº¿t thu tháº­p Ä‘Æ°á»£c: {len(all_articles)} - multi_source_scraper.py:1111")
        return all_articles

    def _fetch_article_detail(self, link: str, rss_category: str = None) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o tá»« DÃ¢n trÃ­"""
        html = self.fetch_html(link)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. TrÃ­ch xuáº¥t TiÃªu Ä‘á»
        title_el = soup.select_one('h1.title-page') or soup.select_one('h1')
        title = title_el.get_text(strip=True) if title_el else ''
        
        if not title:
            return None
        
        # 2. TrÃ­ch xuáº¥t NgÃ y xuáº¥t báº£n
        published_at = 0
        date_el = soup.select_one('.author-time')
        if date_el:
            date_text = date_el.get_text(strip=True)
            date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})\s*-\s*(\d{1,2}):(\d{2})', date_text)
            if date_match:
                day, month, year, hour, minute = date_match.groups()
                try:
                    dt = datetime(int(year), int(month), int(day), int(hour), int(minute))
                    published_at = int(dt.timestamp())
                except:
                    pass
        
        # 3. TrÃ­ch xuáº¥t Ná»™i dung
        content = ""
        content_el = soup.select_one('.singular-content')
        if content_el:
            # Loáº¡i bá» quáº£ng cÃ¡o vÃ  video liÃªn quan
            for unwanted in content_el.select('.gui-check-parent, .video-content-wrapper, .ad-container'):
                unwanted.decompose()
                
            paragraphs = content_el.select('p')
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # 4. TrÃ­ch xuáº¥t ChuyÃªn má»¥c (Æ¯u tiÃªn tá»« RSS)
        if rss_category and rss_category != "TIN Má»šI":
            category = rss_category
        else:
            # Dá»± phÃ²ng láº¥y tá»« Meta Tag náº¿u RSS thiáº¿u
            meta_cate = soup.find('meta', property='article:section')
            category = meta_cate.get('content') if meta_cate else "TIN Má»šI"

        category = category.upper().strip()

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",   # stock_related
            "NA",   # sentiment_score
            False,  # server_pushed
            category,
        )


class ThanhNienRSSScraper(NewsScraperBase):
    """
    Scraper cho ThanhNien.vn sá»­ dá»¥ng RSS Feed
    """

    def __init__(self):
        super().__init__()
        self.source = "thanhnien.vn"
        # Sá»­ dá»¥ng RSS Tin má»›i nháº¥t
        self.rss_url = "https://thanhnien.vn/rss/home.rss"

    def fetch_news(self) -> List[Tuple]:
        """
        Láº¥y tá»‘i Ä‘a 20 tin tá»©c má»›i nháº¥t tá»« RSS feed cá»§a Thanh NiÃªn
        """
        all_articles = []
        print(f"\nğŸ“¡ Äang Ä‘á»c RSS tá»«: {self.rss_url} - multi_source_scraper.py:1193")
        
        feed = feedparser.parse(self.rss_url)
        
        if not feed.entries:
            print("âš  KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t nÃ o trong RSS Thanh NiÃªn. - multi_source_scraper.py:1198")
            return []

        # Giá»›i háº¡n 20 bÃ i Ä‘áº§u tiÃªn
        entries_to_process = feed.entries[:20]
        print(f"Thanh NiÃªn: TÃ¬m tháº¥y {len(feed.entries)} bÃ i. Sáº½ xá»­ lÃ½ {len(entries_to_process)} bÃ i. - multi_source_scraper.py:1203")

        for entry in entries_to_process:
            link = entry.link
            
            print(f"Fetching: {link[:60]}... - multi_source_scraper.py:1208")
            self.sleep()
            
            # VÃ¬ RSS Thanh NiÃªn khÃ´ng cÃ³ tháº» <category>, ta sáº½ bÃ³c tÃ¡ch nÃ³ trong hÃ m detail
            article_data = self._fetch_article_detail(link)
            if article_data:
                all_articles.append(article_data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o tá»« Thanh NiÃªn"""
        html = self.fetch_html(link)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. TrÃ­ch xuáº¥t TiÃªu Ä‘á»
        # Thanh NiÃªn dÃ¹ng class .detail-title hoáº·c h1
        title_el = soup.select_one('h1.detail-title') or soup.select_one('.detail-title') or soup.select_one('h1')
        title = title_el.get_text(strip=True) if title_el else ''
        
        if not title:
            return None
        
        # 2. TrÃ­ch xuáº¥t NgÃ y xuáº¥t báº£n
        published_at = 0
        # Thanh NiÃªn: <div class="detail-time"><span>01/01/2026 17:24</span></div>
        date_el = soup.select_one('.detail-time') or soup.select_one('.detail-time span')
        if date_el:
            date_text = date_el.get_text(strip=True)
            # Regex láº¥y Ä‘á»‹nh dáº¡ng dd/mm/yyyy HH:MM
            date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})\s+(\d{1,2}):(\d{2})', date_text)
            if date_match:
                day, month, year, hour, minute = date_match.groups()
                try:
                    dt = datetime(int(year), int(month), int(day), int(hour), int(minute))
                    published_at = int(dt.timestamp())
                except: pass

        # 3. TrÃ­ch xuáº¥t Ná»™i dung
        content = ""
        # Thanh NiÃªn thÆ°á»ng dÃ¹ng div#abb-content hoáº·c [itemprop="articleBody"]
        content_el = soup.select_one('#abb-content') or soup.select_one('.detail-content') or soup.select_one('[itemprop="articleBody"]')
        if content_el:
            # Loáº¡i bá» cÃ¡c thÃ nh pháº§n thá»«a
            for unwanted in content_el.select('.morenews, .display-ads, .video-content-wrapper, .banner-ads'):
                unwanted.decompose()
            
            paragraphs = content_el.select('p')
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # 4. TrÃ­ch xuáº¥t ChuyÃªn má»¥c (Giáº£i phÃ¡p cho viá»‡c thiáº¿u tháº» trong RSS)
        category = "TIN Tá»¨C"
        
        # Æ¯u tiÃªn 1: Láº¥y tá»« Meta Tag article:section (Chuáº©n SEO cá»§a Thanh NiÃªn)
        meta_cate = soup.find('meta', property='article:section')
        if meta_cate and meta_cate.get('content'):
            category = meta_cate.get('content').strip()
        else:
            # Æ¯u tiÃªn 2: TÃ¡ch tá»« URL (VÃ­ dá»¥ thanhnien.vn/thoi-su/abc.htm -> THOI SU)
            try:
                path_parts = link.replace('https://thanhnien.vn/', '').split('/')
                if len(path_parts) > 1:
                    category = path_parts[0].replace('-', ' ')
            except: pass

        category = category.upper().strip()

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",   # stock_related
            "NA",   # sentiment_score
            False,  # server_pushed
            category,
        )


class TuoiTreRSSScraper(NewsScraperBase):
    """
    Scraper cho TuoiTre.vn sá»­ dá»¥ng RSS Feed
    """

    def __init__(self):
        super().__init__()
        self.source = "tuoitre.vn"
        self.rss_url = "https://tuoitre.vn/rss/tin-moi-nhat.rss"

    def fetch_news(self) -> List[Tuple]:
        """Láº¥y 20 tin má»›i nháº¥t tá»« Tuá»•i Tráº»"""
        all_articles = []
        print(f"\nğŸ“¡ Äang Ä‘á»c RSS tá»«: {self.rss_url} - multi_source_scraper.py:1304")

        feed = feedparser.parse(self.rss_url)
        if not feed.entries:
            return []

        entries_to_process = feed.entries[:20]
        for entry in entries_to_process:
            link = entry.link
            self.sleep()
            article_data = self._fetch_article_detail(link)
            if article_data:
                all_articles.append(article_data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # 1. TiÃªu Ä‘á»
        title_el = soup.select_one('.detail-title') or soup.select_one('h1')
        title = title_el.get_text(strip=True) if title_el else ''
        if not title: return None

        # 2. NgÃ y xuáº¥t báº£n (Xá»­ lÃ½ format: 01/01/2026 18:00 GMT+7)
        published_at = 0
        date_el = soup.select_one('.detail-time') or soup.find('meta', property='article:published_time')
        if date_el:
            # Láº¥y text náº¿u lÃ  tag, láº¥y content náº¿u lÃ  meta
            date_text = date_el.get_text(strip=True) if not date_el.get('content') else date_el.get('content')
            # Regex tÃ¬m: dd/mm/yyyy HH:mm
            date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})\s+(\d{1,2}):(\d{2})', date_text)
            if date_match:
                day, month, year, hour, minute = date_match.groups()
                try:
                    dt = datetime(int(year), int(month), int(day), int(hour), int(minute))
                    published_at = int(dt.timestamp())
                except: pass

        # 3. Ná»™i dung
        content = ""
        # Tuá»•i tráº» dÃ¹ng class .fck hoáº·c div[itemprop="articleBody"]
        content_el = soup.select_one('.fck') or soup.select_one('.detail-content')
        if content_el:
            # Loáº¡i bá» video, tin liÃªn quan, quáº£ng cÃ¡o
            for unwanted in content_el.select('.vnn-title, .box-tin-lien-quan, .ad-container'):
                unwanted.decompose()
            paragraphs = content_el.select('p')
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # 4. ChuyÃªn má»¥c (BÃ³c tá»« Meta article:section)
        category = "TIN Má»šI"
        meta_cate = soup.find('meta', property='article:section')
        if meta_cate:
            category = meta_cate.get('content').upper().strip()

        return (
            published_at, title, link, content, self.source,
            "NA", "NA", False, category,
        )


class VietStockScraper(NewsScraperBase):
    """
    Scraper cho VietStock.vn - crawl tá»« trang "Má»›i cáº­p nháº­t"

    Sá»­ dá»¥ng Selenium Ä‘á»ƒ xá»­ lÃ½ trang load báº±ng JavaScript.
    """

    def __init__(self):
        super().__init__()
        self.source = "vietstock.vn"
        self.headers['Referer'] = 'https://vietstock.vn/'

    def fetch_news(self, max_articles: int = 15) -> List[Tuple]:
        """
        Fetch tin tá»©c tá»« VietStock trang "Má»›i cáº­p nháº­t" báº±ng Selenium

        Args:
            max_articles: Sá»‘ bÃ i tá»‘i Ä‘a cáº§n crawl (máº·c Ä‘á»‹nh 15)
        """
        all_articles = []
        url = "https://vietstock.vn/chu-de/1-2/moi-cap-nhat.htm"

        print(f"\nğŸ“° Crawling VietStock.vn  Má»›i cáº­p nháº­t (with Selenium)")
        print(f"\nğŸ“„ Fetching: {url}")

        # Import Selenium
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.chrome.service import Service
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from webdriver_manager.chrome import ChromeDriverManager

        # Setup Chrome options (headless mode)
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # Cháº¡y khÃ´ng hiá»ƒn thá»‹ browser
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument(f'user-agent={self.headers["User-Agent"]}')

        # Chá»‰ Ä‘á»‹nh Ä‘Æ°á»ng dáº«n Chrome binary (Windows)
        import os
        chrome_paths = [
            r'C:\Program Files\Google\Chrome\Application\chrome.exe',
            r'C:\Program Files (x86)\Google\Chrome\Application\chrome.exe',
        ]
        for chrome_path in chrome_paths:
            if os.path.exists(chrome_path):
                chrome_options.binary_location = chrome_path
                break

        driver = None
        try:
            # Khá»Ÿi táº¡o Chrome driver vá»›i webdriver-manager
            print("â†’ Starting Chrome browser...")
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)

            # Má»Ÿ trang
            driver.get(url)

            # Äá»£i JavaScript load xong - Ä‘á»£i cÃ³ link bÃ i viáº¿t xuáº¥t hiá»‡n
            print("â†’ Waiting for page to load...")
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "a[href*='.htm']"))
            )

            # Äá»£i thÃªm má»™t chÃºt Ä‘á»ƒ Ä‘áº£m báº£o táº¥t cáº£ content Ä‘Ã£ load
            import time
            time.sleep(2)

            # Láº¥y HTML Ä‘Ã£ render
            html = driver.page_source
            print(f"âœ“ Got rendered HTML: {len(html)} chars")

        except Exception as e:
            print(f"âš  Selenium error: {e}")
            return all_articles
        finally:
            # ÄÃ³ng browser
            if driver:
                driver.quit()
                print("â†’ Browser closed")

        # Parse HTML vá»›i BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')

        # Find all article links - VietStock articles have title attribute
        # URL pattern: /YYYY/MM/article-slug-###-XXXXXXX.htm
        article_links = []

        # Find links with title attribute (actual articles)
        titled_links = soup.find_all('a', title=True, href=lambda x: x and '.htm' in x)

        for link in titled_links:
            href = link.get('href', '')
            title = link.get('title', '')

            # Skip non-articles
            if not href or href.startswith('javascript:') or href.startswith('#'):
                continue

            # Skip topic pages
            if '/chu-de/' in href:
                continue

            # Make absolute URL
            if not href.startswith('http'):
                if href.startswith('//'):
                    href = 'https:' + href
                else:
                    href = f"https://vietstock.vn{href}"

            # Article URLs have pattern: /YYYY/MM/slug.htm
            # Must have title (real article) and proper URL depth
            if (len(title) > 10 and
                href.count('/') >= 5 and  # /YYYY/MM/article.htm = 5 slashes minimum
                href not in article_links and
                '/20' in href):  # Has year in path like /2026/

                article_links.append(href)

        if not article_links:
            print(f"âš  No articles found")
            return all_articles

        # Limit to requested number
        article_links = article_links[:max_articles]
        print(f"Found {len(article_links)} article URLs")

        # Fetch article details
        for i, article_url in enumerate(article_links, 1):
            print(f"[{i}/{len(article_links)}] Fetching: {article_url[:60]}...")
            self.sleep()

            article_data = self._fetch_article_detail(article_url)
            if article_data:
                all_articles.append(article_data)

        print(f"\nâœ“ Total articles collected: {len(all_articles)}")
        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o VietStock"""
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # Extract title
        title_el = soup.select_one('h1') or soup.select_one('.article-title')
        title = title_el.get_text(strip=True) if title_el else ''

        if not title:
            print(f"âœ— No title found for: {link[:60]}...")
            return None

        # Extract date - VietStock has multiple sources
        # Priority 1: Meta tag article:published_time (ISO format)
        # Priority 2: span.datenew
        published_at = 0

        # Try meta tag first (ISO format: 2026-01-01T21:11:44+07:00)
        meta_date = soup.find('meta', property='article:published_time')
        if meta_date and meta_date.get('content'):
            try:
                from dateutil import parser
                dt = parser.parse(meta_date.get('content'))
                published_at = int(dt.timestamp())
            except Exception as e:
                print(f"âš  Could not parse meta date: {e}")

        # Fallback to span.datenew (format: 01-01-2026 21:11:44+07:00)
        if published_at == 0:
            date_el = soup.select_one('span.datenew')
            if date_el:
                date_text = date_el.get_text(strip=True)
                # Format: "01-01-2026 21:11:44+07:00" or "01/01/2026 21:11:44"
                # Try parsing with dateutil
                try:
                    from dateutil import parser
                    dt = parser.parse(date_text)
                    published_at = int(dt.timestamp())
                except Exception as e:
                    # Fallback to regex
                    date_match = re.search(r'(\d{1,2})[-/](\d{1,2})[-/](\d{4})\s+(\d{1,2}):(\d{2}):(\d{2})', date_text)
                    if date_match:
                        day, month, year, hour, minute, second = date_match.groups()
                        try:
                            dt = datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))
                            published_at = int(dt.timestamp())
                        except Exception as e:
                            print(f"âš  Could not parse date '{date_text}': {e}")

        # Extract content
        content_el = soup.select_one('.detail-content') or soup.select_one('.article-content') or soup.select_one('[itemprop="articleBody"]')
        content = ""

        if content_el:
            paragraphs = content_el.select('p')
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # Extract category from breadcrumb
        # <a href="/kinh-te.htm" itemprop="item" title="Kinh táº¿" class="bcrumbs-item"><span itemprop="name">Kinh táº¿</span></a>
        category = "Má»šI Cáº¬P NHáº¬T"  # Default (Ä‘Ã£ in hoa)

        # Try to get from breadcrumb
        breadcrumb_links = soup.select('a.bcrumbs-item[itemprop="item"]')
        if len(breadcrumb_links) >= 2:
            # Skip first (usually "Trang chá»§"), take second as category
            category_el = breadcrumb_links[1]
            category_span = category_el.select_one('span[itemprop="name"]')
            if category_span:
                category_text = category_span.get_text(strip=True)
                if category_text:
                    category = category_text.upper()  # In hoa category

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA",
            False,
            category,
        )


class NLDScraper(NewsScraperBase):
    """
    Scraper cho NLD.com.vn (NgÆ°á»i Lao Äá»™ng) - crawl tá»« trang "Tin 24h"
    """

    def __init__(self):
        super().__init__()
        self.source = "nld.com.vn"
        self.headers['Referer'] = 'https://nld.com.vn/'

    def fetch_news(self, max_articles: int = 20) -> List[Tuple]:
        """
        Fetch tin tá»©c tá»« NLD trang "Tin 24h"

        Args:
            max_articles: Sá»‘ bÃ i tá»‘i Ä‘a cáº§n crawl (máº·c Ä‘á»‹nh 20)
        """
        all_articles = []
        url = "https://nld.com.vn/tin-24h.htm"

        print(f"\nğŸ“° Crawling NLD.com.vn  Tin 24h")
        print(f"\nğŸ“„ Fetching: {url}")

        self.sleep()
        html = self.fetch_html(url)
        if not html:
            print(f"âš  Failed to fetch page, stopping")
            return all_articles

        # Parse listing page
        soup = BeautifulSoup(html, 'html.parser')

        # Find all article links on the page
        article_links = []

        # Try multiple selectors to find articles
        for selector in ['article a', '.article-item a', '.news-item a', 'h3 a', 'h2 a', '.box-category-item a']:
            links = soup.select(selector)
            if links:
                for link in links:
                    href = link.get('href', '')
                    if href and not href.startswith('javascript:') and not href.startswith('#'):
                        # Make sure link is absolute
                        if not href.startswith('http'):
                            href = f"https://nld.com.vn{href}"
                        # Filter to only include article URLs (not category pages, etc.)
                        if '/tin-24h' not in href and '.htm' in href and href not in article_links:
                            article_links.append(href)

                if article_links:
                    break

        if not article_links:
            print(f"âš  No articles found")
            return all_articles

        # Limit to requested number of articles
        article_links = article_links[:max_articles]
        print(f"Found {len(article_links)} article URLs")

        # Fetch article details
        for i, article_url in enumerate(article_links, 1):
            print(f"[{i}/{len(article_links)}] Fetching: {article_url[:60]}...")
            self.sleep()

            article_data = self._fetch_article_detail(article_url)
            if article_data:
                all_articles.append(article_data)

        print(f"\nâœ“ Total articles collected: {len(all_articles)}")
        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o NLD"""
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # Extract title
        title_el = soup.select_one('h1') or soup.select_one('.article-title')
        title = title_el.get_text(strip=True) if title_el else ''

        if not title:
            print(f"âœ— No title found for: {link[:60]}...")
            return None

        # Extract date from time[data-role="publishdate"]
        # Format: <time data-role="publishdate" datetime="2026-01-01T21:35:00+07:00">01/01/2026 21:35 GMT+7</time>
        published_at = 0
        date_el = soup.select_one('time[data-role="publishdate"]')

        if date_el:
            # Try to use datetime attribute first (ISO format)
            datetime_attr = date_el.get('datetime', '')
            if datetime_attr:
                try:
                    # Parse ISO format: 2026-01-01T21:35:00+07:00
                    from dateutil import parser
                    dt = parser.parse(datetime_attr)
                    published_at = int(dt.timestamp())
                except:
                    # Fallback to text parsing
                    date_text = date_el.get_text(strip=True)
                    # Format: "01/01/2026 21:35 GMT+7"
                    date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})\s+(\d{1,2}):(\d{2})', date_text)
                    if date_match:
                        day, month, year, hour, minute = date_match.groups()
                        try:
                            dt = datetime(int(year), int(month), int(day), int(hour), int(minute))
                            published_at = int(dt.timestamp())
                        except Exception as e:
                            print(f"âš  Could not parse date '{date_text}': {e}")
            else:
                # No datetime attribute, parse text
                date_text = date_el.get_text(strip=True)
                date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})\s+(\d{1,2}):(\d{2})', date_text)
                if date_match:
                    day, month, year, hour, minute = date_match.groups()
                    try:
                        dt = datetime(int(year), int(month), int(day), int(hour), int(minute))
                        published_at = int(dt.timestamp())
                    except Exception as e:
                        print(f"âš  Could not parse date '{date_text}': {e}")

        # Extract content
        content_el = soup.select_one('.detail-content') or soup.select_one('.article-content') or soup.select_one('[itemprop="articleBody"]')
        content = ""

        if content_el:
            paragraphs = content_el.select('p')
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # Extract category from a.category-name_ac[data-role="cate-name"]
        # <a href="/the-thao.htm" title="Thá»ƒ thao" class="category-name_ac" data-role="cate-name">Thá»ƒ thao</a>
        category = "TIN 24H"  # Default (Ä‘Ã£ in hoa)

        category_el = soup.select_one('a.category-name_ac[data-role="cate-name"]')
        if category_el:
            category_text = category_el.get_text(strip=True)
            if category_text:
                category = category_text.upper()  # In hoa category

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA",
            False,
            category,
        )


class LaoDongScraper(NewsScraperBase):
    """
    Scraper cho LaoDong.vn - crawl tá»« trang "Tin má»›i"
    """

    def __init__(self):
        super().__init__()
        self.source = "laodong.vn"
        self.headers['Referer'] = 'https://laodong.vn/'

    def fetch_news(self, max_articles: int = 20) -> List[Tuple]:
        """
        Fetch tin tá»©c tá»« LaoDong trang "Tin má»›i"

        Args:
            max_articles: Sá»‘ bÃ i tá»‘i Ä‘a cáº§n crawl (máº·c Ä‘á»‹nh 20)
        """
        all_articles = []
        url = "https://laodong.vn/tin-moi"

        print(f"\nğŸ“° Crawling LaoDong.vn  Tin má»›i - multi_source_scraper.py")
        print(f"\nğŸ“„ Fetching: {url}")

        self.sleep()
        html = self.fetch_html(url)
        if not html:
            print(f"âš  Failed to fetch page, stopping")
            return all_articles

        # Handle anti-bot cookie redirect (similar to VOV)
        if len(html) < 500 and 'document.cookie' in html and 'window.location.reload' in html:
            print(f"âš  Antibot detected, extracting cookie and retrying...")

            # Extract cookie from JavaScript
            import re as re_module
            cookie_match = re_module.search(r'document\.cookie\s*=\s*"([^"]+)"', html)
            if cookie_match:
                cookie_str = cookie_match.group(1)
                print(f"â†’ Setting cookie: {cookie_str[:50]}...")

                # Parse cookie name and value
                cookie_parts = cookie_str.split('=', 1)
                if len(cookie_parts) == 2:
                    cookie_name = cookie_parts[0]
                    cookie_value = cookie_parts[1].split(';')[0]  # Get value before options

                    # Set cookie in session
                    self.session.cookies.set(cookie_name, cookie_value, domain='laodong.vn', path='/')

                    # Retry request with cookie
                    self.sleep()
                    html = self.fetch_html(url)
                    if not html:
                        print(f"âš  Failed to fetch page after cookie, stopping")
                        return all_articles
                    print(f"âœ“ Got HTML with cookie: {len(html)} chars")
            else:
                print(f"âš  Could not extract cookie, stopping")
                return all_articles

        # Parse listing page
        soup = BeautifulSoup(html, 'html.parser')

        # Find all article tags - LaoDong uses <article> tags for news items
        articles = soup.find_all('article')

        if not articles:
            print(f"âš  No article tags found")
            return all_articles

        print(f"Found {len(articles)} article tags on page")

        # Extract links from articles
        article_links = []
        for article in articles:
            # Find the main link in the article (usually first link or link in title)
            link_el = article.find('a')
            if link_el:
                href = link_el.get('href', '')
                if href:
                    # Make sure link is absolute
                    if not href.startswith('http'):
                        href = f"https://laodong.vn{href}"

                    # Filter out non-article links
                    # Valid article URLs: /xa-hoi/..., /the-thao/..., /suc-khoe/...
                    # Skip: /tin-moi, /thong-tin-doanh-nghiep, category pages
                    if (href not in article_links and
                        '/tin-moi' not in href and
                        '/thong-tin-doanh-nghiep' not in href and
                        not href.endswith('laodong.vn/') and
                        href.count('/') >= 4):  # Article URLs have at least domain/category/slug
                        article_links.append(href)

        if not article_links:
            print(f"âš  No valid article links found")
            return all_articles

        # Limit to requested number of articles
        article_links = article_links[:max_articles]
        print(f"Found {len(article_links)} article URLs")

        # Fetch article details
        for i, article_url in enumerate(article_links, 1):
            print(f"[{i}/{len(article_links)}] Fetching: {article_url[:60]}...")
            self.sleep()

            article_data = self._fetch_article_detail(article_url)
            if article_data:
                all_articles.append(article_data)

        print(f"\nâœ“ Total articles collected: {len(all_articles)}")
        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o LaoDong"""
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # Extract title
        title_el = soup.select_one('h1') or soup.select_one('.article-title')
        title = title_el.get_text(strip=True) if title_el else ''

        if not title:
            print(f"âœ— No title found for: {link[:60]}...")
            return None

        # Extract date from span.time
        # Format: "Thá»© nÄƒm, 01/01/2026 21:59 (GMT+7)"
        published_at = 0
        date_el = soup.select_one('span.time')

        if date_el:
            date_text = date_el.get_text(strip=True)
            # Remove day of week and GMT info
            # "Thá»© nÄƒm, 01/01/2026 21:59 (GMT+7)" -> "01/01/2026 21:59"
            date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})\s+(\d{1,2}):(\d{2})', date_text)
            if date_match:
                day, month, year, hour, minute = date_match.groups()
                try:
                    dt = datetime(int(year), int(month), int(day), int(hour), int(minute))
                    published_at = int(dt.timestamp())
                except Exception as e:
                    print(f"âš  Could not parse date '{date_text}': {e}")

        # Extract content
        content_el = soup.select_one('.detail-content') or soup.select_one('.article-content') or soup.select_one('[itemprop="articleBody"]')
        content = ""

        if content_el:
            paragraphs = content_el.select('p')
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # Extract category from a.main-cat-lnk
        # <a class="main-cat-lnk" href="https://laodong.vn/the-thao"> Thá»ƒ thao </a>
        category = "TIN Má»šI"  # Default (Ä‘Ã£ in hoa)

        category_el = soup.select_one('a.main-cat-lnk')
        if category_el:
            category_text = category_el.get_text(strip=True)
            if category_text:
                category = category_text.upper()  # In hoa category

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA",
            False,
            category,
        )


class ANTTRSSScraper(NewsScraperBase):
    """
    Scraper cho ANTT.vn sá»­ dá»¥ng RSS Feed
    """

    def __init__(self):
        super().__init__()
        self.source = "antt.vn"
        self.rss_url = "https://antt.vn/rss/trang-chu.rss"

    def fetch_news(self) -> List[Tuple]:
        """
        Láº¥y tá»‘i Ä‘a 20 tin tá»©c má»›i nháº¥t tá»« RSS feed cá»§a ANTT
        """
        all_articles = []
        print(f"\nğŸ“¡ Äang Ä‘á»c RSS tá»«: {self.rss_url}")
        
        feed = feedparser.parse(self.rss_url)
        
        if not feed.entries:
            print("âš  KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t nÃ o trong RSS.")
            return []

        entries_to_process = feed.entries[:20]
        print(f"TÃ¬m tháº¥y {len(feed.entries)} bÃ i. Sáº½ xá»­ lÃ½ {len(entries_to_process)} bÃ i má»›i nháº¥t.")

        for entry in entries_to_process:
            link = entry.link
            
            # RSS cá»§a ANTT thÆ°á»ng khÃ´ng cÃ³ tháº» <category> trá»±c tiáº¿p cho tá»«ng entry nhÆ° DÃ¢n TrÃ­
            # Ta sáº½ máº·c Ä‘á»‹nh lÃ  'TIN Má»šI' vÃ  Ä‘á»ƒ hÃ m detail láº¥y tá»« Meta Tag
            rss_category = getattr(entry, 'category', 'TIN Má»šI')
            
            print(f"Fetching: {link[:60]}...")
            self.sleep() # Äáº£m báº£o khÃ´ng crawl quÃ¡ nhanh
            
            article_data = self._fetch_article_detail(link, rss_category)
            if article_data:
                all_articles.append(article_data)

        print(f"\nâœ“ Tá»•ng sá»‘ bÃ i viáº¿t tá»« ANTT thu tháº­p Ä‘Æ°á»£c: {len(all_articles)}")
        return all_articles

    def _fetch_article_detail(self, link: str, rss_category: str = None) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o tá»« ANTT"""
        # Fix: ANTT RSS tráº£ vá» URLs dáº¡ng "-nXXXXXX.html" nhÆ°ng website dÃ¹ng "-XXXXXX.htm"
        # Cáº§n bá» chá»¯ 'n' trÆ°á»›c sá»‘ vÃ  Ä‘á»•i extension
        import re
        original_link = link
        if link.endswith('.html'):
            # Thay -nXXXXXX.html thÃ nh -XXXXXX.htm
            link = re.sub(r'-n(\d+)\.html$', r'-\1.htm', link)
        elif link.endswith('.htm'):
            # Náº¿u Ä‘Ã£ lÃ  .htm nhÆ°ng váº«n cÃ³ 'n', thÃ¬ bá» 'n'
            link = re.sub(r'-n(\d+)\.htm$', r'-\1.htm', link)

        if link != original_link:
            print(f"  â†’ URL fixed: ...{original_link[-50:]} â†’ ...{link[-50:]}")

        # ANTT server tráº£ vá» encoding sai (ISO-8859-1) nhÆ°ng content lÃ  UTF-8
        # Cáº§n fetch vá»›i explicit UTF-8 encoding
        try:
            resp = self.session.get(link, headers=self.headers, timeout=30)
            resp.raise_for_status()
            # Force UTF-8 decoding
            html = resp.content.decode('utf-8', errors='replace')
        except Exception as e:
            print(f"âœ— Error fetching {link}: {e}")
            return None

        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # 1. TrÃ­ch xuáº¥t TiÃªu Ä‘á» (ANTT dÃ¹ng div.title_detail)
        title_el = soup.select_one('.title_detail') or soup.select_one('h1')
        title = title_el.get_text(strip=True) if title_el else ''

        if not title:
            print(f"  âš  No title found for {link[-60:]}")
            return None
        
        # 2. TrÃ­ch xuáº¥t NgÃ y xuáº¥t báº£n
        # ANTT dÃ¹ng format: "02/01/2026 11:02:40"
        published_at = 0
        date_el = soup.select_one('.time_home')
        if date_el:
            date_text = date_el.get_text(strip=True)
            # Format: dd/mm/yyyy hh:mm:ss
            date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})\s+(\d{1,2}):(\d{2}):(\d{2})', date_text)
            if date_match:
                day, month, year, hour, minute, second = date_match.groups()
                try:
                    dt = datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))
                    published_at = int(dt.timestamp())
                except:
                    pass
        
        # 3. TrÃ­ch xuáº¥t Ná»™i dung
        content = ""
        # ANTT: content náº±m trong cÃ¡c tháº» p trá»±c tiáº¿p trong content_main
        content_el = soup.select_one('.content_main')
        if content_el:
            # Loáº¡i bá» cÃ¡c thÃ nh pháº§n khÃ´ng cáº§n thiáº¿t
            for unwanted in content_el.select('.related-box, .ad-container, script, style, .tag_detail, .article-footer'):
                unwanted.decompose()

            paragraphs = content_el.select('p')
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # 4. TrÃ­ch xuáº¥t ChuyÃªn má»¥c
        # ANTT: category náº±m trong breadcrumb (item thá»© 2)
        category = "TIN Má»šI"

        # TÃ¬m breadcrumb items
        breadcrumb_items = soup.select('a[itemprop="url"] span[itemprop="title"]')

        # Láº¥y item thá»© 2 (index 1) - Ä‘Ã³ lÃ  category chÃ­nh
        # Item 1: Trang chá»§, Item 2: Category, Item 3: Sub-category
        if len(breadcrumb_items) >= 2:
            category = breadcrumb_items[1].get_text(strip=True)

        category = category.upper().strip()

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",   # stock_related
            "NA",   # sentiment_score
            False,  # server_pushed
            category,
        )

# class AgroMonitorScraper(NewsScraperBase):
#     """
#     Scraper cho Agromonitor.vn - crawl tá»« trang "Trang chá»§"
 # #     Sá»­ dá»¥ng Selenium Ä‘á»ƒ xá»­ lÃ½ trang load báº±ng JavaScript vÃ  tá»± Ä‘á»™ng Ä‘Äƒng nháº­p.
#     """
 # #     def __init__(self):
#         super().__init__()
#         self.source = "agromonitor.vn"
#         self.headers['Referer'] = 'https://agromonitor.vn/'
#         self.driver = None  # Selenium driver instance
#         self.logged_in = False  # Track login status
 #     # def _get_driver(self):
        # """Táº¡o vÃ  configure Selenium Chrome driver"""
        # from selenium import webdriver
        # from selenium.webdriver.chrome.options import Options
        # from selenium.webdriver.chrome.service import Service
        # from webdriver_manager.chrome import ChromeDriverManager
        # import os
 #         # chrome_options = Options()
        # chrome_options.add_argument('--headless')
        # chrome_options.add_argument('--no-sandbox')
        # chrome_options.add_argument('--disable-dev-shm-usage')
        # chrome_options.add_argument('--disable-gpu')
        # chrome_options.add_argument('--window-size=1920,1080')
        # chrome_options.add_argument(f'user-agent={self.headers["User-Agent"]}')
 #         # Chá»‰ Ä‘á»‹nh Ä‘Æ°á»ng dáº«n Chrome binary
        # chrome_paths = [
            # r'C:\Program Files\Google\Chrome\Application\chrome.exe',
            # r'C:\Program Files (x86)\Google\Chrome\Application\chrome.exe',
        # ]
        # for chrome_path in chrome_paths:
            # if os.path.exists(chrome_path):
                # chrome_options.binary_location = chrome_path
                # break
 #         # service = Service(ChromeDriverManager().install())
        # return webdriver.Chrome(service=service, options=chrome_options)
 #     # def _login(self):
        # """Tá»± Ä‘á»™ng Ä‘Äƒng nháº­p vÃ o AgroMonitor"""
        # from selenium.webdriver.common.by import By
        # from selenium.webdriver.support.ui import WebDriverWait
        # from selenium.webdriver.support import expected_conditions as EC
        # from dotenv import load_dotenv
        # import os
        # import time
 #         # Load credentials tá»« .env
        # load_dotenv()
        # email = os.getenv('AGROMONITOR_EMAIL')
        # password = os.getenv('AGROMONITOR_PASSWORD')
 #         # if not email or not password or email == 'your_email@example.com':
            # print("âš  Warning: AgroMonitor credentials not configured in .env file")
            # print("  Please update AGROMONITOR_EMAIL and AGROMONITOR_PASSWORD in .env")
            # return False
 #         # try:
            # print("â†’ Logging in to AgroMonitor...")
 #             # Má»Ÿ trang login
            # self.driver.get("https://agromonitor.vn/login")
            # time.sleep(3)  # Äá»£i trang load
 #             # ÄÃ³ng popup/modal náº¿u cÃ³ (nÃºt "ÄÃ³ng" hoáº·c "X")
            # try:
                # close_buttons = self.driver.find_elements(By.CSS_SELECTOR, "button.ant-modal-close, button.btn-danger")
                # for btn in close_buttons:
                    # if btn.is_displayed():
                        # print("  â†’ Closing popup/modal...")
                        # btn.click()
                        # time.sleep(1)
                        # break
            # except:
                # pass  # KhÃ´ng cÃ³ popup, tiáº¿p tá»¥c
 #             # TÃ¬m vÃ  Ä‘iá»n username
            # username_input = self.driver.find_element(By.NAME, "username")
            # username_input.clear()
            # username_input.send_keys(email)
 #             # TÃ¬m vÃ  Ä‘iá»n password
            # password_input = self.driver.find_element(By.NAME, "password")
            # password_input.clear()
            # password_input.send_keys(password)
 #             # TÃ¬m nÃºt Ä‘Äƒng nháº­p
            # login_button = self.driver.find_element(By.CSS_SELECTOR, "button[type='submit']")
 #             # Scroll Ä‘áº¿n nÃºt login
            # self.driver.execute_script("arguments[0].scrollIntoView(true);", login_button)
            # time.sleep(1)
 #             # Click báº±ng JavaScript (trÃ¡nh bá»‹ intercepted)
            # print("  â†’ Clicking login button...")
            # self.driver.execute_script("arguments[0].click();", login_button)
 #             # Äá»£i login hoÃ n táº¥t
            # time.sleep(5)
 #             # Kiá»ƒm tra xem Ä‘Ã£ login thÃ nh cÃ´ng chÆ°a
            # current_url = self.driver.current_url
 #             # Debug: Save HTML after login attempt
            # with open('debug_after_login.html', 'w', encoding='utf-8') as f:
                # f.write(self.driver.page_source)
            # print(f"  â†’ Saved HTML after login to debug_after_login.html")
            # print(f"  â†’ Current URL: {current_url}")
 #             # TÃ¬m error message náº¿u cÃ³
            # try:
                # error_elements = self.driver.find_elements(By.CSS_SELECTOR, ".ant-message-error, .error, .alert-danger")
                # for err in error_elements:
                    # if err.is_displayed():
                        # print(f"  âœ— Error message: {err.text}")
            # except:
                # pass
 #             # if "login" not in current_url:
                # print("âœ“ Login successful!")
                # self.logged_in = True
                # return True
            # else:
                # print("âœ— Login failed - still on login page")
                # return False
 #         # except Exception as e:
            # print(f"âœ— Login error: {e}")
            # import traceback
            # traceback.print_exc()
            # return False
 #     # def fetch_news(self, max_pages: int = 1, max_articles_per_page: int = 20) -> List[Tuple]:
        # all_articles = []
        # seen_urls = {}
        # import time
 #         # url = "https://agromonitor.vn/category/16/trang-chu"
        # print(f"\nğŸ“„ Fetching Agromonitor (with auto-login): {url}")
 #         # try:
            # Khá»Ÿi táº¡o driver
            # print("â†’ Starting Chrome browser...")
            # self.driver = self._get_driver()
 #             # ÄÄƒng nháº­p
            # if not self._login():
                # print("âš  Failed to login, aborting scrape")
                # return all_articles
 #             # Má»Ÿ trang category
            # print(f"â†’ Opening category page...")
            # self.driver.get(url)
            # time.sleep(5)  # Äá»£i trang load
 #             # Láº¥y HTML Ä‘Ã£ render
            # html = self.driver.page_source
            # print(f"âœ“ Got rendered HTML: {len(html)} chars")
 #             # Parse HTML vá»›i BeautifulSoup
            # soup = BeautifulSoup(html, 'html.parser')
 #             # TÃ¬m cÃ¡c link bÃ i viáº¿t (Agromonitor sá»­ dá»¥ng pattern /post/ID/slug)
            # links = soup.select('a[href*="/post/"]')
 #             # article_urls = []
            # for link in links:
                # href = link.get('href', '')
                # if href:
                    # Chuyá»ƒn link tÆ°Æ¡ng Ä‘á»‘i thÃ nh tuyá»‡t Ä‘á»‘i
                    # full_url = href if href.startswith('http') else f"https://agromonitor.vn{href}"
 #                     # if full_url not in seen_urls:
                        # seen_urls[full_url] = True
                        # article_urls.append(full_url)
 #             # Giá»›i háº¡n sá»‘ lÆ°á»£ng bÃ i
            # article_urls = article_urls[:max_articles_per_page]
            # print(f"Found {len(article_urls)} potential article URLs")
 #             # Fetch chi tiáº¿t tá»«ng bÃ i viáº¿t (sá»­ dá»¥ng same driver instance)
            # for i, article_url in enumerate(article_urls, 1):
                # print(f"[{i}/{len(article_urls)}] Fetching: {article_url[:65]}...")
                # article_data = self._fetch_article_detail(article_url)
                # if article_data:
                    # all_articles.append(article_data)
                # time.sleep(2)  # Delay giá»¯a cÃ¡c request
 #         # except Exception as e:
            # print(f"âš  Error during scraping: {e}")
            # import traceback
            # traceback.print_exc()
        # finally:
            # Cleanup: ÄÃ³ng browser
            # if self.driver:
                # self.driver.quit()
                # print("â†’ Browser closed")
                # self.driver = None
                # self.logged_in = False
 #         # return all_articles
 #     # def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        # """Fetch chi tiáº¿t bÃ i viáº¿t sá»­ dá»¥ng driver instance Ä‘Ã£ Ä‘Äƒng nháº­p"""
        # import time
 #         # if not self.driver or not self.logged_in:
            # print(f"  âœ— Driver not initialized or not logged in")
            # return None
 #         # try:
            # Má»Ÿ trang detail
            # self.driver.get(link)
            # time.sleep(3)  # Äá»£i trang load
 #             # Láº¥y HTML Ä‘Ã£ render
            # html = self.driver.page_source
 #         # except Exception as e:
            # print(f"  âœ— Error fetching detail: {e}")
            # return None
 #         # if not html:
            # return None
 #         # soup = BeautifulSoup(html, 'html.parser')
 #         # 1. Extract Title: ThÆ°á»ng náº±m trong h1 hoáº·c class title-detail
        # title_el = soup.select_one('h1') or soup.select_one('.title-detail')
        # title = title_el.get_text(strip=True) if title_el else ''
 #         # if not title:
            # print(f"  âœ— No title found for: {link[:60]}...")
            # return None
 #         # 2. Extract Date: Agromonitor thÆ°á»ng ghi "02:37 31/12/2025"
        # published_at = 0
        # date_el = soup.select_one('.date') or soup.select_one('.time')
        # if date_el:
            # date_text = date_el.get_text(strip=True)
            # Regex tÃ¬m: HH:mm DD/MM/YYYY hoáº·c DD/MM/YYYY
            # date_match = re.search(r'(\d{1,2}):(\d{2})\s+(\d{1,2})/(\d{1,2})/(\d{4})', date_text)
            # if date_match:
                # hour, minute, day, month, year = date_match.groups()
                # try:
                    # dt = datetime(int(year), int(month), int(day), int(hour), int(minute))
                    # published_at = int(dt.timestamp())
                # except:
                    # pass
 #         # 3. Extract Content: ThÆ°á»ng náº±m trong cÃ¡c tháº» div cÃ³ class content hoáº·c detail-content
        # content = ""
        # content_el = soup.select_one('.content-detail') or soup.select_one('.post-content')
        # if content_el:
            # Loáº¡i bá» cÃ¡c pháº§n khÃ´ng cáº§n thiáº¿t nhÆ° quáº£ng cÃ¡o, tag náº¿u cÃ³
            # paragraphs = content_el.find_all(['p', 'div'], recursive=False)
            # content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
 #         # 4. Extract Category
        # category = "NÃ”NG Sáº¢N" # Máº·c Ä‘á»‹nh
        # Thá»­ láº¥y tá»« breadcrumb náº¿u cÃ³
        # breadcrumb = soup.select_one('.breadcrumb')
        # if breadcrumb:
            # category = breadcrumb.get_text(" > ", strip=True)
 #         # return (
            # published_at,
            # title,
            # link,
            # content,
            # self.source,
            # "NA",
            # "NA",
            # False,
            # category,
        # )

class CNARSSScraper(NewsScraperBase):
    """
    Scraper cho ChannelNewsAsia.com sá»­ dá»¥ng RSS Feed
    """

    def __init__(self):
        super().__init__()
        self.source = "channelnewsasia.com"
        # Link RSS chÃ­nh thá»©c cá»§a CNA
        self.rss_url = "https://www.channelnewsasia.com/api/v1/rss-outbound-feed?_format=xml"

    def fetch_news(self) -> List[Tuple]:
        """
        Láº¥y tá»‘i Ä‘a 20 tin tá»©c má»›i nháº¥t tá»« RSS feed cá»§a CNA
        """
        all_articles = []
        print(f"\nğŸ“¡ Äang Ä‘á»c RSS tá»«: {self.rss_url}")
        
        # 1. Sá»­ dá»¥ng feedparser Ä‘á»ƒ Ä‘á»c ná»™i dung RSS
        feed = feedparser.parse(self.rss_url)
        
        if not feed.entries:
            print("âš  KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t nÃ o trong RSS.")
            return []

        # 2. Giá»›i háº¡n 20 bÃ i viáº¿t Ä‘áº§u tiÃªn
        entries_to_process = feed.entries[:20]
        print(f"TÃ¬m tháº¥y {len(feed.entries)} bÃ i. Sáº½ xá»­ lÃ½ {len(entries_to_process)} bÃ i má»›i nháº¥t.")

        for entry in entries_to_process:
            link = entry.link
            
            # Láº¥y Category (CNA thÆ°á»ng Ä‘á»ƒ trong tags hoáº·c category field cá»§a RSS)
            rss_category = "WORLD" # Máº·c Ä‘á»‹nh cho CNA
            if hasattr(entry, 'tags'):
                rss_category = entry.tags[0].term if entry.tags else "ASIA"
            
            # Láº¥y timestamp trá»±c tiáº¿p tá»« RSS (CNA há»— trá»£ cá»±c tá»‘t pháº§n nÃ y)
            published_at = 0
            if hasattr(entry, 'published_parsed'):
                published_at = int(datetime(*entry.published_parsed[:6]).timestamp())
            
            print(f"Fetching: {link[:60]}...")
            self.sleep()
            
            # Truyá»n cÃ¡c thÃ´ng tin Ä‘Ã£ cÃ³ vÃ o hÃ m detail
            article_data = self._fetch_article_detail(link, rss_category, published_at)
            if article_data:
                all_articles.append(article_data)

        print(f"\nâœ“ Tá»•ng sá»‘ bÃ i viáº¿t CNA thu tháº­p Ä‘Æ°á»£c: {len(all_articles)}")
        return all_articles

    def _fetch_article_detail(self, link: str, rss_category: str = None, rss_published_at: int = 0) -> Optional[Tuple]:
        """Fetch chi tiáº¿t má»™t bÃ i bÃ¡o tá»« CNA"""
        html = self.fetch_html(link)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. TrÃ­ch xuáº¥t TiÃªu Ä‘á» (CNA thÆ°á»ng dÃ¹ng class h1.entry-title hoáº·c class liÃªn quan Ä‘áº¿n content)
        title_el = soup.select_one('h1.page-title') or soup.select_one('h1')
        title = title_el.get_text(strip=True) if title_el else ''
        
        if not title:
            return None
        
        # 2. TrÃ­ch xuáº¥t NgÃ y xuáº¥t báº£n (Æ¯u tiÃªn láº¥y tá»« RSS Ä‘Ã£ cÃ³ á»Ÿ bÆ°á»›c trÆ°á»›c)
        published_at = rss_published_at
        
        # 3. TrÃ­ch xuáº¥t Ná»™i dung
        content = ""
        # Selector cho ná»™i dung bÃ i viáº¿t cá»§a CNA (thÆ°á»ng lÃ  div chá»©a text-long hoáº·c cÃ¡c tháº» p trong article)
        content_el = soup.select_one('.content-wrapper') or soup.select_one('.text-long')
        if content_el:
            # Loáº¡i bá» cÃ¡c thÃ nh pháº§n rÃ¡c nhÆ° "Also read", Video player, Ads
            for unwanted in content_el.select('.related-section, .video-embed, .ad-slot, .infographic'):
                unwanted.decompose()
                
            paragraphs = content_el.select('p')
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

        # 4. TrÃ­ch xuáº¥t ChuyÃªn má»¥c
        category = rss_category if rss_category else "WORLD"
        category = category.upper().strip()

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",   # stock_related
            "NA",   # sentiment_score
            False,  # server_pushed
            category,
        )




class KinhTeNgoaiThuongScraper(NewsScraperBase):
    def __init__(self):
        super().__init__()
        self.source = "kinhtengoaithuong.vn"
        # ThÃªm header Ä‘á»ƒ giáº£ láº­p trÃ¬nh duyá»‡t xem tin tá»©c
        self.headers.update({
            'Referer': 'https://www.google.com/',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        })

    def fetch_news(self, max_articles: int = 15) -> List[Tuple]:
        all_articles = []
        url = "https://kinhtengoaithuong.vn/" 

        print(f"\nğŸ“¡ Äang quÃ©t trang chá»§: {url}")
        html = self.fetch_html(url)
        
        if not html:
            print("âš  KhÃ´ng thá»ƒ truy cáº­p trang chá»§ kinhtengoaithuong.vn")
            return []

        soup = BeautifulSoup(html, 'html.parser')

        # 1. TÃ¬m táº¥t cáº£ cÃ¡c link bÃ i viáº¿t
        # Äáº·c Ä‘iá»ƒm: CÃ¡c bÃ i viáº¿t trÃªn trang nÃ y thÆ°á»ng náº±m trong cÃ¡c tháº» h2, h3 
        # hoáº·c div cÃ³ class chá»©a 'post', 'item', 'title'
        potential_links = soup.select('h2 a, h3 a, .post-title a, .entry-title a')
        
        article_urls = []
        seen_urls = set()

        for a in potential_links:
            href = a.get('href', '')
            if not href: continue

            # Chuáº©n hÃ³a link tuyá»‡t Ä‘á»‘i
            if href.startswith('/'):
                href = f"https://kinhtengoaithuong.vn{href}"
            
            # Lá»c: Pháº£i thuá»™c domain, khÃ´ng pháº£i trang chá»§, khÃ´ng pháº£i link rÃ¡c
            if "kinhtengoaithuong.vn" in href and href != "https://kinhtengoaithuong.vn/":
                # Loáº¡i bá» cÃ¡c trang chá»©c nÄƒng
                if not any(x in href for x in ['/category/', '/tag/', '/author/', '/contact/', '/gioi-thieu/']):
                    if href not in seen_urls:
                        seen_urls.add(href)
                        article_urls.append(href)

        # 2. Náº¿u Selector trÃªn khÃ´ng ra káº¿t quáº£, dÃ¹ng Regex Ä‘á»ƒ quÃ©t toÃ n bá»™ link
        if not article_urls:
            print("ğŸ” Thá»­ quÃ©t link báº±ng phÆ°Æ¡ng thá»©c Regex...")
            for a in soup.find_all('a', href=True):
                href = a['href']
                # Link bÃ i viáº¿t thÆ°á»ng cÃ³ Ä‘á»™ sÃ¢u path > 3 (vÃ­ dá»¥ domain.vn/ten-bai-viet/)
                if "kinhtengoaithuong.vn" in href and len(href.strip('/').split('/')) >= 3:
                     if href not in seen_urls and not any(x in href for x in ['/category/', '/tag/']):
                        seen_urls.add(href)
                        article_urls.append(href)

        article_urls = article_urls[:max_articles]
        print(f"âœ“ TÃ¬m tháº¥y {len(article_urls)} bÃ i viáº¿t tá»« trang chá»§.")

        for i, article_url in enumerate(article_urls, 1):
            print(f"[{i}/{len(article_urls)}] Fetching: {article_url[:50]}...")
            self.sleep()
            article_data = self._fetch_article_detail(article_url)
            if article_data:
                all_articles.append(article_data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. TiÃªu Ä‘á»
        title_el = soup.find('h1')
        title = title_el.get_text(strip=True) if title_el else ''
        if not title: return None

        # 2. NgÃ y xuáº¥t báº£n
        published_at = int(datetime.now().timestamp())
        # TÃ¬m trong cÃ¡c tháº» meta hoáº·c class date phá»• biáº¿n cá»§a trang
        date_el = soup.select_one('.detail-date, .post-date, .time')
        if date_el:
            date_match = re.search(r'(\d{1,2})/(\d{1,2})/(\d{4})', date_el.get_text())
            if date_match:
                d, m, y = date_match.groups()
                try: published_at = int(datetime(int(y), int(m), int(d)).timestamp())
                except: pass

        # 3. Ná»™i dung (Sá»­ dá»¥ng Selector: .article-content)
        content_el = soup.select_one('.article-content')
        content = ""
        if content_el:
            # Thu tháº­p dá»¯ liá»‡u tá»« cáº£ tháº» <p> vÃ  cÃ¡c dÃ²ng trong <table> (chÃº thÃ­ch áº£nh)
            # Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng sÃ³t thÃ´ng tin quan trá»ng
            parts = []
            
            # Láº¥y cÃ¡c Ä‘oáº¡n vÄƒn báº£n
            for p in content_el.find_all(['p', 'td']):
                # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a vÃ  cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t
                txt = p.get_text(strip=True)
                if txt and len(txt) > 5: # Chá»‰ láº¥y cÃ¡c Ä‘oáº¡n cÃ³ nghÄ©a
                    parts.append(txt)
            
            content = ' '.join(parts)

        # 4. ChuyÃªn má»¥c (Category)
        category = "TÃ€I CHÃNH" # Default dá»±a trÃªn link máº«u
        # Theo element báº¡n Ä‘Æ°a: <a itemprop="item" ... title="TÃ i chÃ­nh">
        # ChÃºng ta tÃ¬m danh sÃ¡ch Breadcrumb vÃ  láº¥y pháº§n tá»­ thá»© 2 (sau Trang chá»§)
        bread_items = soup.select('a[itemprop="item"]')
        if len(bread_items) >= 2:
            # Láº¥y text tá»« thuá»™c tÃ­nh title hoáº·c tá»« tháº» span bÃªn trong
            category = bread_items[1].get('title') or bread_items[1].get_text(strip=True)
        
        category = category.upper().strip()

        return (
            published_at, 
            title, 
            link, 
            content, 
            self.source, 
            "NA", "NA", False, 
            category
        )


class ThoiBaoNganHangScraper(NewsScraperBase):
    def __init__(self):
        super().__init__()
        self.source = "thoibaonganhang.vn"
        self.headers.update({
            'Referer': 'https://thoibaonganhang.vn/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

    def fetch_news(self, max_articles: int = 15) -> List[Tuple]:
        all_articles = []
        url = "https://thoibaonganhang.vn/"

        print(f"\nğŸ“¡ Äang quÃ©t trang chá»§ Thá»i bÃ¡o NgÃ¢n hÃ ng: {url}")
        html = self.fetch_html(url)
        if not html: return []

        soup = BeautifulSoup(html, 'html.parser')
        article_urls = []
        seen_urls = set()
        
        # QuÃ©t táº¥t cáº£ link bÃ i viáº¿t cÃ³ Ä‘uÃ´i .html vÃ  chá»©a sá»‘ ID
        links = soup.find_all('a', href=True)
        for link in links:
            href = link.get('href', '')
            if ".html" in href and any(char.isdigit() for char in href):
                if not href.startswith('http'):
                    href = f"https://thoibaonganhang.vn{href}"
                
                if not any(x in href for x in ['/video-', '/anh-', '/chuyen-muc/', '/tags/']):
                    if href not in seen_urls:
                        seen_urls.add(href)
                        article_urls.append(href)

        article_urls = article_urls[:max_articles]
        print(f"âœ“ TÃ¬m tháº¥y {len(article_urls)} bÃ i viáº¿t tiá»m nÄƒng.")

        for i, article_url in enumerate(article_urls, 1):
            print(f"[{i}/{len(article_urls)}] Fetching: {article_url[:60]}...")
            self.sleep() # KhÃ´ng truyá»n tham sá»‘ Ä‘á»ƒ trÃ¡nh lá»—i NewsScraperBase
            article_data = self._fetch_article_detail(article_url)
            if article_data:
                all_articles.append(article_data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. TiÃªu Ä‘á»
        title_el = soup.find('h1')
        if not title_el: return None
        title = title_el.get_text(strip=True)

        # 2. NgÃ y xuáº¥t báº£n (.format_date)
        published_at = int(datetime.now().timestamp())
        date_el = soup.select_one('.format_date')
        if date_el:
            try:
                dt = datetime.strptime(date_el.get_text(strip=True), '%d/%m/%Y')
                published_at = int(dt.timestamp())
            except: pass

        # 3. ChuyÃªn má»¥c (.bx-cat-link)
        category = "NGÃ‚N HÃ€NG"
        cat_el = soup.select_one('.bx-cat-link')
        if cat_el:
            category = cat_el.get_text(strip=True).upper()

        # 4. Ná»™i dung (Gá»™p Sapo + Body)
        content = ""
        container = soup.select_one('.article-detail-body')
        if container:
            content_box = copy.copy(container)
            
            # Láº¥y pháº§n Sapo (TÃ³m táº¯t)
            sapo_el = content_box.select_one('.article-detail-desc')
            sapo_text = sapo_el.get_text(strip=True) if sapo_el else ""
            
            # Loáº¡i bá» rÃ¡c trÆ°á»›c khi láº¥y Body
            for noise in content_box.select('.article-share-button, .article-extension, script, style, .article-detail-desc'):
                noise.decompose()
            
            # Láº¥y cÃ¡c Ä‘oáº¡n vÄƒn báº£n chÃ­nh
            paragraphs = content_box.find_all('p')
            if paragraphs:
                body_text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            else:
                body_text = content_box.get_text(" ", strip=True)
            
            # Káº¿t há»£p Sapo vÃ  Body
            content = f"{sapo_text} {body_text}".strip()

        if not content or len(content) < 50:
            return None

    
        return (published_at, title, link, content, self.source, "NA", "NA", False, category)



class TaiChinhDoanhNghiepScraper(NewsScraperBase):
    def __init__(self):
        super().__init__()
        self.source = "taichinhdoanhnghiep.net.vn"
        self.headers.update({
            'Referer': 'https://taichinhdoanhnghiep.net.vn/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

    def fetch_news(self, max_articles: int = 15) -> List[Tuple]:
        all_articles = []
        # QuÃ©t trang chá»§ Ä‘á»ƒ láº¥y danh sÃ¡ch bÃ i má»›i
        url = "https://taichinhdoanhnghiep.net.vn/"
        html = self.fetch_html(url)
        if not html: return []

        soup = BeautifulSoup(html, 'html.parser')
        article_urls = []
        seen_urls = set()
        
        # Trang nÃ y thÆ°á»ng dÃ¹ng link káº¿t thÃºc báº±ng -dXXXXX.html
        links = soup.find_all('a', href=True)
        for link in links:
            href = link.get('href', '')
            if "-d" in href and ".html" in href:
                if not href.startswith('http'):
                    href = f"https://taichinhdoanhnghiep.net.vn{href}"
                if href not in seen_urls:
                    seen_urls.add(href)
                    article_urls.append(href)

        for article_url in article_urls[:max_articles]:
            self.sleep() # Sá»­a lá»—i sleep() khÃ´ng tham sá»‘
            data = self._fetch_article_detail(article_url)
            if data: all_articles.append(data)
        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')

        # 1. TiÃªu Ä‘á» (Æ¯u tiÃªn láº¥y tá»« #getTitle, fallback sang h1)
        title_el = soup.select_one('#getTitle') or soup.find('h1')
        if not title_el:
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y title cho URL: {link}")
            return None
        title = title_el.get_text(strip=True)
        if not title:
            print(f"âš ï¸ Title rá»—ng cho URL: {link}")
            return None

        # 2. ChuyÃªn má»¥c (Theo máº«u: .c-j a)
        category = "TÃ€I CHÃNH"
        cat_el = soup.select_one('.c-j a')
        if cat_el:
            category = cat_el.get_text(strip=True).upper()

        # 3. NgÃ y xuáº¥t báº£n (Theo máº«u: .bx-time)
        published_at = int(datetime.now().timestamp())
        date_el = soup.select_one('.bx-time')
        if date_el:
            date_text = date_el.get_text(strip=True) # Dáº¡ng: 01/01/2026, 10:52
            try:
                # Cáº¯t láº¥y pháº§n ngÃ y trÆ°á»›c dáº¥u pháº©y
                clean_date = date_text.split(',')[0].strip()
                dt = datetime.strptime(clean_date, '%d/%m/%Y')
                published_at = int(dt.timestamp())
            except: pass

        # 4. Ná»™i dung (Xá»­ lÃ½ khá»‘i .chuyennoidung vÃ  #noidung)
        content = ""
        container = soup.select_one('#noidung')
        if container:
            # Sao chÃ©p Ä‘á»ƒ lá»c rÃ¡c
            content_box = copy.copy(container)
            
            # LOáº I Bá» RÃC: Audio player, khá»‘i social dÆ°á»›i bÃ i, quáº£ng cÃ¡o
            # Theo máº«u báº¡n gá»­i: .audio_box, .detail-share-2, .qc1, blockquote
            for noise in content_box.select('.audio_box, .detail-share-2, .qc1, blockquote, script, .audio_tool'):
                noise.decompose()
            
            # Láº¥y Sapo (ThÆ°á»ng náº±m trong tháº» h2 hoáº·c cÃ³ id getIntro)
            sapo_el = content_box.select_one('#getIntro, h2')
            sapo_text = sapo_el.get_text(strip=True) if sapo_el else ""
            
            # Láº¥y cÃ¡c Ä‘oáº¡n vÄƒn báº£n (p)
            paragraphs = content_box.find_all('p')
            body_parts = []
            for p in paragraphs:
                txt = p.get_text(strip=True)
                if txt: body_parts.append(txt)
            
            body_text = ' '.join(body_parts)
            content = f"{sapo_text} {body_text}".strip()

        if len(content) < 50: return None

        

        return (published_at, title, link, content, self.source, "NA", "NA", False, category)


class BaoChinhPhuScraper(NewsScraperBase):
    def __init__(self):
        super().__init__()
        self.source = "baochinhphu.vn"
        self.headers.update({
            'Referer': 'https://baochinhphu.vn/',
        })

    def fetch_news(self, max_articles: int = 15) -> List[Tuple]:
        all_articles = []
        url = "https://baochinhphu.vn/tin-moi.htm"
        
        print(f"\nğŸ“¡ Äang quÃ©t BÃ¡o ChÃ­nh phá»§: {url}")
        html = self.fetch_html(url)
        if not html: return []

        soup = BeautifulSoup(html, 'html.parser')
        article_urls = []
        seen_urls = set()
        
        # BÃ¡o ChÃ­nh phá»§ thÆ°á»ng bá»c link trong khá»‘i cÃ³ class 'story' hoáº·c 'box-category'
        links = soup.select('a[data-role="title"], .story__title a, .box-category-link-title')
        for link in links:
            href = link.get('href', '')
            if href and href.endswith('.htm'):
                if not href.startswith('http'):
                    href = f"https://baochinhphu.vn{href}"
                
                if href not in seen_urls:
                    seen_urls.add(href)
                    article_urls.append(href)
            
            if len(article_urls) >= max_articles:
                break

        for i, article_url in enumerate(article_urls, 1):
            print(f"[{i}/{len(article_urls)}] Fetching: {article_url[:60]}...")
            self.sleep()
            article_data = self._fetch_article_detail(article_url)
            if article_data:
                all_articles.append(article_data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. TiÃªu Ä‘á» (data-role="title")
        title_el = soup.select_one('[data-role="title"]')
        if not title_el: return None
        title = title_el.get_text(strip=True)

        # 2. NgÃ y xuáº¥t báº£n (Xá»­ lÃ½ chuá»—i 02/01/2026 ... 15:21)
        published_at = int(datetime.now().timestamp())
        date_container = soup.select_one('[data-role="publishdate"]')
        if date_container:
            # Loáº¡i bá» cÃ¡c tag con (nhÆ° icon SVG) Ä‘á»ƒ láº¥y text thuáº§n
            raw_date = date_container.get_text(" ", strip=True)
            # DÃ¹ng regex láº¥y Ä‘á»‹nh dáº¡ng dd/mm/yyyy
            date_match = re.search(r'(\d{2}/\d{2}/\d{4})', raw_date)
            if date_match:
                try:
                    dt = datetime.strptime(date_match.group(1), '%d/%m/%Y')
                    published_at = int(dt.timestamp())
                except: pass

        # 3. ChuyÃªn má»¥c (data-role="cate-name")
        category = "CHÃNH TRá»Š"
        cat_el = soup.select_one('[data-role="cate-name"]')
        if cat_el:
            category = cat_el.get_text(strip=True).upper()

        # 4. Ná»™i dung (data-role="content")
        content = ""
        body_container = soup.select_one('[data-role="content"]')
        if body_container:
            content_box = copy.copy(body_container)
            
            # LOáº I Bá» RÃC:
            # - RelatedNewsBox (Tin liÃªn quan giá»¯a bÃ i)
            # - button-dowload-img (NÃºt táº£i áº£nh)
            # - script, style
            for noise in content_box.select('.VCSortableInPreviewMode[type="RelatedNewsBox"], .button-dowload-img, script, style'):
                noise.decompose()
            
            # Láº¥y toÃ n bá»™ text tá»« cÃ¡c tháº» p
            paragraphs = content_box.find_all(['p', 'h2', 'h3'])
            text_parts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
            content = ' '.join(text_parts).strip()

        if len(content) < 50: return None


        return (published_at, title, link, content, self.source, "NA", "NA", False, category)


class TinNhanhChungKhoanScraper(NewsScraperBase):
    """
    Scraper cho tinnhanhchungkhoan.vn
    Láº¥y 10 bÃ i viáº¿t má»›i nháº¥t tá»« trang chá»§
    """
    def __init__(self):
        super().__init__()
        self.source = "tinnhanhchungkhoan.vn"
        self.headers.update({
            'Referer': 'https://www.tinnhanhchungkhoan.vn/',
        })

    def fetch_news(self, max_articles: int = 10) -> List[Tuple]:
        """Láº¥y bÃ i viáº¿t má»›i nháº¥t tá»« trang chá»§"""
        all_articles = []
        url = "https://www.tinnhanhchungkhoan.vn/"

        print(f"\nğŸ“¡ Äang quÃ©t Tin nhanh chá»©ng khoÃ¡n: {url}")
        html = self.fetch_html(url)
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')

        # TÃ¬m cÃ¡c link bÃ i viáº¿t trÃªn trang chá»§
        article_urls = []
        seen_urls = set()

        # TÃ¬m cÃ¡c link bÃ i viáº¿t (thá»­ nhiá»u selector phá»• biáº¿n)
        links = soup.select('article a, .news-item a, .article-link, h2 a, h3 a, .cms-link a')

        for link in links:
            href = link.get('href', '')
            if not href:
                continue

            # Chuáº©n hÃ³a URL
            if not href.startswith('http'):
                if href.startswith('/'):
                    href = f"https://www.tinnhanhchungkhoan.vn{href}"
                else:
                    href = f"https://www.tinnhanhchungkhoan.vn/{href}"

            # Chá»‰ láº¥y cÃ¡c link bÃ i viáº¿t tá»« domain tinnhanhchungkhoan.vn
            # vÃ  trÃ¡nh cÃ¡c link menu, category, tag
            if 'tinnhanhchungkhoan.vn' in href and href not in seen_urls:
                # Bá» qua cÃ¡c link khÃ´ng pháº£i bÃ i viáº¿t
                skip_patterns = ['/tag/', '/author/', '/category/', '#', 'javascript:']
                if any(pattern in href for pattern in skip_patterns):
                    continue

                seen_urls.add(href)
                article_urls.append(href)

                if len(article_urls) >= max_articles:
                    break

        print(f"TÃ¬m tháº¥y {len(article_urls)} bÃ i viáº¿t")

        # Fetch chi tiáº¿t tá»«ng bÃ i
        for i, article_url in enumerate(article_urls, 1):
            print(f"[{i}/{len(article_urls)}] Fetching: {article_url[:70]}...")
            self.sleep()
            article_data = self._fetch_article_detail(article_url)
            if article_data:
                all_articles.append(article_data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        """Láº¥y chi tiáº¿t má»™t bÃ i viáº¿t"""
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # 1. Title - <h1 class="article__header cms-title">
        title_el = soup.select_one('h1.article__header.cms-title')
        if not title_el:
            # Fallback: thá»­ selector khÃ¡c
            title_el = soup.select_one('h1.cms-title')
        if not title_el:
            return None
        title = title_el.get_text(strip=True)

        # 2. Published_at - <time class="time" datetime="..." data-time="1767315611">
        published_at = int(datetime.now().timestamp())  # Default
        time_el = soup.select_one('time.time')
        if time_el:
            # Æ¯u tiÃªn dÃ¹ng data-time vÃ¬ nÃ³ lÃ  Unix timestamp sáºµn
            data_time = time_el.get('data-time', '')
            if data_time and data_time.isdigit():
                published_at = int(data_time)
            else:
                # Fallback: parse datetime attribute
                datetime_str = time_el.get('datetime', '')
                if datetime_str:
                    try:
                        # Format: "2026-01-02T08:00:11+0700"
                        # Loáº¡i bá» timezone offset Ä‘á»ƒ parse
                        datetime_str_clean = re.sub(r'[+-]\d{4}$', '', datetime_str)
                        dt = datetime.fromisoformat(datetime_str_clean)
                        published_at = int(dt.timestamp())
                    except:
                        pass

        # 3. Category - <li class="main-cate"><a title="...">
        category = "Chá»©ng khoÃ¡n"  # Default
        cat_el = soup.select_one('li.main-cate a')
        if cat_el:
            category = cat_el.get_text(strip=True)

        # 4. Content - sapo + body
        content_parts = []

        # Sapo
        sapo_el = soup.select_one('div.article__sapo.cms-desc')
        if sapo_el:
            content_parts.append(sapo_el.get_text(strip=True))

        # Body
        body_el = soup.select_one('div.article__body.cms-body')
        if body_el:
            # Loáº¡i bá» ads vÃ  script
            body_copy = copy.copy(body_el)
            for noise in body_copy.select('.ads_middle, script, style, .banner'):
                noise.decompose()

            # Láº¥y text tá»« cÃ¡c tháº» p, h2, h3
            paragraphs = body_copy.find_all(['p', 'h2', 'h3'])
            text_parts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
            content_parts.extend(text_parts)

        content = ' '.join(content_parts).strip()

        if len(content) < 50:  # BÃ i viáº¿t quÃ¡ ngáº¯n, bá» qua
            return None

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",
            "NA",
            False,
            category,
        )


class NguoiQuanSatScraper(NewsScraperBase):
    """
    Scraper cho nguoiquansat.vn â€“ chuáº©n longform (Ä‘Ã£ fix lá»—i 0 articles)
    """

    def __init__(self):
        super().__init__()
        self.source = "nguoiquansat.vn"
        self.headers["Referer"] = "https://nguoiquansat.vn/"

    # =====================================================
    # 1. FETCH LIST PAGE
    # =====================================================
    def fetch_news(self, max_articles: int = 10) -> List[Tuple]:
        url = "https://nguoiquansat.vn/tin-moi-nhat"
        print(f"\nğŸ“¡ Crawling: {url}")

        html = self.fetch_html(url)
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")

        article_urls = []
        seen = set()

        # ---- FIX CHÃNH: Lá»ŒC LINK ÄÃšNG ----
        for a in soup.select("a[href]"):
            href = a.get("href", "").strip()

            if not href.endswith(".html"):
                continue

            # loáº¡i link rÃ¡c
            if any(x in href for x in ["/video", "/media", "/tag", "/author"]):
                continue

            full_url = (
                href if href.startswith("http")
                else f"https://nguoiquansat.vn{href}"
            )

            if full_url not in seen:
                seen.add(full_url)
                article_urls.append(full_url)

            if len(article_urls) >= max_articles:
                break

        print(f"âœ“ Found {len(article_urls)} article URLs")

        results = []
        for i, link in enumerate(article_urls, 1):
            print(f"[{i}/{len(article_urls)}] Fetching: {link}")
            self.sleep()
            data = self._fetch_article_detail(link)
            if data:
                results.append(data)

        return results

    # =====================================================
    # 2. FETCH ARTICLE DETAIL
    # =====================================================
    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, "html.parser")

        # -------- TITLE --------
        title_el = soup.select_one("h1.sc-longform-header-title")
        title = title_el.get_text(strip=True) if title_el else ""
        if not title:
            return None

        # -------- PUBLISHED AT --------
        published_at = int(datetime.now().timestamp())
        date_el = soup.select_one("span.sc-longform-header-date")
        if date_el:
            date_str = date_el.get_text(strip=True).replace(" - ", " ")
            ts = self.parse_date_to_timestamp(date_str, "%d/%m/%Y %H:%M")
            if ts > 0:
                published_at = ts
        # -------- CATEGORY --------
        category_el = soup.select_one("li.breadcrumb-item.active a")
        category = category_el.get_text(strip=True).upper() if category_el else "TIN Tá»¨C"


        # -------- CONTENT --------
        article = soup.select_one("article.entry")
        if not article:
            return None

        paragraphs = []

        # sapo
        sapo_el = article.select_one("p.sc-longform-header-sapo")
        if sapo_el:
            paragraphs.append(sapo_el.get_text(strip=True))

        for p in article.find_all("p", recursive=True):
            if p.find_parent(
                ["div", "figure"],
                class_=["c-box", "oneads", "ads_viewport"]
            ):
                continue

            txt = p.get_text(strip=True)
            if txt and txt not in paragraphs:
                paragraphs.append(txt)

        content = "\n\n".join(paragraphs)
        if not content:
            return None

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",     # author
            "NA",
            False,
            category,
        )

class ThoiBaoTaiChinhScraper(NewsScraperBase):
    """Scraper cho thoibaotaichinhvietnam.vn"""
    def __init__(self):
        super().__init__()
        self.source = "thoibaotaichinhvietnam.vn"
        self.headers["Referer"] = "https://thoibaotaichinhvietnam.vn/"

    # =====================================================
    # 1. FETCH LIST PAGE
    # =====================================================
    def fetch_news(self, max_articles: int = 10) -> List[Tuple]:
        url = "https://thoibaotaichinhvietnam.vn/"
        print(f"\nğŸ“¡ Crawling: {url}")

        html = self.fetch_html(url)
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        article_urls = []
        seen = set()

        # ---- Láº¥y link bÃ i má»›i nháº¥t (.html) ----
        for a in soup.select("a[href$='.html']"):
            href = a.get("href", "").strip()
            if not href:
                continue

            # loáº¡i link rÃ¡c náº¿u cáº§n
            if any(x in href for x in ["/video", "/media", "/tag", "/author"]):
                continue

            # Build full URL an toÃ n (fix lá»—i domain)
            full_url = href if href.startswith("http") else f"https://thoibaotaichinhvietnam.vn/{href.lstrip('/')}"

            if full_url not in seen:
                seen.add(full_url)
                article_urls.append(full_url)

            if len(article_urls) >= max_articles:
                break

        print(f"âœ“ Found {len(article_urls)} article URLs")

        # Crawl chi tiáº¿t tá»«ng bÃ i
        results = []
        for i, link in enumerate(article_urls, 1):
            print(f"[{i}/{len(article_urls)}] Fetching: {link}")
            self.sleep()
            data = self._fetch_article_detail(link)
            if data:
                results.append(data)

        return results

    # =====================================================
    # 2. FETCH ARTICLE DETAIL
    # =====================================================
    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html:
            return None

        soup = BeautifulSoup(html, "html.parser")

        # -------- TITLE --------
        title_el = soup.select_one("h1.post-title")
        title = title_el.get_text(strip=True) if title_el else ""
        if not title:
            return None

        # -------- PUBLISHED AT --------
        published_at = int(datetime.now().timestamp())
        date_el = soup.select_one("span.format_date")
        time_el = soup.select_one("span.format_time")
        if date_el and time_el:
            datetime_str = f"{date_el.get_text(strip=True)} {time_el.get_text(strip=True)}"
            try:
                dt = datetime.strptime(datetime_str, "%d/%m/%Y %H:%M")
                published_at = int(dt.timestamp())
            except Exception as e:
                print(f"âš  Could not parse date '{datetime_str}': {e}")

        # -------- CATEGORY --------
        cate_el = soup.select_one("a.article-catname")
        category = cate_el.get_text(strip=True).upper() if cate_el else "TIN Tá»¨C"

        # -------- CONTENT --------
        paragraphs = []
        desc_el = soup.select_one("div.post-desc")
        if desc_el:
            paragraphs.append(desc_el.get_text(strip=True))

        content_el = soup.select_one("div.post-content.__MASTERCMS_CONTENT")
        if content_el:
            for p in content_el.find_all("p", recursive=True):
                txt = p.get_text(strip=True)
                if txt and txt not in paragraphs:
                    paragraphs.append(txt)

        content = "\n\n".join(paragraphs)
        if not content:
            return None

        return (
            published_at,
            title,
            link,
            content,
            self.source,
            "NA",  # author
            "NA",
            False,
            category,
        )


class Coin68Scraper(NewsScraperBase):
    def __init__(self):
        super().__init__()
        self.source = "coin68.com"
        self.base_url = "https://coin68.com"

    def fetch_news(self, max_articles: int = 10) -> List[Tuple]:
        all_articles = []
        url = self.base_url
        
        print(f"\nğŸ“¡ Äang quÃ©t cáº¥u trÃºc Hot News Coin68...")
        html = self.fetch_html(url)
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. TÃ¬m táº¥t cáº£ cÃ¡c khá»‘i tin dá»±a trÃªn class 'css-19idom' báº¡n cung cáº¥p
        items = soup.find_all('div', class_='css-19idom')
        
        article_urls = []
        for item in items:
            # 2. TÃ¬m tháº» a chá»©a tiÃªu Ä‘á» (tháº» a náº±m trong div css-112x203 nhÆ° máº«u cá»§a báº¡n)
            link_el = item.select_one('div.css-112x203 a')
            if link_el and link_el.get('href'):
                href = link_el.get('href')
                full_url = f"{self.base_url}{href}" if href.startswith('/') else href
                
                if full_url not in article_urls:
                    article_urls.append(full_url)
            
            if len(article_urls) >= max_articles:
                break

        print(f"âœ“ TÃ¬m tháº¥y {len(article_urls)} bÃ i viáº¿t tá»« giao diá»‡n Hot News.")

        # 3. Láº¥y chi tiáº¿t tá»«ng bÃ i
        for i, article_url in enumerate(article_urls, 1):
            print(f"[{i}/{len(article_urls)}] Äang cÃ o: {article_url}")
            self.sleep()
            data = self._fetch_article_detail(article_url)
            if data:
                all_articles.append(data)
                
        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html_text = self.fetch_html(link)
        if not html_text: return None
        soup = BeautifulSoup(html_text, 'html.parser')

        # 1. Láº¥y Title - DÃ¹ng selector á»•n Ä‘á»‹nh (h1 hoáº·c MuiTypography-h2)
        title = ""
        title_el = soup.find('h1')
        if title_el:
            title = title_el.get_text(strip=True)

        # 2. Láº¥y Category (Dá»±a trÃªn breadcrumbs)
        category = "CRYPTO"
        # TÃ¬m breadcrumb chá»©a link /article/ (Ä‘Ã³ lÃ  category)
        category_link = soup.select_one('.MuiBreadcrumbs-li a[href*="/article/"]')
        if category_link:
            span = category_link.find('span')
            if span:
                category = span.get_text(strip=True).upper()

        # 3. Láº¥y Published At - TÃ¬m tháº» span chá»©a ngÃ y
        published_at = int(datetime.now().timestamp())
        # Thá»­ tÃ¬m span chá»©a pattern ngÃ y DD/MM/YYYY
        for span in soup.find_all('span'):
            text = span.get_text(strip=True)
            if '/' in text and len(text) == 10:  # Format: DD/MM/YYYY
                try:
                    dt = datetime.strptime(text, "%d/%m/%Y")
                    published_at = int(dt.timestamp())
                    break
                except:
                    continue

        # 4. Láº¥y Content - DÃ¹ng div#content (khÃ´ng cáº§n class cá»¥ thá»ƒ)
        paragraphs = []
        content_div = soup.find('div', id='content')

        if content_div:
            # Chá»‰ láº¥y text tá»« cÃ¡c tháº» p, bá» qua cÃ¡c tháº» script/iframe/ads
            for p in content_div.find_all('p', recursive=True):
                # Loáº¡i bá» cÃ¡c Ä‘oáº¡n text chá»©a "áº¢nh:", "Nguá»“n:", "CÃ³ thá»ƒ báº¡n quan tÃ¢m"
                txt = p.get_text(strip=True)
                if len(txt) > 30 and not any(x in txt for x in ["áº¢nh:", "Nguá»“n:", "tá»•ng há»£p"]):
                    paragraphs.append(txt)

        content = "\n\n".join(paragraphs)

        # Kiá»ƒm tra Ä‘iá»u kiá»‡n cuá»‘i cÃ¹ng Ä‘á»ƒ trÃ¡nh lÆ°u bÃ i rá»—ng
        if not title or not content:
            return None

        return (published_at, title, link, content, self.source, "NA", "NA", False, category)


class VietnamFinanceScraper(NewsScraperBase):
    def __init__(self):
        super().__init__()
        self.source = "vietnamfinance.vn"
        self.base_url = "https://vietnamfinance.vn"

    def fetch_news(self, max_articles: int = 15) -> List[Tuple]:
        all_articles = []
        html = self.fetch_html(self.base_url)
        if not html: return []

        soup = BeautifulSoup(html, 'html.parser')
        article_links = []

        # 1. Láº¥y link tá»« khu vá»±c articles (bao gá»“m cáº£ Swiper vÃ  Danh sÃ¡ch bÃªn dÆ°á»›i)
        container = soup.select_one('.section-secondary__left .articles')
        if container:
            # TÃ¬m táº¥t cáº£ tháº» a cÃ³ class title hoáº·c náº±m trong h3.article__title
            # CÃ¡ch bÃ³c tÃ¡ch nÃ y khá»›p vá»›i cáº£ 2 máº«u HTML báº¡n gá»­i
            links = container.find_all('a', href=True)
            for a in links:
                href = a['href']
                # Chá»‰ láº¥y link bÃ i viáº¿t (thÆ°á»ng cÃ³ Ä‘uÃ´i .html vÃ  chá»©a mÃ£ d+sá»‘)
                if '.html' in href and href != self.base_url:
                    full_url = href if href.startswith('http') else self.base_url + href
                    if full_url not in article_links:
                        article_links.append(full_url)
                
                if len(article_links) >= max_articles:
                    break

        print(f"âœ“ TÃ¬m tháº¥y {len(article_links)} bÃ i viáº¿t tá»« trang chá»§.")

        for i, link in enumerate(article_links, 1):
            print(f"[{i}/{len(article_links)}] Äang cÃ o: {link}")
            self.sleep()
            data = self._fetch_article_detail(link)
            if data:
                all_articles.append(data)
        
        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')

        # 1. Title (Khá»›p vá»›i h1.detail-title)
        title_el = soup.select_one('h1.detail-title')
        title = title_el.get_text(strip=True) if title_el else ""

        # 2. Category (Khá»›p vá»›i li.breadcrumb-item a)
        category = "FINANCE"
        cate_el = soup.select_one('.breadcrumb-item a.breadcrumb-link')
        if cate_el:
            category = cate_el.get_text(strip=True).upper()

        # 3. Published At (Khá»›p vá»›i tháº» span chá»©a Ä‘á»‹nh dáº¡ng dd/mm/yyyy hh:mm)
        published_at = int(datetime.now().timestamp())
        # TÃ¬m tháº» span cÃ³ ná»™i dung chá»©a ngÃ y thÃ¡ng
        date_el = soup.find('span', string=re.compile(r'\d{2}/\d{2}/\d{4}'))
        if date_el:
            date_str = re.search(r'\d{2}/\d{2}/\d{4}', date_el.get_text()).group()
            try:
                dt = datetime.strptime(date_str, "%d/%m/%Y")
                published_at = int(dt.timestamp())
            except: pass

        # 4. Content (Khá»›p vá»›i #news_detail #explus-editor)
        paragraphs = []
        # Láº¥y Sapo trÆ°á»›c (vÃ¬ nÃ³ chá»©a tÃ³m táº¯t quan trá»ng)
        sapo_el = soup.select_one('.detail-sapo')
        if sapo_el:
            paragraphs.append(sapo_el.get_text(strip=True))

        # Láº¥y cÃ¡c Ä‘oáº¡n trong ná»™i dung chÃ­nh
        content_div = soup.select_one('#news_detail #explus-editor')
        if content_div:
            # Duyá»‡t qua cÃ¡c tháº» p, bá» qua cÃ¡c tháº» ads/script bÃªn trong
            for p in content_div.find_all('p', recursive=False):
                txt = p.get_text(strip=True)
                if len(txt) > 20: # Bá» qua cÃ¡c dÃ²ng quÃ¡ ngáº¯n
                    paragraphs.append(txt)
        
        content = "\n\n".join(paragraphs)

        if not title or len(content) < 100:
            return None

        return (published_at, title, link, content, self.source, "NA", "NA", False, category)


class XaydungChinhsachScraper(NewsScraperBase):
    def __init__(self):
        super().__init__()
        self.source = "xaydungchinhsach.chinhphu.vn"
        self.base_url = "https://xaydungchinhsach.chinhphu.vn"

    def fetch_news(self, max_articles: int = 10) -> List[Tuple]:
        all_articles = []
        # Trang chá»§ cá»§a site nÃ y chÃ­nh lÃ  danh sÃ¡ch tin ná»•i báº­t/má»›i nháº¥t
        html = self.fetch_html(self.base_url)
        if not html: return []

        soup = BeautifulSoup(html, 'html.parser')
        article_links = []

        # TÃ¬m táº¥t cáº£ cÃ¡c link bÃ i viáº¿t (thÆ°á»ng náº±m trong cÃ¡c khá»‘i tin)
        # Site nÃ y sá»­ dá»¥ng cÃ¡c tháº» a cÃ³ thuá»™c tÃ­nh title ráº¥t Ä‘áº§y Ä‘á»§
        links = soup.select('a[title]')
        for a in links:
            href = a.get('href')
            # Lá»c cÃ¡c link lÃ  bÃ i viáº¿t:
            # - Pháº£i cÃ³ .htm
            # - Pháº£i cÃ³ mÃ£ ID sá»‘ (vÃ­ dá»¥: 119260101192109677.htm)
            # - Loáº¡i bá» category pages (khÃ´ng cÃ³ sá»‘ hoáº·c quÃ¡ ngáº¯n)
            if href and '.htm' in href and not href.startswith('javascript'):
                # Kiá»ƒm tra href cÃ³ chá»©a chuá»—i sá»‘ dÃ i (article ID)
                # Article URLs thÆ°á»ng cÃ³ format: /abc-xyz-119260101192109677.htm
                if re.search(r'\d{10,}', href):  # CÃ³ Ã­t nháº¥t 10 chá»¯ sá»‘ liÃªn tiáº¿p = article ID
                    full_url = href if href.startswith('http') else self.base_url + href
                    if full_url not in article_links:
                        article_links.append(full_url)

            if len(article_links) >= max_articles:
                break

        print(f"âœ“ TÃ¬m tháº¥y {len(article_links)} bÃ i viáº¿t tá»« XÃ¢y dá»±ng chÃ­nh sÃ¡ch.")

        for i, link in enumerate(article_links, 1):
            print(f"[{i}/{len(article_links)}] Äang cÃ o: {link}")
            self.sleep()
            data = self._fetch_article_detail(link)
            if data:
                all_articles.append(data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')

        # 1. Title - DÃ¹ng data-role hoáº·c class chÃ­nh xÃ¡c
        title = ""
        title_el = soup.select_one('h1[data-role="title"]') or soup.select_one('h1.title') or soup.find('h1')
        if title_el:
            title = title_el.get_text(strip=True)

        # 2. Category - Tá»« breadcrumbs
        category = "POLICY"
        cat_el = soup.select_one('.list-cate a[data-role="cate-name"]') or soup.select_one('.list-cate a.item-cate')
        if cat_el:
            category = cat_el.get_text(strip=True).upper()

        # 3. Published At - DÃ¹ng data-role="publishdate"
        published_at = int(datetime.now().timestamp())
        date_el = soup.select_one('p[data-role="publishdate"]') or soup.select_one('p.days')
        if date_el:
            date_text = date_el.get_text(strip=True)
            # Format: "03/01/2026 08:56" hoáº·c "03/01/2026"
            match = re.search(r'(\d{2}/\d{2}/\d{4})', date_text)
            if match:
                try:
                    dt = datetime.strptime(match.group(1), "%d/%m/%Y")
                    published_at = int(dt.timestamp())
                except: pass

        # 4. Content - Láº¥y tá»« sapo + detail-content
        paragraphs = []

        # Láº¥y sapo (lead/summary)
        sapo_el = soup.select_one('h2[data-role="sapo"]') or soup.select_one('.detail-sapo')
        if sapo_el:
            sapo_text = sapo_el.get_text(strip=True)
            if len(sapo_text) > 20:
                paragraphs.append(sapo_text)

        # Láº¥y content chÃ­nh
        content_area = soup.select_one('div[data-role="content"]') or soup.select_one('.detail-content.afcbc-body')
        if content_area:
            # Láº¥y cÃ¡c tháº» p, h2, h3, h4 (bá» qua figure, script, style)
            for elem in content_area.find_all(['p', 'h2', 'h3', 'h4']):
                txt = elem.get_text(strip=True)
                # Bá» qua cÃ¡c Ä‘oáº¡n quÃ¡ ngáº¯n, chÃº thÃ­ch áº£nh, link download
                if len(txt) > 30 and not any(skip in txt.lower() for skip in ['nguá»“n:', 'tham kháº£o thÃªm', 'toÃ n vÄƒn:', '---']):
                    paragraphs.append(txt)

        content = "\n\n".join(paragraphs)

        if not title or not content: return None

        return (published_at, title, link, content, self.source, "NA", "NA", False, category)

class QDNDRSSScraper(NewsScraperBase):
    def __init__(self):
        super().__init__()
        self.source = "qdnd.vn"
        self.rss_url = "https://www.qdnd.vn/rss/cate/tin-tuc-moi-nhat.rss"

    def fetch_news(self, max_articles: int = 10) -> List[Tuple]:
        all_articles = []

        print(f"\nğŸ“¡ Äang Ä‘á»c RSS tá»«: {self.rss_url}")

        # 1. Fetch RSS vá»›i requests (vÃ¬ feedparser trá»±c tiáº¿p bá»‹ cháº·n bá»Ÿi redirect)
        try:
            import requests
            response = requests.get(self.rss_url, headers=self.headers, timeout=15)
            response.raise_for_status()

            # Parse RSS content báº±ng feedparser
            feed = feedparser.parse(response.text)
        except Exception as e:
            print(f"âš  Lá»—i khi fetch RSS: {e}")
            return []

        if not feed.entries:
            print("âš  KhÃ´ng thá»ƒ láº¥y dá»¯ liá»‡u tá»« RSS.")
            return []

        # 2. Láº¥y danh sÃ¡ch cÃ¡c link bÃ i viáº¿t
        article_links = []
        for entry in feed.entries:
            link = entry.link
            if link not in article_links:
                article_links.append(link)
            if len(article_links) >= max_articles:
                break

        print(f"âœ“ TÃ¬m tháº¥y {len(article_links)} bÃ i viáº¿t má»›i tá»« RSS.")

        # 3. Duyá»‡t tá»«ng bÃ i Ä‘á»ƒ cÃ o ná»™i dung chi tiáº¿t
        for i, link in enumerate(article_links, 1):
            print(f"[{i}/{len(article_links)}] Äang cÃ o: {link}")
            self.sleep()
            data = self._fetch_article_detail(link)
            if data:
                all_articles.append(data)

        return all_articles

    def _fetch_article_detail(self, link: str) -> Optional[Tuple]:
        html = self.fetch_html(link)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')

        # 1. Title: BÃ¡o QDND dÃ¹ng class post-title
        title_el = soup.select_one('h1.post-title')
        title = title_el.get_text(strip=True) if title_el else ""

        # 2. Category: Láº¥y tá»« breadcrumb (link Ä‘áº§u tiÃªn)
        category = "MILITARY"
        # TÃ¬m link Ä‘áº§u tiÃªn trong breadcrumb vá»›i rel="v:url" vÃ  property="v:title"
        cate_el = soup.find('a', rel='v:url', property='v:title')
        if cate_el:
            category = cate_el.get_text(strip=True).upper()

        # 3. Published At: Láº¥y tá»« class post-date (VÃ­ dá»¥: Chá»§ nháº­t, 04/01/2026)
        published_at = int(datetime.now().timestamp())
        date_el = soup.select_one('.post-date')
        if date_el:
            date_match = re.search(r'(\d{2}/\d{2}/\d{4})', date_el.get_text())
            if date_match:
                try:
                    dt = datetime.strptime(date_match.group(1), "%d/%m/%Y")
                    published_at = int(dt.timestamp())
                except: pass

        # 4. Content: BÃ¡o QDND dÃ¹ng class post-content
        paragraphs = []
        # Láº¥y Sapo (TÃ³m táº¯t)
        sapo_el = soup.select_one('.post-summary')
        if sapo_el:
            paragraphs.append(sapo_el.get_text(strip=True))

        # Láº¥y ná»™i dung chÃ­nh
        content_area = soup.select_one('.post-content')
        if content_area:
            # Loáº¡i bá» cÃ¡c div quáº£ng cÃ¡o, video, áº£nh liÃªn quan náº¿u cÃ³
            for r in content_area.select('.related-post, .video-wrapper, .author-info'):
                r.decompose()
            
            for p in content_area.find_all('p'):
                txt = p.get_text(strip=True)
                if len(txt) > 30:
                    paragraphs.append(txt)
        
        content = "\n\n".join(paragraphs)

        if not title or len(content) < 100:
            return None

        return (published_at, title, link, content, self.source, "NA", "NA", False, category)
